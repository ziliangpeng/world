# ğŸ“š Non-LLM Foundation Models: Vision, Audio, Video, Robotics & Scientific

This document tracks foundation models and specialized builders across non-LLM domains: image generation, video synthesis, audio/speech generation, robotics, scientific models, and medical AI.

---

## ğŸ¨ Image & Vision Generation Models

### European Image Gen Labs

- [x] ğŸ‡©ğŸ‡ª Black Forest Labs
  - ğŸ¨ FLUX.1 series (12B parameters)
  - ğŸ¨ FLUX.1 Schnell (open-source, fastest)
  - ğŸ¨ FLUX.1 Dev (open-weight)
  - ğŸ¨ FLUX.1 Pro (premium API)
  - ğŸ¨ FLUX 1.1 Pro (ultra mode)
  - ğŸ’° $4B valuation talks (2024)
  - ğŸ‘¥ Founded by ex-Stability AI team

### Chinese Image Gen Labs

- [x] ğŸ‡¨ğŸ‡³ Tencent - Hunyuan Image 3.0
  - ğŸ¨ Hunyuan Image 3.0: Text-to-image generation
  - ğŸ¨ Supports multiple styles (watercolor, oil painting, animation, 3D)
  - ğŸ“Š Chinese and English support
  - ğŸ­ Different Dimension Me: Anime photo generator
  - ğŸ”“ Parts of Hunyuan open-sourced (3D models, translation tools)
  - ğŸ“± Available on Hunyuan-image.com

### Multimodal Companies with Image Gen (See LLM-Overview for full details)
- Google (Gemini Diffusion, Veo video)
- OpenAI (DALL-E via GPT-4V)
- Stability AI (Stable Diffusion series)
- Meta (Imagine/Emu image models)

---

## ğŸ¬ Video Generation Models

### Frontier Video Gen (US Dominance)

- [x] ğŸ‡ºğŸ‡¸ OpenAI - Sora/Sora 2
  - ğŸ¥ Sora: 10-20 second videos, 480p-1080p, text/image-to-video
  - ğŸ¥ Sora 2 (Sep 2025): Up to 1 minute, 16-bit HDR, synchronized audio, enhanced physics
  - ğŸ’° Available: ChatGPT Plus ($20/mo, 50 videos), Pro ($200/mo, higher res)
  - ğŸ† Diffusion transformer architecture

- [x] ğŸ‡ºğŸ‡¸ Runway - Gen-2, Gen-3, Gen-4
  - ğŸ¥ Gen-2 (2023): 4-second videos, text/image-to-video
  - ğŸ¥ Gen-3 Alpha (2024): Up to 10 seconds, expressive humans, 60-90s generation
  - ğŸ¥ Gen-4 (March 2025): 5-10 seconds, world consistency, physics simulation
  - ğŸ¥ Gen-4 Turbo (April 2025): 5x faster generation
  - ğŸ’° $536.5M funding, Series D: $308M at $3B valuation (2025)
  - ğŸ“Š 12T+ tokens training

- [x] ğŸ‡ºğŸ‡¸ Google DeepMind - Veo/Veo 2/Veo 3
  - ğŸ¥ Veo (May 2024): 1080p, 60+ seconds, cinematography-aware
  - ğŸ¥ Veo 2 (Dec 2024): Up to 4K, minutes-long videos
  - ğŸ¥ Veo 3 (May 2025): 8-second videos with synchronized audio, lip-syncing
  - ğŸ¥ Veo 3.1 (2025): Available via Gemini API, VideoFX
  - ğŸ“Š State-of-the-art physics simulation

- [x] ğŸ‡ºğŸ‡¸ Meta - Movie Gen/Emu Video
  - ğŸ¥ Emu Video (Nov 2023): 4-second videos, 512x512, text/image-to-video
  - ğŸ¥ Movie Gen (Oct 2024): 16 seconds at 1080p, synchronized audio (up to 45 seconds)
  - ğŸ“Š Video model: 30B params, Audio model: 13B params
  - ğŸ“Š Training: 100M video-text pairs, 1B image-text pairs
  - ğŸ† Outperforms Runway Gen-3 and Sora in benchmarks (research only)

- [x] ğŸ‡ºğŸ‡¸ Midjourney - V1 Video
  - ğŸ¥ V1 Video (June 2025): Image-to-video via "Animate"
  - ğŸ¥ 5-second base clips, extendable by 4s up to 4 times (max 21 seconds)
  - ğŸ¨ Multiple styles: live-action, stop-motion, animation, VFX
  - ğŸ’° $10/month ($10 cheaper than competitors, per Midjourney claims)
  - âš¡ 8x more compute than standard image gen

- [x] ğŸ‡ºğŸ‡¸ Pika - Pika 2.0/2.2
  - ğŸ¥ Pika 2.0 (Dec 2024): 8-second videos, improved text alignment, motion rendering
  - ğŸ¥ Pika 2.2 (2025): 10-second 1080p videos, Pikaframes keyframing
  - ğŸ’° $141M funding, Series B: $80M at $470-700M valuation
  - ğŸ¯ Scene Ingredients: upload/customize characters, objects, settings

- [x] ğŸ‡ºğŸ‡¸ Luma AI - Dream Machine/Ray
  - ğŸ¥ Dream Machine 1.6: 5 seconds at 24fps, text/image-to-video, camera control
  - ğŸ¥ Ray3 (2025): State-of-the-art physics, 16-bit HDR, 5x faster/cheaper
  - ğŸ’° $173M total, Series C: $90M (Dec 2024) led by Amazon
  - ğŸ† First 16-bit HDR video generation

### Specialized Avatar/Synthetic Video (Enterprise Focus)

- [x] ğŸ‡¬ğŸ‡§ Synthesia
  - ğŸ¬ Deep learning for text-to-video + facial animation
  - ğŸ—£ï¸ 140+ languages/accents, AI avatars with natural expressions
  - ğŸ’° $536M total, Series D: $180M at $2.1-4B valuation (Jan 2025)
  - ğŸ‘¥ 60%+ of Fortune 100 use Synthesia
  - ğŸ¯ Enterprise-focused, production-ready

- [x] ğŸ‡ºğŸ‡¸ HeyGen
  - ğŸ¬ Avatar 3.0/Avatar IV with diffusion-inspired audio-to-expression engine
  - ğŸ—£ï¸ 175+ languages/dialects, 100+ AI voices
  - ğŸ“Š Photorealistic facial movements, voice-sync with hand gestures
  - ğŸ¢ Relocated from China to Los Angeles (2022)

- [x] ğŸ‡®ğŸ‡± D-ID
  - ğŸ¬ Creative Reality Studio, 3D facial modeling, RAG for conversations
  - ğŸ—£ï¸ 119 languages/dialects, up to 5-minute videos
  - ğŸ’° $48M total funding
  - ğŸ‘¥ 200M+ videos generated, 280K+ developers

### Asian Video Gen

- [x] ğŸ‡¨ğŸ‡³ Kuaishou - Kling AI
  - ğŸ¥ Kling 1.0/1.6/2.0/2.1: Up to 2-minute videos at 30fps, 1080p
  - ğŸ¨ DiT (Diffusion Transformer) + 3D VAE architecture
  - ğŸ“Š Text/image-to-video, various aspect ratios
  - ğŸ† Positioned as "world's most powerful" video generator (Chinese competitor to OpenAI)

- [x] ğŸ‡¨ğŸ‡³ Shengshu Technology - Vidu
  - ğŸ¥ Vidu Q1/Q2: 16-second to 2-minute videos, 1080p resolution
  - ğŸ¬ Vidu Q2 (upgraded): 3x faster reference video generation
  - ğŸ¨ Reference image generation: Supports inputting up to 7 images simultaneously
  - ğŸ¯ Image-to-video capabilities with high consistency
  - ğŸ’° Series A: Billions of yuan funding (2024)
  - ğŸ¤ Investors: Baidu, Alibaba Ant Group, Qiming Venture Partners, Beijing AI Industry Investment Fund
  - ğŸ“Š Commercial success: 400M+ videos generated, $20M ARR
  - ğŸ¤ Partnerships: JD.com, Amazon, e-commerce/advertising/animation verticals
  - ğŸ‘¨â€ğŸ”¬ Founded March 2023 (spinout from Tsinghua University)

---

## ğŸ™ï¸ Audio & Speech Foundation Models

### Voice & Speech Generation (Proprietary)

- [x] ğŸ‡ºğŸ‡¸ ğŸ‡¬ğŸ‡§ ElevenLabs
  - ğŸ—£ï¸ Eleven v3 (latest): Most expressive with emotional control
  - ğŸ—£ï¸ Eleven Turbo v2.5, Flash v2.5 (75ms ultra-low latency)
  - ğŸ“Š 70+ languages (v3) vs 29 in v2
  - ğŸ’¬ Text-to-Dialogue API for multi-speaker conversations
  - ğŸ’° $3.3B valuation (Series C Jan 2025)
  - ğŸ† Transformer-based, context-aware speech synthesis

- [x] ğŸ‡ºğŸ‡¸ Microsoft VALL-E / Azure AI Speech
  - ğŸ—£ï¸ VALL-E 2: Achieves human parity zero-shot TTS
  - ğŸ—£ï¸ VALL-E X: Multilingual, cross-lingual synthesis
  - ğŸ—£ï¸ DragonV2.1 Neural: Personal voice models
  - ğŸ“Š 100+ languages, voice cloning from 3 seconds
  - ğŸ¯ Neural codec language model architecture

- [x] ğŸ‡ºğŸ‡¸ Meta Voicebox / Audiobox
  - ğŸ—£ï¸ Voicebox: Flow Matching model, voice editing, style transfer
  - ğŸ—£ï¸ Audiobox: Multi-modal generation (voice + text prompts), 160k+ hours training
  - ğŸ“Š 50k+ hours multilingual (6 languages)
  - ğŸ† 20x faster than VALL-E, 10x more intelligible
  - âš ï¸ Research only, not publicly released

- [x] ğŸ‡ºğŸ‡¸ PlayAI / Meta Acquisition
  - ğŸ—£ï¸ PlayHT 2.0: 1M+ hours training, 10x larger model
  - ğŸ—£ï¸ Play 3.0-mini: 30+ languages, optimized for AI agents
  - ğŸ“Š Sub-800ms latency, real-time voice cloning (3-10 seconds)
  - ğŸ’° Meta acquired PlayAI July 2025
  - ğŸŒ 100+ languages with emotional expressiveness

- [x] ğŸ‡ºğŸ‡¸ Cartesia - Sonic/Sonic 2/Sonic 3
  - ğŸ—£ï¸ Sonic 3: State Space Models (SSM) architecture, 3x faster than transformers
  - âš¡ Ultra-low latency: 40ms (Turbo), 90ms (standard)
  - ğŸ“Š 42+ languages, emotional expression (laughter, emotion)
  - ğŸ¯ First SSM alternative to Transformers for TTS
  - ğŸ’° Founded 2023 by Stanford AI Lab alumni

- [x] ğŸ‡ºğŸ‡¸ Descript - Overdub (Lyrebird acquisition)
  - ğŸ—£ï¸ Overdub Voice: Custom voice cloning
  - ğŸ—£ï¸ Rapid Voice Clone 2.0 (2024): 20 seconds needed
  - ğŸ“Š Voice Design: text-to-voice generation
  - ğŸ’° $101M total funding, Lyrebird acquired Sept 2019

- [x] ğŸ‡¨ğŸ‡¦ Resemble AI
  - ğŸ—£ï¸ Rapid Voice Clone 2.0 (2024): 20 seconds audio needed
  - ğŸ—£ï¸ Deepfake detection (Resemble Detect)
  - ğŸ’° $12M funding, Series A: $8M (2023)

- [x] ğŸ‡ºğŸ‡¸ Speechify (WWDC 2025 Apple Design Award)
  - ğŸ—£ï¸ 1,000+ natural voices, 60+ languages
  - ğŸ‘¥ 50M+ users
  - ğŸ¯ AI Voice Cloning, Dubbing, Voice Changer

- [x] ğŸ‡¨ğŸ‡³ iFlyTek (ç§‘å¤§è®¯é£) - Unisound
  - ğŸ—£ï¸ Speech recognition, synthesis, voiceprint synthesis
  - ğŸ—£ï¸ Offline control and far-field processing
  - ğŸ“Š Anti-noise and far-field speech signal processing
  - ğŸ¯ Use cases: Robotics, education, healthcare, connected vehicles
  - ğŸ¢ Major Chinese AI company, publicly listed
  - ğŸ¤ Partnerships with Huawei, Xiaomi, other major tech firms

### Music & Sound Generation

- [x] ğŸ‡ºğŸ‡¸ Suno
  - ğŸµ Chirp v1-v5: Text-to-music with 1,200+ genres
  - ğŸµ v5 (Sept 2025): Up to 8 minutes of music, 90% prompt adherence
  - ğŸµ Suno Studio (2025): Audio workstation with stem editing
  - ğŸ“Š Multimodal transformer + latent diffusion
  - ğŸ’° $125M Series B at $500M valuation
  - âš–ï¸ RIAA copyright infringement lawsuit

- [x] ğŸ‡ºğŸ‡¸ Udio
  - ğŸµ Allegro v1.5 (Oct 2025): Cleaner vocals, cohesive harmonies, precise control
  - ğŸµ Multi-lingual support, extended track creation
  - ğŸ’° $10M seed, led by Andreessen Horowitz
  - âš–ï¸ RIAA copyright infringement lawsuit

- [x] ğŸ‡¬ğŸ‡§ Stability AI - Stable Audio
  - ğŸµ Stable Audio Open: 1.1B params (full), 341M (compact)
  - ğŸµ Stable Audio 2.0/2.5: Up to 47 seconds, full tracks with coherent structures
  - ğŸµ 2.5 (Sept 2025): Enterprise-grade, 8-step generation (50 steps previously)
  - ğŸ“Š Latent diffusion with DiT, compressed autoencoder
  - ğŸ’° $225M total, eliminated debt 2024

- [x] ğŸ‡ºğŸ‡¸ Meta - MusicGen / AudioCraft
  - ğŸµ MusicGen: 300M, 1.5B, 3.3B parameter variants
  - ğŸµ AudioGen: Text-to-sound effects
  - ğŸ“Š EnCodec neural codec + transformer autoregressive LM
  - ğŸ“Š Training: 20k hours (400k recordings)
  - ğŸ”“ Open-source

- [x] ğŸ‡ºğŸ‡¸ Google - MusicLM / Magenta
  - ğŸµ MusicLM (2023): Hierarchical sequence-to-sequence
  - ğŸµ Magenta RealTime (June 2025): 800M params, 48kHz stereo streaming
  - ğŸ“Š 24kHz music for several minutes (MusicLM), real-time streaming (RT)
  - ğŸ¯ Research focus

- [x] ğŸ‡¨ğŸ‡³ ByteDance - Seed-Music / MeLoDy / StemGen
  - ğŸµ Seed-Music (2024): Multimodal (text, audio, scores, sound prompts)
  - ğŸµ MeLoDy: LM-guided diffusion, 257k hours training
  - ğŸµ StemGen: End-to-end generation, 500 hours licensed music
  - ğŸ“± Ripple app integration (iOS, US beta)

- [x] ğŸ‡ºğŸ‡¸ Adobe - Project Music / Firefly Audio
  - ğŸµ Project Music GenAI Control (research preview, Feb 2024)
  - ğŸµ Firefly Audio (Oct 2025): Commercial, studio-quality tracks
  - ğŸµ Full licensing, adjust tempo/structure/intensity/length
  - ğŸ”§ 8-step generation breakthrough (2025)

- [x] ğŸ‡ºğŸ‡¸ Riffusion
  - ğŸµ Fine-tuned Stable Diffusion on spectrograms
  - ğŸµ Real-time generation, latent space interpolation
  - ğŸ’° $4M seed (Oct 2023), Advisors: The Chainsmokers
  - ğŸ”“ Open-source approach

- [x] ğŸ‡¦ğŸ‡º Splash Music
  - ğŸµ HummingLM: Hum-to-full-track generation
  - ğŸ“Š Trained on AWS Trainium + SageMaker HyperPod
  - ğŸ’° $20.1M funding
  - ğŸ† 50% faster training, 54% cost reduction

- [x] ğŸ‡±ğŸ‡º AIVA (Artificial Intelligence Virtual Artist)
  - ğŸµ 250+ musical styles, 85% accuracy in stylistic nuances
  - ğŸµ Trained on 4 centuries of classical masterpieces
  - ğŸ¯ Orchestral arrangement optimization

- [x] ğŸ‡ºğŸ‡¸ Boomy
  - ğŸµ 10M+ songs created (as of 2022)
  - ğŸ“± Distribution to Spotify, Apple Music, TikTok, etc.
  - ğŸ¤ ADA Worldwide (Warner Music Group) partnership

- [x] ğŸ‡¯ğŸ‡µ Soundraw
  - ğŸµ Click-based prompting (genre, mood, tempo, instruments)
  - ğŸ’° $3M Series A (March 2024)

- [x] ğŸŒ Mubert
  - ğŸµ 150+ channel categories, text-to-music, image-to-music
  - ğŸµ Real-time unique generation (not database)

---

## ğŸ¤– Robotics & Embodied AI Foundation Models

### US Humanoid Leaders

- [x] ğŸ‡ºğŸ‡¸ Tesla - Optimus
  - ğŸ¤– **Foundation Model:** End-to-end neural network + Neural World Simulator
  - ğŸ¯ Leverages Full Self-Driving (FSD) AI stack (48 neural networks)
  - ğŸ’¾ **Training Data:** 1.5+ petabytes from vehicle fleet
  - ğŸ¯ **Vision:** Vision-based only (no LiDAR)
  - ğŸ§  **Capabilities:** Reinforcement learning, human-like movement, real-time decisions
  - ğŸ“Š **Production:** 5,000 units targeted for 2025
  - âš™ï¸ **Transfer Learning:** From autonomous vehicles to robotics

- [x] ğŸ‡ºğŸ‡¸ Figure AI - Figure-02
  - ğŸ¤– **Foundation Model:** Vision-Language Model (VLM) + Vision-Language-Action
  - ğŸ¤– **Partner:** OpenAI (custom AI models partnership)
  - ğŸ“Š **Vision:** 6 RGB cameras + onboard VLM
  - ğŸƒ **Physical Specs:** 168cm, 70kg, 5h battery, 1.2m/s, 16 DOF hands
  - ğŸ¯ **Capabilities:** 3x more compute than Figure-01, autonomous conversation
  - ğŸ’° **Funding:** $675M Series B, $39.5B valuation talks (Feb 2025)
  - ğŸ’¼ **Deployment:** BMW factory pilot

- [x] ğŸ‡ºğŸ‡¸ Physical Intelligence - Ï€0 (pi-zero)
  - ğŸ¤– **Architecture:** 3.3B total (3B PaliGemma + 300M control)
  - ğŸ”§ **Innovation:** Novel "flow matching" architecture, 50Hz control
  - ğŸ“Š **Training:** 7 different robots, 68 tasks, 1-20h fine-tuning per new task
  - ğŸ† **Capabilities:** First autonomous laundry folder, box assembly, bussing dishes
  - ğŸ’° **Funding:** $400M (Nov 2024)
  - ğŸ”“ **Status:** Open-sourced (GitHub: openpi)

- [x] ğŸ‡ºğŸ‡¸ Agility Robotics - Digit
  - ğŸ¤– **Foundation Model:** <1M param LSTM "motor cortex"
  - ğŸ“Š **Training:** 2,000 hours simulated motion (3-4 days real-time in NVIDIA Isaac Sim)
  - ğŸ¯ **Zero-shot sim-to-real:** >99% success on real hardware
  - ğŸ’¼ **Deployments:** GXO Georgia fulfillment, Schaeffler plant (Cheraw, SC)
  - ğŸ¤ **Partner:** NVIDIA Isaac Sim training

- [x] ğŸ‡ºğŸ‡¸ Apptronik - Apollo
  - ğŸ¤– **Foundation Model:** Integrates NVIDIA Project GR00T VLM
  - ğŸ“Š **Physical Specs:** 173cm, 73kg, 25kg payload
  - ğŸ¯ **Learning:** From text, video, demonstrations
  - ğŸ’¼ **Partnership:** Mercedes-Benz testing

- [x] ğŸ‡ºğŸ‡¸ NVIDIA - GR00T N1.5
  - ğŸ¤– **Architecture:** Dual-system (fast reflexive + slow deliberate reasoning)
  - ğŸ“Š **Specs:** 3B parameters, first open humanoid FM (2025)
  - âš™ï¸ **Hardware:** Runs on Jetson on-device
  - ğŸ“Š **Training:** 2 trillion tokens, heterogeneous data (real, video, synthetic)
  - ğŸ¤ **Partnerships:** Figure AI, Apptronik, Agility, Boston Dynamics, Fourier
  - ğŸ”“ **Status:** Open-source (GitHub: NVIDIA/Isaac-GR00T)

- [x] ğŸ‡¬ğŸ‡§ ğŸ‡ºğŸ‡¸ 1X Technologies - NEO
  - ğŸ¤– **Platform:** EVE (commercial), NEO (consumer home robot)
  - ğŸ¤– **Foundation Model:** Redwood AI (vision-language transformer)
  - ğŸŒ **World Model:** 1X World Model (data-driven simulator, physics-grounded)
  - ğŸ¯ **Capabilities:** Voice commands, natural language, laundry, door answering
  - ğŸ’° **Founder:** Trond Riiber Knudsen (ex-Halodi)
  - ğŸ’° **Funding:** $23.5M Series A2 (March 2023, OpenAI Startup Fund)
  - ğŸ† **World's first consumer humanoid with home deployment**

- [x] ğŸ‡ºğŸ‡¸ Sanctuary AI - Phoenix
  - ğŸ¤– **Foundation Model:** Carbon AI Control System (mimics brain subsystems)
  - ğŸ“Š **Dexterity:** 20 DOF hands with proprietary haptic tech
  - âš¡ **Task Automation:** <24 hours (vs weeks previously)
  - ğŸ¯ **Capabilities:** 100+ tasks across 12+ industries, natural language tasking
  - ğŸ“Š **Physical:** 170cm, 70kg, 25kg payload

- [x] ğŸ‡¨ğŸ‡¦ Skild AI (CMU Spinout)
  - ğŸ¤– **Foundation Model:** Skild Brain (scalable across hardware/tasks)
  - ğŸ‘¨â€ğŸ”¬ **Founders:** CMU Profs Deepak Pathak & Abhinav Gupta
  - ğŸ“Š **Training:** 1,000x more data than competitors (claimed)
  - ğŸ’° **Funding:** $300M (2024)
  - ğŸ¤– **Platforms:** Mobile manipulation, quadruped

- [x] ğŸ‡®ğŸ‡± Mentee Robotics
  - ğŸ¤– **Foundation Model:** Transformer-based LLM for task interpretation
  - ğŸ‘¨â€ğŸ”¬ **Founder:** Prof. Amnon Shashua (AI expert)
  - ğŸ¯ **Approach:** "Learning from data" + "Learning from experience"
  - ğŸ¯ **Features:** Verbal command execution, 360-degree vision, marker-less navigation

### Chinese Humanoid (Rapid Scale-Up)

- [x] ğŸ‡¨ğŸ‡³ UBTech - Walker S1/S2
  - ğŸ¤– **Foundation Model:** DeepSeek-R1 deep reasoning multimodal model
  - ğŸ§  **BrainNet:** "super brain" + "intelligent sub-brain" architecture
  - ğŸ“Š **Training:** High-quality industrial dataset from real factories
  - ğŸ† **World-First:** Multi-humanoid robot coordination in factories (swarm intelligence)
  - ğŸ’¼ **Deployments:** BYD, Audi China, Zeekr 5G smart factory
  - ğŸ“¦ **Orders:** 500+ for Walker S1
  - ğŸ“ˆ **2025 Target:** 500-1K units (60%+ Walker S2)

- [x] ğŸ‡¨ğŸ‡³ AgiBot (Zhiyuan Robotics)
  - ğŸ¤– **Foundation Model:** Genie Operator-1 (GO-1) - generalist embodied FM
  - ğŸ‘¨â€ğŸ”¯ **Founders:** Deng Taihua & Peng Zhihui (ex-Huawei engineers)
  - ğŸ“Š **Dataset:** AgiBot World (1M+ training sets, 100 robots, open-source)
  - ğŸ† **Largest humanoid manipulation dataset** (as of Dec 2024)
  - ğŸ“¦ **Production:** 962 units manufactured (Dec 15, 2024)
  - ğŸ­ **Status:** Mass production begun
  - ğŸ¯ **Goal:** Match Tesla Optimus output in 2025

- [x] ğŸ‡¨ğŸ‡³ Fourier Intelligence - GR-1
  - ğŸ¤– **Foundation Model:** NVIDIA GR00T N1 support
  - ğŸ“Š **Physical Specs:** 165cm, 55kg, 40 DOF, 50kg payload
  - ğŸ† **Claim:** World's first mass-produced humanoid (100+ units)
  - ğŸ“… **Launch:** July 2023 World AI Conference Shanghai

- [x] ğŸ‡¨ğŸ‡³ Huawei - Kuafu Robot (Pangu 5.0 Embodied AI)
  - ğŸ¤– **Foundation Model:** Pangu 5.0 (billions-trillions parameters)
  - ğŸ“Š **Architecture:** Understanding, NLP, task planning, dual-arm, autonomous execution
  - â˜ï¸ **CloudRobo:** Deploys algorithms to cloud for lightweight on-robot processing
  - ğŸ¢ **Innovation Center:** Shenzhen Global Embodied AI (2024)
  - ğŸ’° **Investment:** $413M into robotics subsidiary
  - ğŸ¯ **Goal:** "Humanoid Robot+" open ecosystem
  - ğŸ¤ **Partnerships:** Leju Robot, Han's Robot

- [x] ğŸ‡¨ğŸ‡³ Unitree Robotics - G1 Humanoid
  - ğŸ¤– **G1 Specs:** $16K (lowest-cost humanoid), superhuman flexibility
  - ğŸ¯ **Capabilities:** Martial arts maneuvers, aerial cartwheels, kip-ups
  - ğŸ“š **Tech Base:** Years of quadruped accumulation â†’ bipedal in 6 months (2023)
  - ğŸ¯ **CEO Prediction:** General-purpose robotic AI model by end 2025
  - ğŸ“ˆ **Launch:** August 2024

- [x] ğŸ‡¨ğŸ‡³ Ex-Robots (Dalian) - Entertainment Humanoids
  - ğŸ¤– **Foundation Model:** Multi-modal for environment recognition + facial feedback
  - ğŸ‘¨â€ğŸ”¬ **Founder:** Li Boyang (PhD AI, Waseda Univ 2010)
  - ğŸ¯ **Focus:** Lifelike facial expressions
  - ğŸ“¦ **Production:** 200+ operational, target 500+ by end 2024
  - ğŸ’° **Price:** $210K-$280K per unit
  - ğŸ¤ **Partnerships:** Huawei, iFlyTek, China Mobile

- [x] ğŸ‡¨ğŸ‡³ Xiaomi - CyberOne
  - ğŸ¤– **Foundation Model:** Mi-Sense depth vision + AI interaction algorithm
  - ğŸ“Š **Physical Specs:** 177cm, 52kg, 21 DOF
  - ğŸ¯ **Capabilities:** 3D perception, gesture/emotion recognition, 45 emotion classifications
  - ğŸ”„ **Display:** Curved OLED for interactive information
  - ğŸ¯ **Future:** Manufacturing integration for specific scenarios

### Quadruped Robots

- [x] ğŸ‡ºğŸ‡¸ Boston Dynamics - Spot/Atlas
  - ğŸ¤– **Foundation Model:** Large Behavior Models (LBMs) partnership with TRI
  - ğŸ“Š **LBM Specs:** 450M-param diffusion transformer, 30Hz action output
  - ğŸ¯ **Capabilities:** Rigid + deformable manipulation, T-shirt folding, assembly
  - ğŸ¤ **Partnership:** Toyota Research Institute (Oct 2024)
  - ğŸ† **Industry leader in dynamic mobility**

- [x] ğŸ‡¨ğŸ‡­ ANYbotics - ANYmal
  - ğŸ¤– **Foundation Model:** Attention-based recurrent encoder + neural network policy
  - ğŸ“Š **Approach:** Sim-to-real transfer, fast automated data generation
  - ğŸ¯ **Capabilities:** Jumping, climbing, crouching, parkour navigation
  - ğŸŒ **Community:** Hundreds of contributors (universities + corporate)
  - ğŸ† **Strong sim-to-real, robust outdoor operation**

### Research & Academic Robotics

- [x] ğŸ‡ºğŸ‡¸ Google DeepMind - RT-1/RT-2/RT-2-X
  - ğŸ¤– **RT-2:** First Vision-Language-Action (VLA) model (55B params)
  - ğŸ“Š **RT-2-X:** 3x more successful than RT-2 for emergent skills
  - ğŸ“Š **Training:** Web-scale vision-language + robotic data
  - ğŸ¯ **Capabilities:** Novel object generalization, emergent reasoning
  - ğŸ† **Pioneered VLA paradigm in robotics**

- [x] ğŸ‡ºğŸ‡¸ Stanford - Mobile ALOHA/OpenVLA
  - ğŸ¤– **Mobile ALOHA:** Bimanual mobile manipulation ($32K, vs $200K commercial)
  - ğŸ“Š **Training:** Supervised behavior cloning, 50 demos/task, 90% success with co-training
  - ğŸ¯ **Tasks:** Shrimp sautÃ©, elevator operation, pan rinsing
  - ğŸ¤– **OpenVLA:** 7B params, beats RT-2-X (55B) by 16.5% with 7x fewer params
  - ğŸ“Š **Training:** Open X-Embodiment dataset, 29 tasks, multiple embodiments

- [x] ğŸ‡ºğŸ‡¸ Berkeley RAIL - BridgeData V2/CrossFormer
  - ğŸ“Š **BridgeData V2:** Large-scale robot manipulation dataset
  - ğŸ“Š **CrossFormer:** 900K trajectories across 20 embodiments
  - ğŸ”¬ **Hardware:** WidowX 250 6DOF arm
  - ğŸ† **Major cross-embodiment learning contributor**

- [x] ğŸ‡ºğŸ‡¸ MIT CSAIL - Foundation Model Supervision
  - ğŸ¤– **KALM:** Pre-trained VLMs for task-relevant keypoints
  - ğŸ¤– **Neural Jacobian Fields (NJF):** Robots learn body response via vision only
  - ğŸ¯ **Approaches:** Leverage non-robot FMs for scalable supervision
  - ğŸ’ª **Impact:** 5-10 demonstrations sufficient for policy generalization

- [x] ğŸ‡ºğŸ‡¸ Toyota Research Institute - Large Behavior Models (LBMs)
  - ğŸ¤– **Architecture:** Diffusion-based transformer (450M params)
  - ğŸ“Š **Training:** ~1,700 hours real robot + 47K sim rollouts, 1,800 evaluations
  - ğŸ† **2024 Breakthrough:** 80% less data, learns 3-5x faster
  - ğŸ¤– **Single LBM:** Hundreds of manipulation tasks
  - ğŸ¤– **Output:** 50 DOF control at 30Hz
  - ğŸ’¡ **Motto:** "If you can demonstrate it, the robot can learn it"
  - ğŸ¤ **Partnership:** Boston Dynamics Atlas (Oct 2024)

- [x] ğŸ‡¨ğŸ‡­ ETH Zurich Robotics
  - ğŸ”¬ **Labs:** ASL, Robotics & Perception, IRIS (7 labs)
  - ğŸ¯ **Focus:** Autonomous navigation (camera/computation only), control strategies
  - ğŸ“š **Research:** Flying, service, mobile robots

- [x] ğŸ‡¨ğŸ‡³ Tencent - Robotics X Lab / Tairos Platform
  - ğŸ¤– **Foundation Model:** Tairos (modular, plug-and-play embodied AI)
  - ğŸ“Š **SLAP3 Framework:** Sensing, Learning, Action, Planning, Perception models
  - â˜ï¸ **Infrastructure:** Cloud simulation, training, data management
  - ğŸ¯ **Vision:** Neutral platform for startups
  - ğŸ¤ **Partnerships:** AgiBot, KEENON, Unitree
  - ğŸ’° **Investor:** Agibot

### Foundation Model Frameworks (Infrastructure)

- [x] **Covariant - RFM-1 (Robotics Foundation Model)**
  - ğŸ¤– **Architecture:** 8B parameter any-to-any transformer
  - ğŸ“Š **Training:** 4 years warehouse pick-and-place data + internet data
  - ğŸ¯ **Capabilities:** First commercial GenAI for robots, physics world model
  - ğŸ“¹ **AI-Generated Video:** Predicts future scenarios, selects best action
  - ğŸ’¼ **Commercial Deployment:** Warehouse operations
  - ğŸ† **First to give robots deeper understanding via GenAI**

- [x] **Open X-Embodiment Dataset**
  - ğŸ“Š **Scale:** 1M+ trajectories, 22 robots, 21 institutions worldwide
  - ğŸ¯ **Impact:** Foundation for cross-embodiment learning

### Robotics Key Trends (2025)

- **Production Scale-Up:** Tesla 5K, UBTech 500-1K, AgiBot mass production
- **Cost Dropping:** Unitree G1 at $16K, Unitree CEO predicts <$10K by 2026
- **Chinese Acceleration:** UBTech multi-robot coordination, AgiBot 1M dataset, Huawei Pangu
- **VLA Model Convergence:** 2-7B parameters becoming standard (vs 55B for RT-2-X)
- **Model Efficiency:** Smaller models (7B OpenVLA) beating larger (55B RT-2-X) by 16.5%
- **Multi-Robot Coordination:** UBTech achieved world-first factory swarm intelligence
- **Data Efficiency:** Toyota LBMs: 80% less data, 3-5x faster learning
- **Sim-to-Real:** Zero-shot transfer >99% success (Agility Digit)
- **Massive Funding:** $1B+ rounds becoming common
- **Task Learning Speed:** <24 hours automation setup (Sanctuary Phoenix)

---

## ğŸ§¬ Scientific & Biological Foundation Models

### Protein & Molecular Biology

- [x] ğŸ‡¬ğŸ‡§ ğŸ‡ºğŸ‡¸ DeepMind/Google - AlphaFold Series
  - ğŸ§¬ AlphaFold 3 (May 2024): Pairformer + diffusion model
  - ğŸ“Š Protein-complex prediction with DNA, RNA, post-translational mods, ligands
  - ğŸ“Š 214M+ structures in public database
  - ğŸ’¾ Proprietary training data (not disclosed)
  - ğŸ† 50%+ improvement over existing methods for interactions

- [x] ğŸ‡ºğŸ‡¸ OpenProtein Foundation - PoET/PoET-2
  - ğŸ§¬ PoET-2: 182M parameters (trillion-param performance equivalent)
  - ğŸ“Š Zero-shot indel prediction, clinical variant effects
  - ğŸ“Š 30-fold less experimental data vs existing
  - ğŸ† State-of-the-art for zero-shot predictions

- [x] ğŸ‡ºğŸ‡¸ EvolutionaryScale - ESM3/Evo/Evo 2
  - ğŸ§¬ ESM3: 98B parameters, 25x more FLOPs than ESM2
  - ğŸ§¬ Evo: 7B params, 131k context (prokaryotic genomes)
  - ğŸ§¬ Evo 2 (2025): 7B/40B params, 1M base pair context, 9.3T nucleotides from 128k+ genomes
  - ğŸ“Š StripedHyena architecture (3x faster than transformers)
  - ğŸ† BRCA1 variant classification: 90% accuracy

- [x] ğŸ‡ºğŸ‡¸ Profluent Bio - ProGen3
  - ğŸ§¬ 112M to 46B parameters (sparse protein LMs)
  - ğŸ“Š 3.4B+ sequences, 1.5T tokens training
  - ğŸ¯ OpenCRISPR-1 (first AI-designed genome editor)
  - ğŸ¯ OpenAntibodies (rivaling blockbuster therapeutics)
  - ğŸ’° $35M+ funding

- [x] ğŸ‡¨ğŸ‡­ ğŸ‡ºğŸ‡¸ Genentech/Roche - Lab-in-the-Loop
  - ğŸ§¬ Proprietary foundation models for drug discovery
  - ğŸ’¼ Therapeutic molecule design across all modalities
  - ğŸ¤ NVIDIA partnership, Recursion $12B commitment

- [x] ğŸ‡ºğŸ‡¸ SchrÃ¶dinger, Inc.
  - ğŸ§¬ Physics-based + ML (Free Energy Perturbation + ML)
  - ğŸ¯ Molecular behavior prediction at atomic level
  - ğŸ¤ NVIDIA DGX A100 systems for acceleration

- [x] ğŸ‡¬ğŸ‡§ Exscientia (acquired by Recursion Nov 2024)
  - ğŸ§¬ Design-Make-Test-Learn loops on AWS
  - ğŸ¯ 4.5 years â†’ 12-15 months drug design
  - ğŸ’° 70% faster, 80% cost reduction

- [x] ğŸ‡ºğŸ‡¸ Recursion Pharmaceuticals - MolE/Phenom/Boltz-2
  - ğŸ§¬ MolE: Molecular foundation model (DeBERTa architecture)
  - ğŸ§¬ Phenom-Beta: Vision transformer for cellular microscopy
  - ğŸ§¬ Boltz-2 (with MIT): First model with structure + binding affinity
  - ğŸ’° $50M NVIDIA investment, BioHive-2 supercomputer (504 H100s)

- [x] ğŸ‡ºğŸ‡¸ Insitro - Biomolecular Models
  - ğŸ§¬ ML models for ADMET, biomarker prediction
  - ğŸ¤ Eli Lilly partnership (ADMET models), Mayo Clinic (ocular biomarkers)

- [x] ğŸ‡ºğŸ‡¸ Tempus AI - Multimodal Oncology FM
  - ğŸ§¬ $200M partnership with AstraZeneca + Pathos AI
  - ğŸ’¾ 8M+ patient records (1.4M imaging, 1.3M genomic, 260k transcriptomics)
  - ğŸ¯ Largest multimodal oncology foundation model

- [x] ğŸ‡ºğŸ‡¸ Chai Discovery - Chai-1/Chai-2
  - ğŸ§¬ Multi-modal FM (proteins, DNA, RNA, small molecules)
  - ğŸ“Š Chai-1: 77% PoseBlast (vs AlphaFold3: 76%)
  - ğŸ“Š Chai-2: Atomic-level structure + binding prediction
  - ğŸ’° $70M Series A (Aug 2024)

- [x] ğŸ‡ºğŸ‡¸ MIT + Recursion - Boltz-2
  - ğŸ§¬ First model combining structure AND binding affinity
  - ğŸ† CASP16 ranked #1 on binding affinity prediction
  - ğŸ”“ Open-source (MIT license for academic + commercial)

- [x] ğŸ‡ºğŸ‡¸ University of Washington - RoseTTAFold/RFdiffusion
  - ğŸ§¬ RoseTTAFold All-Atom (RFAA): Residue + atomic levels
  - ğŸ§¬ RFdiffusion All-Atom: Design proteins with binding pockets
  - ğŸ”“ Open-source, free for all research + drug development
  - ğŸ† Nobel Prize 2024 (David Baker)

### Earth Science & Climate Models

- [x] ğŸ‡¬ğŸ‡§ Google DeepMind - GraphCast/GenCast
  - ğŸŒ GraphCast: 10-day weather forecasts, 0.25Â° resolution
  - ğŸŒ GenCast (Dec 2024): Probabilistic 15-day forecasts, 99.8% accuracy >36hr
  - âš¡ <1 minute on single TPU v4 (vs hours on supercomputer)
  - ğŸ† Hurricane Lee: 9-day landfall prediction (vs 6 days traditional)

- [x] ğŸ‡ºğŸ‡¸ Google Research - NeuralGCM
  - ğŸŒ Traditional fluid dynamics + neural networks for small-scale physics
  - ğŸŒ 2-15 day forecasts, 40-year climate simulation
  - âš¡ 100,000x more efficient than X-SHiELD
  - ğŸ“± Runs on single laptop

- [x] ğŸ‡ºğŸ‡¸ NVIDIA - Earth-2 / cBottle / CorrDiff
  - ğŸŒ cBottle: First generative AI climate foundation model at km resolution
  - ğŸŒ CorrDiff: Generative AI weather at km-scale
  - âš¡ 500x faster, 10,000x more energy-efficient

- [x] ğŸ‡ºğŸ‡¸ NASA + IBM - Prithvi Weather-Climate
  - ğŸŒ Prithvi WxC: 320M params (encoder 220M, decoder 100M)
  - ğŸŒ 2.3B params version available
  - ğŸ“Š 40 years NASA MERRA-2 data, 160 variables
  - ğŸ”“ Open-source on Hugging Face

- [x] ğŸ‡¨ğŸ‡³ Huawei - Pangu-Weather
  - ğŸŒ 10-day typhoon prediction, 5-day regional (3km resolution)
  - âš¡ 10 seconds on single GPU (vs 4-5 hours on 3k-server cluster)
  - ğŸ† Successfully predicted Typhoon Saola (2023)
  - ğŸŒ¾ Madagascar fishermen: 10-day vs 3-day traditional forecasts

- [x] ğŸ‡ªğŸ‡º ESA - TerraMind
  - ğŸŒ Multimodal earth observation (radar + optical + topography)
  - ğŸ’¾ 9M+ samples, 62TB raw data â†’ 1TB optimized
  - ğŸ¤ CloudFerro + ESA Î¦-lab partnership

- [x] ğŸ‡ºğŸ‡¸ Microsoft Research - Aurora
  - ğŸŒ 1.3B parameters, 3D Swin Transformer
  - ğŸ“Š Training: 1M+ hours weather/climate simulations
  - âš¡ 5,000x speedup vs traditional IFS
  - ğŸ¯ First AI to predict global air pollution at km-scale

### Physics & Materials Science

- [x] ğŸ‡¬ğŸ‡§ Google DeepMind - GNoME
  - ğŸ§ª 2.2M new crystal structures discovered (~800 years equivalent knowledge)
  - ğŸ† 380k most stable candidates, 736 independently verified
  - ğŸ¯ Superconductors, batteries (528 lithium-ion conductors), electronics

- [x] ğŸ‡ºğŸ‡¸ Meta FAIR - Universal Model for Atoms (UMA)
  - ğŸ§ª Multi-size models for power/cost/speed tradeoffs
  - ğŸ“Š Open Molecules 2025: 100M quantum mechanics calculations
  - ğŸ¯ Small molecules, biomolecules, metal complexes, electrolytes

- [x] ğŸ‡ºğŸ‡¸ Microsoft Research - MatterGen + MatterSim
  - ğŸ§ª MatterGen: Novel material generation from requirements
  - ğŸ§ª MatterSim: Energy, forces, stress prediction at finite T/P
  - ğŸ“Š MatterGen: 2.9x more stable structures, 17.5x closer to energy minimum
  - ğŸ”“ Open-source (MIT license)

---

## ğŸ¥ Medical & Healthcare Foundation Models

### Medical Imaging & Diagnostics

- [x] ğŸ‡ºğŸ‡¸ ğŸ‡¬ğŸ‡§ Google DeepMind - Med-Gemini
  - ğŸ¥ Med-Gemini: 91.1% on MedQA (USMLE-style), 4.6% improvement over Med-PaLM 2
  - ğŸ¥ State-of-the-art on NEJM Image Challenges
  - ğŸ“Š Multimodal medical understanding (text + images)
  - ğŸ‘ï¸ Superior to GPT-4V on medical benchmarks

- [x] ğŸ‡ºğŸ‡¸ Microsoft - Healthcare AI Models
  - ğŸ¥ MedImageInsight: Embedding model for medical image analysis
  - ğŸ¥ MedImageParse: Precise segmentation (X-ray, CT, MRI, ultrasound, pathology)
  - ğŸ¥ CXRReportGen: Multimodal chest X-ray report generation
  - ğŸ¤ Partnerships: Mass General Brigham, Mayo Clinic, University of Washington

- [x] ğŸ‡ºğŸ‡¸ Paige AI - PRISM2/Digital Pathology
  - ğŸ¥ PRISM2: Foundation model connecting pathology images + clinical language
  - ğŸ¥ Paige Prostate Detect: FDA de novo (Sept 2021)
  - ğŸ¥ Paige PanCancer Detect: FDA Breakthrough (2024)
  - ğŸ”¬ Trained on large-scale pathology datasets

- [x] ğŸ‡ºğŸ‡¸ Providence Healthcare - Prov-GigaPath
  - ğŸ¥ Pretrained on 1.3B pathology image tiles, 171k whole-slides
  - ğŸ“Š Largest whole-slide pretraining (5-10x TCGA)
  - ğŸ† State-of-the-art on 25/26 digital pathology tasks
  - ğŸ“ Available on Microsoft Azure AI Model Catalog

- [x] ğŸ‡©ğŸ‡ª Aignostics - RudolfV / Atlas
  - ğŸ¥ RudolfV: 103K slides, 750M patches, 60+ tissue types
  - ğŸ¥ Atlas (with Mayo Clinic, CharitÃ©): 1.2M+ WSIs
  - ğŸ’° â‚¬31.4M Series B (Oct 2024)
  - ğŸ¤ Bayer strategic collaboration

- [x] ğŸ‡ºğŸ‡¸ PathAI - PLUTO
  - ğŸ¥ PLUTO: Foundation model trained on 160k WSIs, 30+ disease areas
  - ğŸ“Š Multi-scale vision transformer, self-supervised learning
  - ğŸ¯ Cellular, subcellular, and tissue-level analysis
  - ğŸ¤ Roche expanded partnership (Feb 2024)

- [x] ğŸ‡®ğŸ‡± Aidoc - CARE1 Clinical AI Reasoning Engine
  - ğŸ¥ CARE1: First clinical-grade CT foundation model
  - ğŸ¯ Trained on millions of exams, adapts with minimal training
  - ğŸ“Š 50+ FDA-cleared algorithms
  - ğŸ† FDA Breakthrough for acute conditions in CT

- [x] ğŸ‡ºğŸ‡¸ Tempus AI - Multimodal Oncology (see Scientific section)
  - ğŸ¥ $200M partnership with AstraZeneca + Pathos
  - ğŸ’¾ 8M+ de-identified patient records, multimodal data

- [x] ğŸ‡ºğŸ‡¸ ğŸ‡¬ğŸ‡§ IBM Watson Health
  - ğŸ¥ Watson Oncology: Cancer treatment recommendations
  - ğŸ¥ Watsonx Foundation Models: Healthcare-specific fine-tuning

- [x] ğŸ‡¬ğŸ‡§ ğŸ‡­ğŸ‡² ğŸ‡©ğŸ‡ª Siemens Healthineers
  - ğŸ¥ VISTA-3D: CT segmentation (120+ organ classes)
  - ğŸ¥ MAISI: Synthetic 3D CT image generation
  - ğŸ¤ MONAI Deploy + NVIDIA BioNeMo integration

### Clinical Decision Support & EHR

- [x] ğŸ‡ºğŸ‡¸ OpenAI - GPT-4 Medical Applications
  - ğŸ¥ Paradigm partnership: 10% accuracy over human experts on trial matching
  - ğŸ“Š Multimodal medical image interpretation
  - ğŸ’¼ HIPAA-compliant APIs

- [x] ğŸ‡ºğŸ‡¸ Anthropic - Claude for Life Sciences
  - ğŸ¥ Claude Sonnet 4.5: Superior medical imaging accuracy
  - ğŸ¥ Sanofi partnership: Integrated into Concierge app
  - ğŸ¯ Drug development support (discovery â†’ commercialization)

- [x] ğŸ‡«ğŸ‡· Mistral AI - BioMistral
  - ğŸ¥ BioMistral: Mistral 7B pre-trained on PubMed Central
  - ğŸŒ First large-scale multilingual medical LLM evaluation (7 languages)
  - âš ï¸ Research tool only (NOT for production)

- [x] ğŸ‡ºğŸ‡¸ Meta - Me-LLaMA / Meditron
  - ğŸ¥ Me-LLaMA: 13B/70B LLaMA 2-based, 129B medical tokens
  - ğŸ¥ Meditron 7B/70B: Trained on clinical guidelines + papers
  - ğŸ“Š 6 text analysis tasks + clinical diagnosis evaluation
  - ğŸ”“ Open-source

- [x] ğŸ‡¦ğŸ‡ª M42 - Med42
  - ğŸ¥ Med42: 70B parameters, 94.5% on USMLE sample exam (zero-shot)
  - ğŸ† Surpasses prior open medical LLMs
  - ğŸ”“ Free for non-commercial use (LLaMA 2-style license)

### Medical Imaging Foundation Models (Stanford, Harvard, etc.)

- [x] ğŸ‡ºğŸ‡¸ Stanford - CheXagent / RAD-DINO
  - ğŸ¥ CheXagent: Chest X-ray interpretation (8 tasks), outperforms by up to 97.5%
  - ğŸ¥ RAD-DINO: Biomedical image encoder (unimodal training)
  - ğŸ“– CheXbench benchmark, CheXinstruct dataset

- [x] ğŸ‡ºğŸ‡¸ Harvard Medical School - Foundation Models
  - ğŸ¥ Cancer imaging foundation model (Nature ML Intelligence, 2024)
  - ğŸ¥ CONCH: Vision-language FM for pathology (1.17M image-text pairs)
  - ğŸ¥ Chest X-ray, ECG, lung/heart sound models
  - ğŸ¤ Broad Institute, Mass General Hospital collaboration

### Wearables & Health Monitoring

- [x] ğŸ‡ºğŸ‡¸ Apple Health - Biosignal Foundation Models
  - âŒš PPG + ECG encoders with self-supervised learning
  - âŒš Wearable Behavior Model (WBM): 92% accuracy predicting health conditions
  - ğŸ’¾ Apple Heart Movement Study: 141K participants, 3 years
  - ğŸ”’ On-device, end-to-end encrypted

- [x] ğŸ‡ºğŸ‡¸ Google Fitbit - Personal Health LLM
  - âŒš Personal Health LLM: Based on Gemini, fine-tuned on de-identified signals
  - ğŸ¤– Fitbit Labs Chatbot: Conversational Fitbit data queries
  - ğŸ’¼ Device Connect: Enterprise clinical integration

- [x] ğŸ‡«ğŸ‡® Oura Ring - Cardiovascular AI
  - âŒš Cardiovascular Age: Arterial stiffness + pulse wave velocity from PPG
  - âŒš Oura Advisor (July 2024): AI-powered health coaching
  - ğŸ¯ Project RESET: $25M Singapore program mapping heart disease

### Mental Health & Behavioral AI

- [x] ğŸ‡ºğŸ‡¸ Woebot
  - ğŸ’­ Rules-based + generative AI testing
  - ğŸ’­ CBT-based responses, dysfunctional thought recognition
  - ğŸ“Š Remarkable reductions in depression/anxiety
  - âš ï¸ Users report generic/repetitive responses

- [x] ğŸ‡ºğŸ‡¸ Mindstrong
  - ğŸ“± Smartphone usage pattern analysis (typing speed, app navigation)
  - ğŸ“Š 90% accuracy detecting depression/anxiety (Nature Medicine 2023)
  - ğŸ¯ Real-time monitoring, early warning

- [x] ğŸ‡ºğŸ‡¸ Hippocratic AI - Polaris
  - ğŸ¥ Polaris 3.0: 99.38% clinical accuracy
  - ğŸ’° $9/hour operating cost (vs $39/hour RN median)
  - ğŸ’¬ Tested by 6,200+ nurses, 300+ doctors
  - ğŸ¤ NVIDIA H100 GPUs, Universal Health Services deployment

---

## ğŸ“Š Tabular & Specialized Data Foundation Models

- [x] ğŸ‡©ğŸ‡ª University of Freiburg
  - ğŸ“Š TabPFN v2: Bayesian approach, works "out of the box" on time-series
  - ğŸ’¾ 1M+ downloads, 5-10x more data than v1
  - ğŸ”§ CAAFE: Automated feature engineering with LLMs
  - ğŸ›ï¸ ELLIS unit Freiburg, OpenEuroLLM participation

---

## ğŸ“š Infrastructure & Dataset Providers

- [x] ğŸ‡©ğŸ‡ª LAION (Large-scale AI Open Network)
  - ğŸ“Š LAION-5B, LAION-400M, Re-LAION-5B
  - ğŸ¨ Enabled Stable Diffusion, Imagen training
  - âš–ï¸ Won legal case on TDM exceptions (Sept 2024)
  - ğŸ›ï¸ German nonprofit

---

## ğŸ“ˆ Summary Statistics

**Total Organizations Researched: 150+**

**By Category (with entries):**
- ğŸ¨ Image Generation: 1 (Black Forest) + 4 multimodal
- ğŸ¬ Video Generation: 11 major companies
  - US: 5 frontier (OpenAI, Runway, Google, Meta, Midjourney), 2 specialized (Pika, Luma)
  - Avatar/Enterprise: 3 (Synthesia, HeyGen, D-ID)
  - Asian: 1 (Kuaishou Kling)
- ğŸ™ï¸ Audio/Speech: 16+ companies
  - Voice synthesis: 7 major (ElevenLabs, Microsoft, Meta, PlayAI, Cartesia, Descript, Resemble)
  - Music generation: 9+ (Suno, Udio, Stability AI, Meta MusicGen, Google, ByteDance, Adobe, Riffusion, Splash, AIVA, Boomy, Soundraw, Mubert)
- ğŸ¤– Robotics/Embodied AI: 17+ companies/labs
  - US humanoid: 7 (Tesla, Figure AI, Physical Intelligence, Agility, NVIDIA GR00T, 1X, Sanctuary)
  - Chinese: 4 (UBTech, AgiBot, Huawei, Unitree)
  - Research: 6+ (Google DeepMind, Stanford, Berkeley, MIT, Toyota, CMU)
- ğŸ§¬ Scientific/Biological: 25+ organizations
  - Protein/Molecular: 12 (DeepMind, OpenProtein, EvolutionaryScale, Profluent, Genentech, SchrÃ¶dinger, Exscientia, Recursion, Insitro, Tempus, Chai, UW)
  - Earth Science/Climate: 8 (Google DeepMind, NASA-IBM, NVIDIA, Huawei, ESA, Microsoft, etc.)
  - Materials Science: 3 (DeepMind GNoME, Meta UMA, Microsoft MatterGen)
- ğŸ¥ Medical/Healthcare: 28+ organizations
  - Medical imaging: 12 (Google, Microsoft, Paige, Providence, Aignostics, PathAI, Aidoc, Tempus, IBM, Siemens, Stanford, Harvard)
  - Clinical decision support: 5 (OpenAI, Anthropic, Mistral, Meta, M42)
  - Wearables: 3 (Apple, Google Fitbit, Oura Ring)
  - Mental health: 3 (Woebot, Mindstrong, Hippocratic)
- ğŸ“Š Tabular Data: 1 (University of Freiburg)
- ğŸ“š Infrastructure: 1 (LAION)

**Geographic Distribution:**
- ğŸ‡ºğŸ‡¸ United States: 80+ organizations (dominance in video, robotics, medical AI)
- ğŸ‡¨ğŸ‡³ China: 8+ (robotics leaders, music/weather AI)
- ğŸ‡¬ğŸ‡§ United Kingdom: 6 (DeepMind, Stability AI, Exscientia, Boston Dynamics HQ moves, D-ID, Patrick AI parent)
- ğŸ‡ªğŸ‡º Europe: 8+ (Black Forest, Synthesia, AIVA, Aignostics, ESA, Huawei Research)
- ğŸ‡¦ğŸ‡º Australia: 1 (Splash Music)
- ğŸ‡¯ğŸ‡µ Japan: 2 (Soundraw, OpenProtein collaborations)
- ğŸ‡®ğŸ‡± Israel: 2 (D-ID, Aidoc)
- ğŸ‡«ğŸ‡® Finland: 1 (Oura Ring)

**Key Trends (2024-2025):**
1. **Video Gen Convergence:** <1 minute generation becoming standard, audio integration essential
2. **Voice AI Latency War:** 40ms (Cartesia Sonic), 75ms (ElevenLabs Flash) targets achieved
3. **Robotics Scale-Up:** Manufacturing ready, Tesla 5K, UBTech 500+, AgiBot mass production
4. **Multimodal Medical:** Image + genomic + clinical text integration (Tempus $200M initiative)
5. **Open Science Momentum:** AlphaFold 3 released, Boltz-2 open-source, Stable Audio open
6. **Chinese Acceleration:** UBTech swarms, AgiBot 1M datasets, Huawei Pangu scale
7. **Foundation Model Consolidation:** Smaller models (3B-7B) beating large models (55B+)
8. **Computational Efficiency:** 100,000x speedups (NeuralGCM), 5,000x (Aurora weather)

**Funding Highlights:**
- Largest valuations: Synthesia $4B, Runway $3B, Suno $500M, Pika $700M, ElevenLabs $3.3B
- Most funded: Tempus (IPO 2024), Recursion ($50M NVIDIA), Synthesia ($536M)
- Acquisitions: Meta acquired PlayAI (July 2025), Recursion acquired Exscientia (Nov 2024)

**Next Steps for Expansion:**
- Add more detailed model parameters and architecture specifications
- Include performance benchmarks and comparison tables
- Add regulatory approval status (FDA, CE, etc.)
- Include collaboration and partnership networks
- Add predicted 2026 developments
