# ğŸ“š Non-LLM Foundation Models: Vision, Audio, Video, Robotics & Scientific

This document tracks foundation models and specialized builders across non-LLM domains: image generation, video synthesis, audio/speech generation, robotics, scientific models, and medical AI.

---

## ğŸ¨ Image & Vision Generation Models

### European Image Gen Labs

- [x] ğŸ‡©ğŸ‡ª Black Forest Labs
  - ğŸ¨ FLUX.1 series (12B parameters)
  - ğŸ¨ FLUX.1 Schnell (open-source, fastest)
  - ğŸ¨ FLUX.1 Dev (open-weight)
  - ğŸ¨ FLUX.1 Pro (premium API)
  - ğŸ¨ FLUX 1.1 Pro (ultra mode)
  - ğŸ’° $4B valuation talks (2024)
  - ğŸ‘¥ Founded by ex-Stability AI team

### Multimodal Companies with Image Gen (See LLM-Overview for full details)
- Google (Gemini Diffusion, Veo video)
- OpenAI (DALL-E via GPT-4V)
- Stability AI (Stable Diffusion series)
- Meta (Imagine/Emu image models)

---

## ğŸ¬ Video Generation Models

### Frontier Video Gen (US Dominance)

- [x] ğŸ‡ºğŸ‡¸ OpenAI - Sora/Sora 2
  - ğŸ¥ Sora: 10-20 second videos, 480p-1080p, text/image-to-video
  - ğŸ¥ Sora 2 (Sep 2025): Up to 1 minute, 16-bit HDR, synchronized audio, enhanced physics
  - ğŸ’° Available: ChatGPT Plus ($20/mo, 50 videos), Pro ($200/mo, higher res)
  - ğŸ† Diffusion transformer architecture

- [x] ğŸ‡ºğŸ‡¸ Runway - Gen-2, Gen-3, Gen-4
  - ğŸ¥ Gen-2 (2023): 4-second videos, text/image-to-video
  - ğŸ¥ Gen-3 Alpha (2024): Up to 10 seconds, expressive humans, 60-90s generation
  - ğŸ¥ Gen-4 (March 2025): 5-10 seconds, world consistency, physics simulation
  - ğŸ¥ Gen-4 Turbo (April 2025): 5x faster generation
  - ğŸ’° $536.5M funding, Series D: $308M at $3B valuation (2025)
  - ğŸ“Š 12T+ tokens training

- [x] ğŸ‡ºğŸ‡¸ Google DeepMind - Veo/Veo 2/Veo 3
  - ğŸ¥ Veo (May 2024): 1080p, 60+ seconds, cinematography-aware
  - ğŸ¥ Veo 2 (Dec 2024): Up to 4K, minutes-long videos
  - ğŸ¥ Veo 3 (May 2025): 8-second videos with synchronized audio, lip-syncing
  - ğŸ¥ Veo 3.1 (2025): Available via Gemini API, VideoFX
  - ğŸ“Š State-of-the-art physics simulation

- [x] ğŸ‡ºğŸ‡¸ Meta - Movie Gen/Emu Video
  - ğŸ¥ Emu Video (Nov 2023): 4-second videos, 512x512, text/image-to-video
  - ğŸ¥ Movie Gen (Oct 2024): 16 seconds at 1080p, synchronized audio (up to 45 seconds)
  - ğŸ“Š Video model: 30B params, Audio model: 13B params
  - ğŸ“Š Training: 100M video-text pairs, 1B image-text pairs
  - ğŸ† Outperforms Runway Gen-3 and Sora in benchmarks (research only)

- [x] ğŸ‡ºğŸ‡¸ Midjourney - V1 Video
  - ğŸ¥ V1 Video (June 2025): Image-to-video via "Animate"
  - ğŸ¥ 5-second base clips, extendable by 4s up to 4 times (max 21 seconds)
  - ğŸ¨ Multiple styles: live-action, stop-motion, animation, VFX
  - ğŸ’° $10/month ($10 cheaper than competitors, per Midjourney claims)
  - âš¡ 8x more compute than standard image gen

- [x] ğŸ‡ºğŸ‡¸ Pika - Pika 2.0/2.2
  - ğŸ¥ Pika 2.0 (Dec 2024): 8-second videos, improved text alignment, motion rendering
  - ğŸ¥ Pika 2.2 (2025): 10-second 1080p videos, Pikaframes keyframing
  - ğŸ’° $141M funding, Series B: $80M at $470-700M valuation
  - ğŸ¯ Scene Ingredients: upload/customize characters, objects, settings

- [x] ğŸ‡ºğŸ‡¸ Luma AI - Dream Machine/Ray
  - ğŸ¥ Dream Machine 1.6: 5 seconds at 24fps, text/image-to-video, camera control
  - ğŸ¥ Ray3 (2025): State-of-the-art physics, 16-bit HDR, 5x faster/cheaper
  - ğŸ’° $173M total, Series C: $90M (Dec 2024) led by Amazon
  - ğŸ† First 16-bit HDR video generation

### Specialized Avatar/Synthetic Video (Enterprise Focus)

- [x] ğŸ‡¬ğŸ‡§ Synthesia
  - ğŸ¬ Deep learning for text-to-video + facial animation
  - ğŸ—£ï¸ 140+ languages/accents, AI avatars with natural expressions
  - ğŸ’° $536M total, Series D: $180M at $2.1-4B valuation (Jan 2025)
  - ğŸ‘¥ 60%+ of Fortune 100 use Synthesia
  - ğŸ¯ Enterprise-focused, production-ready

- [x] ğŸ‡ºğŸ‡¸ HeyGen
  - ğŸ¬ Avatar 3.0/Avatar IV with diffusion-inspired audio-to-expression engine
  - ğŸ—£ï¸ 175+ languages/dialects, 100+ AI voices
  - ğŸ“Š Photorealistic facial movements, voice-sync with hand gestures
  - ğŸ¢ Relocated from China to Los Angeles (2022)

- [x] ğŸ‡®ğŸ‡± D-ID
  - ğŸ¬ Creative Reality Studio, 3D facial modeling, RAG for conversations
  - ğŸ—£ï¸ 119 languages/dialects, up to 5-minute videos
  - ğŸ’° $48M total funding
  - ğŸ‘¥ 200M+ videos generated, 280K+ developers

### Asian Video Gen

- [x] ğŸ‡¨ğŸ‡³ Kuaishou - Kling AI
  - ğŸ¥ Kling 1.0/1.6/2.0/2.1: Up to 2-minute videos at 30fps, 1080p
  - ğŸ¨ DiT (Diffusion Transformer) + 3D VAE architecture
  - ğŸ“Š Text/image-to-video, various aspect ratios
  - ğŸ† Positioned as "world's most powerful" video generator (Chinese competitor to OpenAI)

---

## ğŸ™ï¸ Audio & Speech Foundation Models

### Voice & Speech Generation (Proprietary)

- [x] ğŸ‡ºğŸ‡¸ ğŸ‡¬ğŸ‡§ ElevenLabs
  - ğŸ—£ï¸ Eleven v3 (latest): Most expressive with emotional control
  - ğŸ—£ï¸ Eleven Turbo v2.5, Flash v2.5 (75ms ultra-low latency)
  - ğŸ“Š 70+ languages (v3) vs 29 in v2
  - ğŸ’¬ Text-to-Dialogue API for multi-speaker conversations
  - ğŸ’° $3.3B valuation (Series C Jan 2025)
  - ğŸ† Transformer-based, context-aware speech synthesis

- [x] ğŸ‡ºğŸ‡¸ Microsoft VALL-E / Azure AI Speech
  - ğŸ—£ï¸ VALL-E 2: Achieves human parity zero-shot TTS
  - ğŸ—£ï¸ VALL-E X: Multilingual, cross-lingual synthesis
  - ğŸ—£ï¸ DragonV2.1 Neural: Personal voice models
  - ğŸ“Š 100+ languages, voice cloning from 3 seconds
  - ğŸ¯ Neural codec language model architecture

- [x] ğŸ‡ºğŸ‡¸ Meta Voicebox / Audiobox
  - ğŸ—£ï¸ Voicebox: Flow Matching model, voice editing, style transfer
  - ğŸ—£ï¸ Audiobox: Multi-modal generation (voice + text prompts), 160k+ hours training
  - ğŸ“Š 50k+ hours multilingual (6 languages)
  - ğŸ† 20x faster than VALL-E, 10x more intelligible
  - âš ï¸ Research only, not publicly released

- [x] ğŸ‡ºğŸ‡¸ PlayAI / Meta Acquisition
  - ğŸ—£ï¸ PlayHT 2.0: 1M+ hours training, 10x larger model
  - ğŸ—£ï¸ Play 3.0-mini: 30+ languages, optimized for AI agents
  - ğŸ“Š Sub-800ms latency, real-time voice cloning (3-10 seconds)
  - ğŸ’° Meta acquired PlayAI July 2025
  - ğŸŒ 100+ languages with emotional expressiveness

- [x] ğŸ‡ºğŸ‡¸ Cartesia - Sonic/Sonic 2/Sonic 3
  - ğŸ—£ï¸ Sonic 3: State Space Models (SSM) architecture, 3x faster than transformers
  - âš¡ Ultra-low latency: 40ms (Turbo), 90ms (standard)
  - ğŸ“Š 42+ languages, emotional expression (laughter, emotion)
  - ğŸ¯ First SSM alternative to Transformers for TTS
  - ğŸ’° Founded 2023 by Stanford AI Lab alumni

- [x] ğŸ‡ºğŸ‡¸ Descript - Overdub (Lyrebird acquisition)
  - ğŸ—£ï¸ Overdub Voice: Custom voice cloning
  - ğŸ—£ï¸ Rapid Voice Clone 2.0 (2024): 20 seconds needed
  - ğŸ“Š Voice Design: text-to-voice generation
  - ğŸ’° $101M total funding, Lyrebird acquired Sept 2019

- [x] ğŸ‡¨ğŸ‡¦ Resemble AI
  - ğŸ—£ï¸ Rapid Voice Clone 2.0 (2024): 20 seconds audio needed
  - ğŸ—£ï¸ Deepfake detection (Resemble Detect)
  - ğŸ’° $12M funding, Series A: $8M (2023)

- [x] ğŸ‡ºğŸ‡¸ Speechify (WWDC 2025 Apple Design Award)
  - ğŸ—£ï¸ 1,000+ natural voices, 60+ languages
  - ğŸ‘¥ 50M+ users
  - ğŸ¯ AI Voice Cloning, Dubbing, Voice Changer

### Music & Sound Generation

- [x] ğŸ‡ºğŸ‡¸ Suno
  - ğŸµ Chirp v1-v5: Text-to-music with 1,200+ genres
  - ğŸµ v5 (Sept 2025): Up to 8 minutes of music, 90% prompt adherence
  - ğŸµ Suno Studio (2025): Audio workstation with stem editing
  - ğŸ“Š Multimodal transformer + latent diffusion
  - ğŸ’° $125M Series B at $500M valuation
  - âš–ï¸ RIAA copyright infringement lawsuit

- [x] ğŸ‡ºğŸ‡¸ Udio
  - ğŸµ Allegro v1.5 (Oct 2025): Cleaner vocals, cohesive harmonies, precise control
  - ğŸµ Multi-lingual support, extended track creation
  - ğŸ’° $10M seed, led by Andreessen Horowitz
  - âš–ï¸ RIAA copyright infringement lawsuit

- [x] ğŸ‡¬ğŸ‡§ Stability AI - Stable Audio
  - ğŸµ Stable Audio Open: 1.1B params (full), 341M (compact)
  - ğŸµ Stable Audio 2.0/2.5: Up to 47 seconds, full tracks with coherent structures
  - ğŸµ 2.5 (Sept 2025): Enterprise-grade, 8-step generation (50 steps previously)
  - ğŸ“Š Latent diffusion with DiT, compressed autoencoder
  - ğŸ’° $225M total, eliminated debt 2024

- [x] ğŸ‡ºğŸ‡¸ Meta - MusicGen / AudioCraft
  - ğŸµ MusicGen: 300M, 1.5B, 3.3B parameter variants
  - ğŸµ AudioGen: Text-to-sound effects
  - ğŸ“Š EnCodec neural codec + transformer autoregressive LM
  - ğŸ“Š Training: 20k hours (400k recordings)
  - ğŸ”“ Open-source

- [x] ğŸ‡ºğŸ‡¸ Google - MusicLM / Magenta
  - ğŸµ MusicLM (2023): Hierarchical sequence-to-sequence
  - ğŸµ Magenta RealTime (June 2025): 800M params, 48kHz stereo streaming
  - ğŸ“Š 24kHz music for several minutes (MusicLM), real-time streaming (RT)
  - ğŸ¯ Research focus

- [x] ğŸ‡¨ğŸ‡³ ByteDance - Seed-Music / MeLoDy / StemGen
  - ğŸµ Seed-Music (2024): Multimodal (text, audio, scores, sound prompts)
  - ğŸµ MeLoDy: LM-guided diffusion, 257k hours training
  - ğŸµ StemGen: End-to-end generation, 500 hours licensed music
  - ğŸ“± Ripple app integration (iOS, US beta)

- [x] ğŸ‡ºğŸ‡¸ Adobe - Project Music / Firefly Audio
  - ğŸµ Project Music GenAI Control (research preview, Feb 2024)
  - ğŸµ Firefly Audio (Oct 2025): Commercial, studio-quality tracks
  - ğŸµ Full licensing, adjust tempo/structure/intensity/length
  - ğŸ”§ 8-step generation breakthrough (2025)

- [x] ğŸ‡ºğŸ‡¸ Riffusion
  - ğŸµ Fine-tuned Stable Diffusion on spectrograms
  - ğŸµ Real-time generation, latent space interpolation
  - ğŸ’° $4M seed (Oct 2023), Advisors: The Chainsmokers
  - ğŸ”“ Open-source approach

- [x] ğŸ‡¦ğŸ‡º Splash Music
  - ğŸµ HummingLM: Hum-to-full-track generation
  - ğŸ“Š Trained on AWS Trainium + SageMaker HyperPod
  - ğŸ’° $20.1M funding
  - ğŸ† 50% faster training, 54% cost reduction

- [x] ğŸ‡±ğŸ‡º AIVA (Artificial Intelligence Virtual Artist)
  - ğŸµ 250+ musical styles, 85% accuracy in stylistic nuances
  - ğŸµ Trained on 4 centuries of classical masterpieces
  - ğŸ¯ Orchestral arrangement optimization

- [x] ğŸ‡ºğŸ‡¸ Boomy
  - ğŸµ 10M+ songs created (as of 2022)
  - ğŸ“± Distribution to Spotify, Apple Music, TikTok, etc.
  - ğŸ¤ ADA Worldwide (Warner Music Group) partnership

- [x] ğŸ‡¯ğŸ‡µ Soundraw
  - ğŸµ Click-based prompting (genre, mood, tempo, instruments)
  - ğŸ’° $3M Series A (March 2024)

- [x] ğŸŒ Mubert
  - ğŸµ 150+ channel categories, text-to-music, image-to-music
  - ğŸµ Real-time unique generation (not database)

---

## ğŸ¤– Robotics & Embodied AI Foundation Models

### US Humanoid Leaders

- [x] ğŸ‡ºğŸ‡¸ Tesla - Optimus
  - ğŸ¤– Neural world simulator trained on fleet video data
  - ğŸ¯ 5,000 units planned for 2025
  - ğŸ’¾ 1.5+ petabytes video data for training
  - ğŸ§  Visual + motor control models

- [x] ğŸ‡ºğŸ‡¸ Figure AI - Figure-02
  - ğŸ¤– Humanoid robot with OpenAI partnership
  - ğŸ’° $675M funding, $39.5B valuation talks
  - ğŸ¯ Dexterous manipulation, real-world deployment

- [x] ğŸ‡ºğŸ‡¸ Physical Intelligence - Ï€0
  - ğŸ¤– Ï€0: 3.3B parameters, first to fold laundry autonomously
  - ğŸ’° $400M funding
  - ğŸ† Autonomous box assembly, laundry folding (1-20 hours fine-tuning per task)

- [x] ğŸ‡ºğŸ‡¸ Agility Robotics - Digit
  - ğŸ¤– <1M param whole-body control (LSTM)
  - ğŸ¯ Zero-shot sim-to-real transfer, >99% success
  - ğŸ’¼ Deployed in GXO warehouses, Schaeffler plant

- [x] ğŸ‡ºğŸ‡¸ NVIDIA - GR00T N1.5
  - ğŸ¤– GR00T N1.5: 3B parameters, first open humanoid foundation model (2025)
  - ğŸ¤– Runs on Jetson on-device
  - ğŸ“Š Training: 2 trillion tokens, robotics datasets

- [x] ğŸ‡¬ğŸ‡§ ğŸ‡ºğŸ‡¸ 1X Technologies - NEO
  - ğŸ¤– NEO: World's first consumer humanoid
  - ğŸ¤– Redwood AI integration with OpenAI backing
  - ğŸ’° Founder: Trond Riiber Knudsen (Halodi/1X)

- [x] ğŸ‡ºğŸ‡¸ Sanctuary AI - Phoenix
  - ğŸ¤– Humanoid with task automation
  - âš¡ <24 hours to automate tasks (vs weeks previously)

- [x] ğŸ‡¬ğŸ‡§ Boston Dynamics - Spot/Atlas
  - ğŸ¤– Mobile manipulation robots
  - ğŸ“Š Advanced control, learning-from-demonstrations

### Chinese Humanoid (Rapid Scale-Up)

- [x] ğŸ‡¨ğŸ‡³ UBTech - Walker S1/S2
  - ğŸ¤– Walker S1/S2: 500+ orders, mass production started
  - ğŸ¯ World's first multi-humanoid coordination (swarm intelligence)
  - ğŸ’¼ BYD, Audi, Zeekr factory deployments
  - ğŸ† Advanced dexterity, industrial deployment

- [x] ğŸ‡¨ğŸ‡³ AgiBot - GO-1
  - ğŸ¤– GO-1: 1M+ training dataset, 962 units manufactured
  - ğŸ“Š AgiBot World dataset (1M+ training sets)
  - ğŸ­ Mass production started

- [x] ğŸ‡¨ğŸ‡³ Huawei - Pangu 5.0 Embodied AI
  - ğŸ¤– Pangu 5.0: Billions-trillions parameters
  - ğŸ’° $413M robotics investment
  - ğŸ¯ Humanoid and quadruped robots

- [x] ğŸ‡¨ğŸ‡³ Unitree - G1
  - ğŸ¤– G1: Humanoid at $16K (lowest cost in market)
  - ğŸ“Š Advanced prediction, general AI model by end 2025 (prediction)

### Research & Academic Robotics

- [x] ğŸ‡ºğŸ‡¸ Google DeepMind - RT-2/RT-X
  - ğŸ¤– RT-2: Vision-Language-Action models (55B parameters)
  - ğŸ¤– RT-X: Open X-Embodiment (1M+ trajectories, 22 robots, 21 institutions)
  - ğŸ”“ Cross-embodiment learning

- [x] ğŸ‡ºğŸ‡¸ Stanford - Mobile ALOHA
  - ğŸ¤– Bimanual teleoperated mobile robot ($32K)
  - ğŸ¯ Tasks: sautÃ© shrimp, operate elevator, rinse pans
  - ğŸ“Š OpenVLA (7B params): Beats RT-2-X (55B) by 16.5% with 7x fewer params

- [x] ğŸ‡ºğŸ‡¸ Berkeley RAIL - BridgeData V2 / CrossFormer
  - ğŸ“Š Robot learning datasets, cross-embodiment models

- [x] ğŸ‡ºğŸ‡¸ MIT CSAIL - KALM / Neural Jacobian Fields
  - ğŸ¤– Foundation model supervision for robotics
  - ğŸ“Š Advanced manipulation learning

- [x] ğŸ‡ºğŸ‡¸ Toyota Research Institute - Large Behavior Models (LBMs)
  - ğŸ¤– 80% less data needed, 3-5x faster learning
  - ğŸ“Š Scale shows dramatic improvement

- [x] ğŸ‡ºğŸ‡¸ CMU - Skild AI
  - ğŸ¤– Spinout company with $300M+ funding
  - ğŸ¯ Robot foundation models for industry

### Robotics Key Trends (2025)

- **Production Scale-Up:** Tesla 5K, UBTech 500-1K, AgiBot mass production
- **Cost Dropping:** Unitree G1 at $16K, trend toward <$10K
- **Chinese Acceleration:** UBTech, AgiBot, Huawei rapid progress
- **VLA Model Convergence:** 2-7B parameters becoming standard (vs 55B for RT-2-X)
- **Multi-Robot Coordination:** UBTech achieved world-first swarm intelligence
- **Massive Funding:** $1B+ rounds becoming common

---

## ğŸ§¬ Scientific & Biological Foundation Models

### Protein & Molecular Biology

- [x] ğŸ‡¬ğŸ‡§ ğŸ‡ºğŸ‡¸ DeepMind/Google - AlphaFold Series
  - ğŸ§¬ AlphaFold 3 (May 2024): Pairformer + diffusion model
  - ğŸ“Š Protein-complex prediction with DNA, RNA, post-translational mods, ligands
  - ğŸ“Š 214M+ structures in public database
  - ğŸ’¾ Proprietary training data (not disclosed)
  - ğŸ† 50%+ improvement over existing methods for interactions

- [x] ğŸ‡ºğŸ‡¸ OpenProtein Foundation - PoET/PoET-2
  - ğŸ§¬ PoET-2: 182M parameters (trillion-param performance equivalent)
  - ğŸ“Š Zero-shot indel prediction, clinical variant effects
  - ğŸ“Š 30-fold less experimental data vs existing
  - ğŸ† State-of-the-art for zero-shot predictions

- [x] ğŸ‡ºğŸ‡¸ EvolutionaryScale - ESM3/Evo/Evo 2
  - ğŸ§¬ ESM3: 98B parameters, 25x more FLOPs than ESM2
  - ğŸ§¬ Evo: 7B params, 131k context (prokaryotic genomes)
  - ğŸ§¬ Evo 2 (2025): 7B/40B params, 1M base pair context, 9.3T nucleotides from 128k+ genomes
  - ğŸ“Š StripedHyena architecture (3x faster than transformers)
  - ğŸ† BRCA1 variant classification: 90% accuracy

- [x] ğŸ‡ºğŸ‡¸ Profluent Bio - ProGen3
  - ğŸ§¬ 112M to 46B parameters (sparse protein LMs)
  - ğŸ“Š 3.4B+ sequences, 1.5T tokens training
  - ğŸ¯ OpenCRISPR-1 (first AI-designed genome editor)
  - ğŸ¯ OpenAntibodies (rivaling blockbuster therapeutics)
  - ğŸ’° $35M+ funding

- [x] ğŸ‡¨ğŸ‡­ ğŸ‡ºğŸ‡¸ Genentech/Roche - Lab-in-the-Loop
  - ğŸ§¬ Proprietary foundation models for drug discovery
  - ğŸ’¼ Therapeutic molecule design across all modalities
  - ğŸ¤ NVIDIA partnership, Recursion $12B commitment

- [x] ğŸ‡ºğŸ‡¸ SchrÃ¶dinger, Inc.
  - ğŸ§¬ Physics-based + ML (Free Energy Perturbation + ML)
  - ğŸ¯ Molecular behavior prediction at atomic level
  - ğŸ¤ NVIDIA DGX A100 systems for acceleration

- [x] ğŸ‡¬ğŸ‡§ Exscientia (acquired by Recursion Nov 2024)
  - ğŸ§¬ Design-Make-Test-Learn loops on AWS
  - ğŸ¯ 4.5 years â†’ 12-15 months drug design
  - ğŸ’° 70% faster, 80% cost reduction

- [x] ğŸ‡ºğŸ‡¸ Recursion Pharmaceuticals - MolE/Phenom/Boltz-2
  - ğŸ§¬ MolE: Molecular foundation model (DeBERTa architecture)
  - ğŸ§¬ Phenom-Beta: Vision transformer for cellular microscopy
  - ğŸ§¬ Boltz-2 (with MIT): First model with structure + binding affinity
  - ğŸ’° $50M NVIDIA investment, BioHive-2 supercomputer (504 H100s)

- [x] ğŸ‡ºğŸ‡¸ Insitro - Biomolecular Models
  - ğŸ§¬ ML models for ADMET, biomarker prediction
  - ğŸ¤ Eli Lilly partnership (ADMET models), Mayo Clinic (ocular biomarkers)

- [x] ğŸ‡ºğŸ‡¸ Tempus AI - Multimodal Oncology FM
  - ğŸ§¬ $200M partnership with AstraZeneca + Pathos AI
  - ğŸ’¾ 8M+ patient records (1.4M imaging, 1.3M genomic, 260k transcriptomics)
  - ğŸ¯ Largest multimodal oncology foundation model

- [x] ğŸ‡ºğŸ‡¸ Chai Discovery - Chai-1/Chai-2
  - ğŸ§¬ Multi-modal FM (proteins, DNA, RNA, small molecules)
  - ğŸ“Š Chai-1: 77% PoseBlast (vs AlphaFold3: 76%)
  - ğŸ“Š Chai-2: Atomic-level structure + binding prediction
  - ğŸ’° $70M Series A (Aug 2024)

- [x] ğŸ‡ºğŸ‡¸ MIT + Recursion - Boltz-2
  - ğŸ§¬ First model combining structure AND binding affinity
  - ğŸ† CASP16 ranked #1 on binding affinity prediction
  - ğŸ”“ Open-source (MIT license for academic + commercial)

- [x] ğŸ‡ºğŸ‡¸ University of Washington - RoseTTAFold/RFdiffusion
  - ğŸ§¬ RoseTTAFold All-Atom (RFAA): Residue + atomic levels
  - ğŸ§¬ RFdiffusion All-Atom: Design proteins with binding pockets
  - ğŸ”“ Open-source, free for all research + drug development
  - ğŸ† Nobel Prize 2024 (David Baker)

### Earth Science & Climate Models

- [x] ğŸ‡¬ğŸ‡§ Google DeepMind - GraphCast/GenCast
  - ğŸŒ GraphCast: 10-day weather forecasts, 0.25Â° resolution
  - ğŸŒ GenCast (Dec 2024): Probabilistic 15-day forecasts, 99.8% accuracy >36hr
  - âš¡ <1 minute on single TPU v4 (vs hours on supercomputer)
  - ğŸ† Hurricane Lee: 9-day landfall prediction (vs 6 days traditional)

- [x] ğŸ‡ºğŸ‡¸ Google Research - NeuralGCM
  - ğŸŒ Traditional fluid dynamics + neural networks for small-scale physics
  - ğŸŒ 2-15 day forecasts, 40-year climate simulation
  - âš¡ 100,000x more efficient than X-SHiELD
  - ğŸ“± Runs on single laptop

- [x] ğŸ‡ºğŸ‡¸ NVIDIA - Earth-2 / cBottle / CorrDiff
  - ğŸŒ cBottle: First generative AI climate foundation model at km resolution
  - ğŸŒ CorrDiff: Generative AI weather at km-scale
  - âš¡ 500x faster, 10,000x more energy-efficient

- [x] ğŸ‡ºğŸ‡¸ NASA + IBM - Prithvi Weather-Climate
  - ğŸŒ Prithvi WxC: 320M params (encoder 220M, decoder 100M)
  - ğŸŒ 2.3B params version available
  - ğŸ“Š 40 years NASA MERRA-2 data, 160 variables
  - ğŸ”“ Open-source on Hugging Face

- [x] ğŸ‡¨ğŸ‡³ Huawei - Pangu-Weather
  - ğŸŒ 10-day typhoon prediction, 5-day regional (3km resolution)
  - âš¡ 10 seconds on single GPU (vs 4-5 hours on 3k-server cluster)
  - ğŸ† Successfully predicted Typhoon Saola (2023)
  - ğŸŒ¾ Madagascar fishermen: 10-day vs 3-day traditional forecasts

- [x] ğŸ‡ªğŸ‡º ESA - TerraMind
  - ğŸŒ Multimodal earth observation (radar + optical + topography)
  - ğŸ’¾ 9M+ samples, 62TB raw data â†’ 1TB optimized
  - ğŸ¤ CloudFerro + ESA Î¦-lab partnership

- [x] ğŸ‡ºğŸ‡¸ Microsoft Research - Aurora
  - ğŸŒ 1.3B parameters, 3D Swin Transformer
  - ğŸ“Š Training: 1M+ hours weather/climate simulations
  - âš¡ 5,000x speedup vs traditional IFS
  - ğŸ¯ First AI to predict global air pollution at km-scale

### Physics & Materials Science

- [x] ğŸ‡¬ğŸ‡§ Google DeepMind - GNoME
  - ğŸ§ª 2.2M new crystal structures discovered (~800 years equivalent knowledge)
  - ğŸ† 380k most stable candidates, 736 independently verified
  - ğŸ¯ Superconductors, batteries (528 lithium-ion conductors), electronics

- [x] ğŸ‡ºğŸ‡¸ Meta FAIR - Universal Model for Atoms (UMA)
  - ğŸ§ª Multi-size models for power/cost/speed tradeoffs
  - ğŸ“Š Open Molecules 2025: 100M quantum mechanics calculations
  - ğŸ¯ Small molecules, biomolecules, metal complexes, electrolytes

- [x] ğŸ‡ºğŸ‡¸ Microsoft Research - MatterGen + MatterSim
  - ğŸ§ª MatterGen: Novel material generation from requirements
  - ğŸ§ª MatterSim: Energy, forces, stress prediction at finite T/P
  - ğŸ“Š MatterGen: 2.9x more stable structures, 17.5x closer to energy minimum
  - ğŸ”“ Open-source (MIT license)

---

## ğŸ¥ Medical & Healthcare Foundation Models

### Medical Imaging & Diagnostics

- [x] ğŸ‡ºğŸ‡¸ ğŸ‡¬ğŸ‡§ Google DeepMind - Med-Gemini
  - ğŸ¥ Med-Gemini: 91.1% on MedQA (USMLE-style), 4.6% improvement over Med-PaLM 2
  - ğŸ¥ State-of-the-art on NEJM Image Challenges
  - ğŸ“Š Multimodal medical understanding (text + images)
  - ğŸ‘ï¸ Superior to GPT-4V on medical benchmarks

- [x] ğŸ‡ºğŸ‡¸ Microsoft - Healthcare AI Models
  - ğŸ¥ MedImageInsight: Embedding model for medical image analysis
  - ğŸ¥ MedImageParse: Precise segmentation (X-ray, CT, MRI, ultrasound, pathology)
  - ğŸ¥ CXRReportGen: Multimodal chest X-ray report generation
  - ğŸ¤ Partnerships: Mass General Brigham, Mayo Clinic, University of Washington

- [x] ğŸ‡ºğŸ‡¸ Paige AI - PRISM2/Digital Pathology
  - ğŸ¥ PRISM2: Foundation model connecting pathology images + clinical language
  - ğŸ¥ Paige Prostate Detect: FDA de novo (Sept 2021)
  - ğŸ¥ Paige PanCancer Detect: FDA Breakthrough (2024)
  - ğŸ”¬ Trained on large-scale pathology datasets

- [x] ğŸ‡ºğŸ‡¸ Providence Healthcare - Prov-GigaPath
  - ğŸ¥ Pretrained on 1.3B pathology image tiles, 171k whole-slides
  - ğŸ“Š Largest whole-slide pretraining (5-10x TCGA)
  - ğŸ† State-of-the-art on 25/26 digital pathology tasks
  - ğŸ“ Available on Microsoft Azure AI Model Catalog

- [x] ğŸ‡©ğŸ‡ª Aignostics - RudolfV / Atlas
  - ğŸ¥ RudolfV: 103K slides, 750M patches, 60+ tissue types
  - ğŸ¥ Atlas (with Mayo Clinic, CharitÃ©): 1.2M+ WSIs
  - ğŸ’° â‚¬31.4M Series B (Oct 2024)
  - ğŸ¤ Bayer strategic collaboration

- [x] ğŸ‡ºğŸ‡¸ PathAI - PLUTO
  - ğŸ¥ PLUTO: Foundation model trained on 160k WSIs, 30+ disease areas
  - ğŸ“Š Multi-scale vision transformer, self-supervised learning
  - ğŸ¯ Cellular, subcellular, and tissue-level analysis
  - ğŸ¤ Roche expanded partnership (Feb 2024)

- [x] ğŸ‡®ğŸ‡± Aidoc - CARE1 Clinical AI Reasoning Engine
  - ğŸ¥ CARE1: First clinical-grade CT foundation model
  - ğŸ¯ Trained on millions of exams, adapts with minimal training
  - ğŸ“Š 50+ FDA-cleared algorithms
  - ğŸ† FDA Breakthrough for acute conditions in CT

- [x] ğŸ‡ºğŸ‡¸ Tempus AI - Multimodal Oncology (see Scientific section)
  - ğŸ¥ $200M partnership with AstraZeneca + Pathos
  - ğŸ’¾ 8M+ de-identified patient records, multimodal data

- [x] ğŸ‡ºğŸ‡¸ ğŸ‡¬ğŸ‡§ IBM Watson Health
  - ğŸ¥ Watson Oncology: Cancer treatment recommendations
  - ğŸ¥ Watsonx Foundation Models: Healthcare-specific fine-tuning

- [x] ğŸ‡¬ğŸ‡§ ğŸ‡­ğŸ‡² ğŸ‡©ğŸ‡ª Siemens Healthineers
  - ğŸ¥ VISTA-3D: CT segmentation (120+ organ classes)
  - ğŸ¥ MAISI: Synthetic 3D CT image generation
  - ğŸ¤ MONAI Deploy + NVIDIA BioNeMo integration

### Clinical Decision Support & EHR

- [x] ğŸ‡ºğŸ‡¸ OpenAI - GPT-4 Medical Applications
  - ğŸ¥ Paradigm partnership: 10% accuracy over human experts on trial matching
  - ğŸ“Š Multimodal medical image interpretation
  - ğŸ’¼ HIPAA-compliant APIs

- [x] ğŸ‡ºğŸ‡¸ Anthropic - Claude for Life Sciences
  - ğŸ¥ Claude Sonnet 4.5: Superior medical imaging accuracy
  - ğŸ¥ Sanofi partnership: Integrated into Concierge app
  - ğŸ¯ Drug development support (discovery â†’ commercialization)

- [x] ğŸ‡«ğŸ‡· Mistral AI - BioMistral
  - ğŸ¥ BioMistral: Mistral 7B pre-trained on PubMed Central
  - ğŸŒ First large-scale multilingual medical LLM evaluation (7 languages)
  - âš ï¸ Research tool only (NOT for production)

- [x] ğŸ‡ºğŸ‡¸ Meta - Me-LLaMA / Meditron
  - ğŸ¥ Me-LLaMA: 13B/70B LLaMA 2-based, 129B medical tokens
  - ğŸ¥ Meditron 7B/70B: Trained on clinical guidelines + papers
  - ğŸ“Š 6 text analysis tasks + clinical diagnosis evaluation
  - ğŸ”“ Open-source

- [x] ğŸ‡¦ğŸ‡ª M42 - Med42
  - ğŸ¥ Med42: 70B parameters, 94.5% on USMLE sample exam (zero-shot)
  - ğŸ† Surpasses prior open medical LLMs
  - ğŸ”“ Free for non-commercial use (LLaMA 2-style license)

### Medical Imaging Foundation Models (Stanford, Harvard, etc.)

- [x] ğŸ‡ºğŸ‡¸ Stanford - CheXagent / RAD-DINO
  - ğŸ¥ CheXagent: Chest X-ray interpretation (8 tasks), outperforms by up to 97.5%
  - ğŸ¥ RAD-DINO: Biomedical image encoder (unimodal training)
  - ğŸ“– CheXbench benchmark, CheXinstruct dataset

- [x] ğŸ‡ºğŸ‡¸ Harvard Medical School - Foundation Models
  - ğŸ¥ Cancer imaging foundation model (Nature ML Intelligence, 2024)
  - ğŸ¥ CONCH: Vision-language FM for pathology (1.17M image-text pairs)
  - ğŸ¥ Chest X-ray, ECG, lung/heart sound models
  - ğŸ¤ Broad Institute, Mass General Hospital collaboration

### Wearables & Health Monitoring

- [x] ğŸ‡ºğŸ‡¸ Apple Health - Biosignal Foundation Models
  - âŒš PPG + ECG encoders with self-supervised learning
  - âŒš Wearable Behavior Model (WBM): 92% accuracy predicting health conditions
  - ğŸ’¾ Apple Heart Movement Study: 141K participants, 3 years
  - ğŸ”’ On-device, end-to-end encrypted

- [x] ğŸ‡ºğŸ‡¸ Google Fitbit - Personal Health LLM
  - âŒš Personal Health LLM: Based on Gemini, fine-tuned on de-identified signals
  - ğŸ¤– Fitbit Labs Chatbot: Conversational Fitbit data queries
  - ğŸ’¼ Device Connect: Enterprise clinical integration

- [x] ğŸ‡«ğŸ‡® Oura Ring - Cardiovascular AI
  - âŒš Cardiovascular Age: Arterial stiffness + pulse wave velocity from PPG
  - âŒš Oura Advisor (July 2024): AI-powered health coaching
  - ğŸ¯ Project RESET: $25M Singapore program mapping heart disease

### Mental Health & Behavioral AI

- [x] ğŸ‡ºğŸ‡¸ Woebot
  - ğŸ’­ Rules-based + generative AI testing
  - ğŸ’­ CBT-based responses, dysfunctional thought recognition
  - ğŸ“Š Remarkable reductions in depression/anxiety
  - âš ï¸ Users report generic/repetitive responses

- [x] ğŸ‡ºğŸ‡¸ Mindstrong
  - ğŸ“± Smartphone usage pattern analysis (typing speed, app navigation)
  - ğŸ“Š 90% accuracy detecting depression/anxiety (Nature Medicine 2023)
  - ğŸ¯ Real-time monitoring, early warning

- [x] ğŸ‡ºğŸ‡¸ Hippocratic AI - Polaris
  - ğŸ¥ Polaris 3.0: 99.38% clinical accuracy
  - ğŸ’° $9/hour operating cost (vs $39/hour RN median)
  - ğŸ’¬ Tested by 6,200+ nurses, 300+ doctors
  - ğŸ¤ NVIDIA H100 GPUs, Universal Health Services deployment

---

## ğŸ“Š Tabular & Specialized Data Foundation Models

- [x] ğŸ‡©ğŸ‡ª University of Freiburg
  - ğŸ“Š TabPFN v2: Bayesian approach, works "out of the box" on time-series
  - ğŸ’¾ 1M+ downloads, 5-10x more data than v1
  - ğŸ”§ CAAFE: Automated feature engineering with LLMs
  - ğŸ›ï¸ ELLIS unit Freiburg, OpenEuroLLM participation

---

## ğŸ“š Infrastructure & Dataset Providers

- [x] ğŸ‡©ğŸ‡ª LAION (Large-scale AI Open Network)
  - ğŸ“Š LAION-5B, LAION-400M, Re-LAION-5B
  - ğŸ¨ Enabled Stable Diffusion, Imagen training
  - âš–ï¸ Won legal case on TDM exceptions (Sept 2024)
  - ğŸ›ï¸ German nonprofit

---

## ğŸ“ˆ Summary Statistics

**Total Organizations Researched: 150+**

**By Category (with entries):**
- ğŸ¨ Image Generation: 1 (Black Forest) + 4 multimodal
- ğŸ¬ Video Generation: 11 major companies
  - US: 5 frontier (OpenAI, Runway, Google, Meta, Midjourney), 2 specialized (Pika, Luma)
  - Avatar/Enterprise: 3 (Synthesia, HeyGen, D-ID)
  - Asian: 1 (Kuaishou Kling)
- ğŸ™ï¸ Audio/Speech: 16+ companies
  - Voice synthesis: 7 major (ElevenLabs, Microsoft, Meta, PlayAI, Cartesia, Descript, Resemble)
  - Music generation: 9+ (Suno, Udio, Stability AI, Meta MusicGen, Google, ByteDance, Adobe, Riffusion, Splash, AIVA, Boomy, Soundraw, Mubert)
- ğŸ¤– Robotics/Embodied AI: 17+ companies/labs
  - US humanoid: 7 (Tesla, Figure AI, Physical Intelligence, Agility, NVIDIA GR00T, 1X, Sanctuary)
  - Chinese: 4 (UBTech, AgiBot, Huawei, Unitree)
  - Research: 6+ (Google DeepMind, Stanford, Berkeley, MIT, Toyota, CMU)
- ğŸ§¬ Scientific/Biological: 25+ organizations
  - Protein/Molecular: 12 (DeepMind, OpenProtein, EvolutionaryScale, Profluent, Genentech, SchrÃ¶dinger, Exscientia, Recursion, Insitro, Tempus, Chai, UW)
  - Earth Science/Climate: 8 (Google DeepMind, NASA-IBM, NVIDIA, Huawei, ESA, Microsoft, etc.)
  - Materials Science: 3 (DeepMind GNoME, Meta UMA, Microsoft MatterGen)
- ğŸ¥ Medical/Healthcare: 28+ organizations
  - Medical imaging: 12 (Google, Microsoft, Paige, Providence, Aignostics, PathAI, Aidoc, Tempus, IBM, Siemens, Stanford, Harvard)
  - Clinical decision support: 5 (OpenAI, Anthropic, Mistral, Meta, M42)
  - Wearables: 3 (Apple, Google Fitbit, Oura Ring)
  - Mental health: 3 (Woebot, Mindstrong, Hippocratic)
- ğŸ“Š Tabular Data: 1 (University of Freiburg)
- ğŸ“š Infrastructure: 1 (LAION)

**Geographic Distribution:**
- ğŸ‡ºğŸ‡¸ United States: 80+ organizations (dominance in video, robotics, medical AI)
- ğŸ‡¨ğŸ‡³ China: 8+ (robotics leaders, music/weather AI)
- ğŸ‡¬ğŸ‡§ United Kingdom: 6 (DeepMind, Stability AI, Exscientia, Boston Dynamics HQ moves, D-ID, Patrick AI parent)
- ğŸ‡ªğŸ‡º Europe: 8+ (Black Forest, Synthesia, AIVA, Aignostics, ESA, Huawei Research)
- ğŸ‡¦ğŸ‡º Australia: 1 (Splash Music)
- ğŸ‡¯ğŸ‡µ Japan: 2 (Soundraw, OpenProtein collaborations)
- ğŸ‡®ğŸ‡± Israel: 2 (D-ID, Aidoc)
- ğŸ‡«ğŸ‡® Finland: 1 (Oura Ring)

**Key Trends (2024-2025):**
1. **Video Gen Convergence:** <1 minute generation becoming standard, audio integration essential
2. **Voice AI Latency War:** 40ms (Cartesia Sonic), 75ms (ElevenLabs Flash) targets achieved
3. **Robotics Scale-Up:** Manufacturing ready, Tesla 5K, UBTech 500+, AgiBot mass production
4. **Multimodal Medical:** Image + genomic + clinical text integration (Tempus $200M initiative)
5. **Open Science Momentum:** AlphaFold 3 released, Boltz-2 open-source, Stable Audio open
6. **Chinese Acceleration:** UBTech swarms, AgiBot 1M datasets, Huawei Pangu scale
7. **Foundation Model Consolidation:** Smaller models (3B-7B) beating large models (55B+)
8. **Computational Efficiency:** 100,000x speedups (NeuralGCM), 5,000x (Aurora weather)

**Funding Highlights:**
- Largest valuations: Synthesia $4B, Runway $3B, Suno $500M, Pika $700M, ElevenLabs $3.3B
- Most funded: Tempus (IPO 2024), Recursion ($50M NVIDIA), Synthesia ($536M)
- Acquisitions: Meta acquired PlayAI (July 2025), Recursion acquired Exscientia (Nov 2024)

**Next Steps for Expansion:**
- Add more detailed model parameters and architecture specifications
- Include performance benchmarks and comparison tables
- Add regulatory approval status (FDA, CE, etc.)
- Include collaboration and partnership networks
- Add predicted 2026 developments
