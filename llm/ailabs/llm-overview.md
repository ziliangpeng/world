# ğŸ“š LLM Foundation Models Research

This file tracks comprehensive research on Large Language Models (LLMs) and foundation models from companies and research labs worldwide.

**Related doc:** See `non-llm-overview.md` for video, image, audio, robotics, and scientific foundation models.

## ğŸ‡ºğŸ‡¸ United States AI Companies

### Frontier Tier (9)

**State-of-the-art models competing at highest level. Massive scale (100B+ parameters), tech giants or heavily funded AI companies.**

- [x] ğŸ‡ºğŸ‡¸ Google (DeepMind)
  - ğŸ¤– Gemini (1.0, 1.5, 2.0, 2.5)
  - ğŸ¤– Gemini Robotics
  - ğŸ¤– Gemma
  - ğŸ¨ Imagen
  - ğŸ¥ Veo
  - ğŸ¨ Gemini Diffusion
- [x] ğŸ‡ºğŸ‡¸ OpenAI
  - ğŸ¤– GPT-1, GPT-2, GPT-3, GPT-3.5, GPT-4, GPT-5
  - ğŸ’» Codex
- [x] ğŸ‡ºğŸ‡¸ Microsoft
  - ğŸ¤– Phi series (Phi-1, Phi-1.5, Phi-2, Phi-3, Phi-4)
- [x] ğŸ‡ºğŸ‡¸ Meta (FAIR)
  - ğŸ¤– LLaMA series (LLaMA, LLaMA 2, LLaMA 3.1, LLaMA 4)
  - ğŸ’» Code Llama
- [x] ğŸ‡ºğŸ‡¸ Amazon
  - ğŸ¤– Titan (Text, Multimodal, Embeddings)
  - ğŸ¤– Amazon Nova
- [x] ğŸ‡ºğŸ‡¸ NVIDIA
  - ğŸ¤– Nemotron series (Nemotron 3, Nano 2 VL, Parse, Safety Guard)
  - ğŸ¤– Llama Nemotron
  - ğŸ¥ Cosmos Nemotron
- [x] ğŸ‡ºğŸ‡¸ Apple
  - ğŸ¤– Apple Intelligence (3B on-device, larger server model)
  - ğŸ¤– Ajax framework (internal LLM infrastructure)
  - ğŸ¯ PT-MoE (Parallel-Track Mixture-of-Experts)
  - ğŸ”’ On-device + Private Cloud Compute architecture
  - ğŸ“± Powers Siri, writing tools, summarization
- [x] ğŸ‡ºğŸ‡¸ Anthropic
  - ğŸ¤– Claude series (1, 3, 3.5, 4, Sonnet, Opus, Haiku)
  - ğŸ‘‘ Claude Sonnet 4.5 (frontier model, competes with GPT-4o)
  - ğŸ‘‘ Claude Opus 4.1 (frontier capability)
  - âš¡ Claude Haiku 4.5
  - ğŸ¯ Multimodal capabilities across all models
  - ğŸ’° $7.3B+ funding
- [x] ğŸ‡ºğŸ‡¸ xAI
  - ğŸ¤– Grok series (Grok-1, Grok-1.5V, Grok 3, Grok 3 Mini, Grok 4, Grok 4 Heavy)
  - ğŸ’ª 314B Mixture-of-Experts (Grok-1)
  - ğŸ¯ Multimodal capabilities
  - ğŸ‘¥ Founded by Elon Musk

## ğŸ‡ªğŸ‡º European AI Labs

- [x] ğŸ‡«ğŸ‡· Kyutai
  - ğŸ¤– Moshi (multimodal voice assistant, 7B + 8B)
  - ğŸ™ï¸ Real-time speech and text understanding
  - ğŸ”“ Open science approach (â‚¬300M budget)
  - ğŸ‘¥ Team from Meta FAIR, Google DeepMind
- [x] ğŸ‡ªğŸ‡º EuroLLM Consortium
  - ğŸ¤– EuroLLM-9B (released Dec 2024)
  - ğŸŒ Supports all 24 EU official languages + 11 international
  - ğŸ¤ Multi-country collaboration
  - ğŸ”“ Open-source commitment
- [x] ğŸ‡ªğŸ‡º OpenEuroLLM
  - ğŸ¤– Open-source LLMs for all EU languages
  - ğŸ“ Led by Charles University (Prague) + Silo AI
  - ğŸ’° EU-funded 2025 program
  - ğŸŒ Pan-European collaboration
- [x] ğŸ‡¬ğŸ‡§ Stability AI
  - ğŸ¨ Stable Diffusion series (1, 2, 3, 3.5)
  - ğŸ¨ Stable Diffusion 3.5 Large
  - ğŸ¤– StableLM (3B, 7B, 15B-65B)
  - ğŸ¥ Stable Video Diffusion
- [x] ğŸ‡«ğŸ‡· Mistral AI
  - ğŸ¤– Mistral 7B
  - ğŸ¤– Mixtral 8x7B
  - ğŸ¤– Mistral series
  - ğŸ’­ Magistral Small (open-source reasoning)
  - ğŸ’­ Magistral Medium (reasoning)
- [x] ğŸ‡©ğŸ‡ª Aleph Alpha
  - ğŸ¤– Luminous model family (multimodal text+image)
  - ğŸ¤– Pharia-1-LLM series (7B parameters)
  - ğŸ”“ Tokenizer-free (T-Free) architecture
  - ğŸŒ Optimized for German, French, Spanish
  - ğŸ¨ MAGMA (open-source multimodal)
- [x] ğŸ‡«ğŸ‡· LightOn
  - ğŸ¤– Lyra-fr (10B French parameters)
  - ğŸ‘ï¸ VLM-4 (European languages)
  - ğŸ¤– Alfred-40B-0723 (based on Falcon-40B)
  - ğŸ”“ 100+ billion parameter open models
  - ğŸ”’ GDPR-compliant platform (Paradigm)
- [x] ğŸ‡®ğŸ‡¹ iGenius
  - ğŸ¤– Colosseum 355B (highly regulated industries)
  - ğŸ’° Italia (open-source financial sector)
  - ğŸŒ 50+ languages support
  - ğŸ› ï¸ Built with NVIDIA NIM microservices
- [x] ğŸ‡«ğŸ‡® Silo AI
  - ğŸ¤– Poro 34B (Finnish, English, code)
  - ğŸ¤– Viking models (Nordic and European languages)
  - ğŸ¤– TildeOpen LLM (30B+, Balto-Slavic)
  - ğŸ”§ SiloGen platform
- [x] ğŸ‡©ğŸ‡ª Nyonic
  - ğŸ¤– Wonton-7B (base model)
  - ğŸ¤– Wonton-3B
  - ğŸ¤– Wonton-1.5B
  - ğŸŒ Industrial-focused multilingual foundation models (30+ languages)
  - ğŸ“¦ Model-as-a-Service platform
  - ğŸ’¼ Domain-specific training for enterprise use
  - ğŸ’° Backed by Lenovo Capital and Incubator Group
- [x] ğŸ‡«ğŸ‡· Bioptimus
  - ğŸ§¬ Universal AI foundation model for biology
  - ğŸ§¬ Integrates biological/biomedical data
  - ğŸ”¬ Domain-specialized foundation models
  - ğŸ‘¥ Founded 2024 in Paris
  - ğŸ¯ Focus on healthcare, genomics, and life sciences
  - ğŸ¤ AI agents for scientific discovery

## ğŸ‡ªğŸ‡º Extended European AI Labs (Additional Countries)

- [x] ğŸ‡¬ğŸ‡§ UK-LLM / BritLLM (University College London)
  - ğŸ¤– UK's national LLM initiative
  - ğŸ—£ï¸ Welsh, Scots, Scottish Gaelic support
  - ğŸ¯ UK language preservation
  - ğŸ¤ Collaboration with Bangor University

- [x] ğŸ‡©ğŸ‡ª OpenGPT-X / Teuken-7B (Fraunhofer Institutes)
  - ğŸ¤– 7B parameters, all 24 EU official languages
  - ğŸ¢ German government-backed initiative
  - ğŸ’» Trained on JÃ¼lich supercomputer
  - ğŸ¯ Sovereign German AI

- [x] ğŸ‡©ğŸ‡ª LeoLM (LAION + Hessian.AI)
  - ğŸ¤– 70B parameters, German-optimized
  - ğŸ“Š First commercially viable open-source German LLM
  - ğŸ† Beats Llama 2 on German benchmarks
  - ğŸ”“ Open-source, community-driven

- [x] ğŸ‡«ğŸ‡· LUCIE (LINAGORA + France 2030)
  - ğŸ¤– Open-source ethical AI model for Europe
  - ğŸ“ Education/EdTech focus
  - ğŸ“… Released January 2025
  - ğŸ‡«ğŸ‡· French government-backed

- [x] ğŸ‡³ğŸ‡± GPT-NL (TNO + Netherlands Forensic Institute + SURF)
  - ğŸ¤– Dutch values-aligned LLM
  - ğŸ’° â‚¬13.5M government funding
  - ğŸ¯ National Dutch language model
  - ğŸ‡³ğŸ‡± Netherlands national initiative

- [x] ğŸ‡³ğŸ‡± Geitje (Netherlands)
  - ğŸ¤– Mistral-7B based, Dutch-trained
  - ğŸ“Š Open-source Dutch foundation model
  - ğŸ—£ï¸ Netherlands language focus

- [x] ğŸ‡¸ğŸ‡ª GPT-SW3 (AI Sweden + RISE)
  - ğŸ¤– First LLM for Nordic languages
  - ğŸ¯ Swedish national model
  - ğŸ‡¸ğŸ‡ª Sweden national initiative

- [x] ğŸ‡³ğŸ‡´ NorskGPT / Bineric AI (Norway)
  - ğŸ¤– Norway's first commercial LLM
  - ğŸ—£ï¸ BokmÃ¥l and Nynorsk support
  - ğŸ‡³ğŸ‡´ Norwegian sovereign AI
  - ğŸ”’ GDPR compliant

- [x] ğŸ‡®ğŸ‡¹ Minerva (Sapienza University + CINECA)
  - ğŸ¤– First Italian LLM family trained from scratch
  - ğŸ’» Leonardo supercomputer
  - ğŸ“Š 50% Italian, 50% English training data
  - ğŸ›ï¸ Italian academic consortium

- [x] ğŸ‡®ğŸ‡¹ Domyn (Italy)
  - ğŸ¤– NVIDIA partnership for Italian LLM
  - ğŸ‡®ğŸ‡¹ Italian Ministry backed
  - ğŸ¯ Italian sovereign AI infrastructure

- [x] ğŸ‡ªğŸ‡¸ Spanish National LLM / Barcelona Supercomputing Center
  - ğŸ¤– Spanish, Catalan, Basque, Galician, Valencian
  - ğŸ’¼ Government + IBM partnership
  - ğŸŒ Latin America collaboration
  - ğŸ›ï¸ Spanish government-backed

- [x] ğŸ‡ªğŸ‡¸ Clibrain / Lince Zero (Spain)
  - ğŸ¤– Spanish-optimized commercial LLM
  - ğŸ¢ Madrid-based startup
  - ğŸŒ European/Spanish focus

- [x] ğŸ‡µğŸ‡± PLLuM (Polish Large Language Model)
  - ğŸ¤– Government-backed, open-source Polish LLM
  - ğŸ‡µğŸ‡± Polish national initiative

- [x] ğŸ‡µğŸ‡± Bielik & Qra (Poland)
  - ğŸ¤– Polish foundation models
  - ğŸ“ Community-driven (Bielik)
  - ğŸ›ï¸ Academic (Qra, GdaÅ„sk University)

- [x] ğŸ‡¹ğŸ‡· Kumru AI (VNGRS, Turkey)
  - ğŸ¤– 7.4B parameters, first Turkish LLM
  - ğŸ“… Unveiled October 2025
  - ğŸ‡¹ğŸ‡· Turkish sovereign AI

- [x] ğŸ‡¹ğŸ‡· T3AI (Turkish Technology Team Foundation + Baykar)
  - ğŸ¤– First Turkish LLM (defense-backed)
  - ğŸ“… Beta launched on Teknofest (2025)
  - ğŸ‡¹ğŸ‡· Turkish national defense initiative

- [x] ğŸ‡ºğŸ‡¦ Lapa LLM (Ukraine AI Factory + Kyivstar)
  - ğŸ¤– National Ukrainian LLM
  - ğŸ“… Launch: Nov-Dec 2025
  - ğŸ’ª 50% more efficient tokenizer
  - ğŸ‡ºğŸ‡¦ Ukrainian sovereign AI amid war

- [x] ğŸ‡®ğŸ‡ª UCCIX (ReML-AI, Ireland)
  - ğŸ¤– Irish-eXcellence LLM (Llama 2-13B based)
  - ğŸ›ï¸ Irish academic model

- [x] ğŸ‡®ğŸ‡¸ MiÃ°eind (Iceland)
  - ğŸ¤– Icelandic language LLM with OpenAI
  - ğŸ—£ï¸ 4B+ words Icelandic training data
  - ğŸ¯ Language preservation

## ğŸ‡¨ğŸ‡¦ Canadian AI Labs

- [x] ğŸ‡¨ğŸ‡¦ Mila - Quebec AI Institute
  - ğŸ“ World's largest academic ML research center
  - ğŸ”¬ Foundation models research and development
  - ğŸ”’ LLM safety and alignment research
  - ğŸŒ Cultural competence in AI
  - ğŸ‘¥ 500+ researchers, led by Yoshua Bengio
- [x] ğŸ‡¨ğŸ‡¦ Vector Institute (Toronto)
  - ğŸ“ National AI institute
  - ğŸ”¬ Foundation model research and safety evaluation
  - ğŸ¤ Pan-Canadian AI Strategy (with Mila, Amii)
  - ğŸ¢ Industry partnerships for AI deployment
- [x] ğŸ‡¨ğŸ‡¦ Recursal AI
  - ğŸ¤– EagleX (RWKV architecture)
  - ğŸŒ 100+ languages support
  - âš¡ Energy-efficient architecture
  - ğŸ”“ Novel approach to transformers
- [x] ğŸ‡¨ğŸ‡¦ Cohere
  - ğŸ¤– Command series (A, R7B, A Translate, A Reasoning, A Vision, R+, R)
  - ğŸŒ Aya family (23 languages, Aya Expanse, Aya Vision)
  - ğŸ” Rerank 3
  - ğŸ“Š Embed
  - ğŸ¢ Enterprise and developer focused

## ğŸ‡¹ğŸ‡¼ Taiwan AI Labs

- [x] ğŸ‡¹ğŸ‡¼ TAIDE (Taiwan)
  - ğŸ¤– Trustworthy AI Dialogue Engine
  - ğŸ“Š Based on Llama 2, 7B and 13B parameters
  - ğŸ“… Released April 2024
  - ğŸ¯ Taiwan's national LLM

- [x] ğŸ‡¹ğŸ‡¼ Formosa Foundation Model (Taiwan AI Cloud)
  - ğŸ¤– 176B parameters, Traditional Chinese
  - ğŸ’» Taiwania-2 supercomputer
  - ğŸ¢ Enterprise-level multilingual model

- [x] ğŸ‡¹ğŸ‡¼ FoxBrain (Hon Hai Research Institute)
  - ğŸ¤– 70B parameters, Llama 3.1 based
  - ğŸ’­ First Traditional Chinese reasoning LLM
  - ğŸ“… Launched March 2025
  - ğŸ¢ Taiwan's latest commercial LLM

- [x] ğŸ‡¹ğŸ‡¼ Llama-3-Taiwan (National Taiwan University + partners)
  - ğŸ¤– TAME (Taiwan Mixture of Experts) project
  - ğŸ“Š 70B parameters
  - ğŸ“ Taiwan academic consortium

## ğŸ‡­ğŸ‡° Hong Kong AI Labs

- [x] ğŸ‡­ğŸ‡° HKGAI V1 (Hong Kong Generative AI R&D Center)
  - ğŸ¤– First Hong Kong sovereign LLM (DeepSeek based)
  - ğŸ“… Released February 2025
  - ğŸ’¬ HKChat chatbot (launching 2025)
  - ğŸ›ï¸ Hong Kong government-backed

- [x] ğŸ‡­ğŸ‡° InvestLM (HKUST Business School)
  - ğŸ’° Hong Kong's first open-source financial LLM
  - ğŸ“Š Finance-domain specialized
  - ğŸ›ï¸ Academic research model

## ğŸ‡·ğŸ‡º Russian AI Labs

- [x] ğŸ‡·ğŸ‡º Yandex (Already listed in Other Commercial)
- [x] ğŸ‡·ğŸ‡º Sber / GigaChat (Russia)
  - ğŸ¤– Leading Russian LLM alongside YandexGPT
  - ğŸ’¬ GigaChat (main product)
  - ğŸŒ Russian frontier foundation model
  - ğŸ¢ Russian banking/tech giant

- [x] ğŸ‡·ğŸ‡º Kandinsky (Sber, Russia)
  - ğŸ¨ Text-to-image multimodal model
  - ğŸ† Competes with DALL-E, Midjourney
  - ğŸŒ Russian multimodal foundation model

## ğŸ‡¨ğŸ‡³ Chinese AI Labs

- [x] Alibaba
  - ğŸ¤– Tongyi Qianwen (Qwen) series
  - ğŸ’­ Qwen3 (with thinking modes)
  - ğŸ‘ï¸ Qwen-VL (vision-language)
  - ğŸ”Š Qwen-TTS
  - ğŸ§ Qwen-Audio
  - ğŸ¯ Qwen3-Omni (multimodal)
  - ğŸ“¦ 100+ open-weight models
- [x] Baidu
  - ğŸ¤– ERNIE series (3.5, 4.0, 4.5)
  - ğŸ’­ ERNIE X1 (reasoning model)
  - ğŸ¤– ERNIE Bot
- [x] Baichuan AI
  - ğŸ¤– Baichuan 1, 2
  - ğŸ‘ï¸ Baichuan-Omni (multimodal, 7B)
  - ğŸ¤– Baichuan 4
- [x] Zhipu AI (z.ai)
  - ğŸ¤– GLM/ChatGLM series
  - ğŸ¤– GLM-4 series
  - ğŸ¤– GLM-4.5 (355B parameters)
  - âš¡ GLM-4.5 Air
- [x] 01.AI
  - ğŸ¤– Yi series (6B, 34B, 9B, 200K context)
  - ğŸ’¬ Yi chat models
  - ğŸ“ˆ Yi depth-upscaled models
  - ğŸ‘ï¸ Yi vision-language models
- [x] SenseTime
  - ğŸ¤– SenseNova series (1.0 through 5.0)
  - ğŸ’¬ SenseChat
  - ğŸ‘ï¸ SenseNova Multimodal
- [x] Rednote (Xiaohongshu)
  - ğŸ¤– dots.llm1 (first open-source foundation model)
  - âš ï¸ Limited documentation available
- [x] DeepSeek
  - ğŸ¤– DeepSeek-V3 (671B parameters, 37B activated)
  - ğŸ’­ DeepSeek-R1
  - ğŸ’­ DeepSeek-R1-Zero
  - ğŸ¤– DeepSeek V3.1
  - ğŸ’» DeepSeek Coder
- [x] Moonshot AI (Kimi)
  - ğŸ¤– Kimi K2 series (1 trillion parameters total, 32B activated)
  - ğŸ¤– Kimi-K2-Base
  - ğŸ’¬ Kimi-K2-Instruct
  - ğŸ’­ Kimi K2 Thinking
  - ğŸ”Š Kimi-Audio (open-source)
- [x] Tencent
  - ğŸ¤– Hunyuan-Large (389B parameters, 52B activated MoE)
  - ğŸ¤– Hunyuan Turbo S (fastest reasoning LLM)
  - ğŸ¤– Hunyuan-A13B (fine-grained MoE architecture)
  - ğŸ¨ Hunyuan text-to-image, 3D generation
  - ğŸ”Œ Integrated into 50+ Tencent products (Cloud, Games, WeChat, etc.)
  - ğŸ“Š Trained on diverse data, May 2024 major upgrade
  - ğŸ”“ Open-source Hunyuan-Large (Nov 2024)

## ğŸ‡°ğŸ‡· South Korean AI Labs

- [x] Naver Cloud
  - ğŸ¤– HyperCLOVA X series
  - ğŸ’­ HyperCLOVA X THINK (reasoning model, 6T tokens)
  - âš¡ HyperCLOVA X DASH (lightweight)
  - ğŸ“– HyperCLOVA X SEED (open-source)
  - ğŸ¯ Multimodal support (image, audio)
- [x] SK Telecom
  - ğŸ¤– A.X series (A.X 3.1 with 34B parameters)
  - âš¡ A.X 3.1 Lite
  - ğŸ¤– A.X 4 series (optimized for Korean business)
  - ğŸŒ Telco-specific LLM (partnership with Deutsche Telekom)
- [x] Upstage
  - ğŸ¤– Solar series
  - âš¡ Solar Mini (10.7B parameters)
  - ğŸ¤– Solar Pro (22B parameters)
  - ğŸ’­ Solar Pro 2 (31B parameters, reasoning mode)
- [x] LG AI Research
  - ğŸ¤– EXAONE series (300B multimodal)
  - ğŸ’­ EXAONE Deep (reasoning model)
  - ğŸ¤– EXAONE Deep-7.8B
  - âš¡ EXAONE Deep-2.4B
  - ğŸ¤– EXAONE Deep-32B
- [x] NC AI
  - âš ï¸ Involved in sovereign LLM development (government project)
  - âŒ No independent models documented
- [x] KT Corporation
  - ğŸ¤– Mi:dm (LLM)
  - âš ï¸ Limited public documentation
- [x] Samsung Electronics
  - ğŸ¤– Samsung Gauss series (Language, Code, Image)
  - ğŸ¯ Samsung Gauss 2 (multimodal, Compact/Balanced/Supreme variants)
- [x] Kakao Enterprise
  - ğŸ¤– KoGPT (6B parameters, Korean-specific)
  - ğŸ¤– KoGPT 2.0 (30B, 65B variants)
  - ğŸ¯ Honeybee (multimodal: text, images, video, audio, code)

## ğŸ‡¯ğŸ‡µ Japanese AI Labs

- [x] ELYZA Inc.
  - ğŸ¤– ELYZA-japanese-Llama-2 series (7B, 70B)
  - ğŸ’¬ ELYZA-japanese-Llama-2-7b-instruct
  - ğŸ¤– Llama-3-ELYZA-JP-8B
  - ğŸ“ˆ Llama-3-ELYZA-JP-120B (depth up-scaled)
  - ğŸ¥ ELYZA-LLM-Med
  - ğŸ’­ ELYZA-Thinking
  - âš¡ ELYZA-Shortcut
- [x] Cogent Labs
  - âš ï¸ No specific LLM foundation models found
  - Research focus on other AI areas
- [x] NTT
  - âš¡ Tsuzumi (lightweight LLM)
  - ğŸ¤– Tsuzumi 2 (next-generation)
  - ğŸ“‰ 600M parameters (300x lighter than GPT-3)
  - ğŸ¤– 7 billion parameters variant
  - ğŸ”§ Full-scratch development (not based on existing models)
- [x] SoftBank
  - ğŸ¤– Sarashina (460B parameters)
  - âš¡ Sarashina mini (70B, launching March 2026)
  - ğŸ”Œ Sarashina API
- [x] SB Intuitions
  - âš¡ Sarashina mini (70B parameters)
  - ğŸ”§ Developed using model distillation
  - ğŸ”Œ Sarashina API (Chat Completion, Embeddings)
- [x] ğŸ‡¯ğŸ‡µ Rakuten
  - ğŸ¤– Rakuten AI 2.0 (MoE architecture)
  - ğŸ¤– Rakuten AI 2.0 mini (SLM)
  - ğŸŒ Optimized for Japanese language
  - ğŸ“¦ Foundation and instruct model versions
  - ğŸ¯ Fine-tuned and specialized variants
  - ğŸ“… Released Dec 2024, updated Feb 2025

## ğŸ‡®ğŸ‡³ Indian AI Labs

- [x] Sarvam
  - ğŸ¤– Sarvam-1 (2B parameters, 10 Indian languages)
  - ğŸ¤– Sarvam-2B
  - ğŸ›ï¸ Sovereign LLM project (under IndiaAI Mission)
  - ğŸ”§ Sarvam-Large, Sarvam-Small, Sarvam-Edge variants (in development)
  - ğŸ’ª 4,096 H100 GPUs allocated for sovereign model
- [x] Krutrim (Ola)
  - ğŸ¤– Krutrim (base LLM, 2 trillion tokens)
  - ğŸ—£ï¸ Supports 22 Indian languages
  - ğŸ¤– Llama 4 models (first deployment on domestic servers)
  - ğŸ’­ DeepSeek R1 671B (deployed on H100s in India)
  - ğŸ¢ In-house cloud infrastructure
  - ğŸ”Œ India's first AI chips
- [x] Uniphore
  - âš ï¸ No foundation models
  - ğŸ’¼ Enterprise AI cloud platform (Business AI Cloud)
  - ğŸ”§ Infrastructure for open and open-source LLMs
  - âŒ NOT a foundation model builder
- [x] CoRover
  - ğŸ¤– BharatGPT-3B-Indic (sovereign model for Indic languages)
  - âš¡ BharatGPT Mini (534M parameters, offline capable)
  - ğŸ—£ï¸ Support for 14-22 Indian languages
  - ğŸ¯ Multimodal capabilities

## ğŸ‡¸ğŸ‡¬ Singaporean AI Labs

- [x] WIZ.AI
  - Bahasa Indonesian LLM (13B parameters)
  - 7B Foundation Model
  - Domain-specific Enterprise AI systems
  - Planning Thai language support
- [x] AI Singapore
  - SEA-LION series
  - SEA-LION v4 (multimodal, Gemma 3 27B based)
  - Supports 11 Southeast Asian languages
  - 500 billion tokens training corpus
  - Open-source multilingual model
- [x] ğŸŒ SEA AI Lab / Singapore University of Technology
  - ğŸ¤– Sailor series (0.5B to 20B parameters)
  - ğŸ¤– Sailor2 (improved multilingual)
  - ğŸŒ Languages: English, Chinese, Vietnamese, Thai, Indonesian, Malay, Lao
  - ğŸ”“ Based on Qwen architecture
  - ğŸ“„ Published at EMNLP 2024
- [x] ğŸŒ Alibaba DAMO Academy (Southeast Asia focus)
  - ğŸ¤– SeaLLMs (7B, 13B parameters)
  - ğŸŒ 12+ Southeast Asian languages
  - ğŸ”“ Open-sourced on Hugging Face
  - â„¹ï¸ Note: DAMO is Alibaba's research arm (main entry under Chinese labs)

## ğŸ‡¦ğŸ‡º Australian AI Labs

- [x] ğŸ‡¦ğŸ‡º Kangaroo LLM Project
  - ğŸ¤– Kangaroo (Australia's flagship sovereign LLM)
  - ğŸ¦˜ Tailored to Australian English, culture, humor, slang
  - ğŸ¤ Nonprofit consortium project
  - âš ï¸ Status: In development (delayed from Oct 2024)

- [x] ğŸ‡¦ğŸ‡º Isaacus / Kanon 2 (Australia)
  - âš–ï¸ Legal AI foundation model
  - ğŸ“š Coverage: 38 jurisdictions
  - ğŸ† Beats OpenAI/Google on legal retrieval
  - ğŸ¯ Australian specialized legal LLM

- [x] ğŸ‡¦ğŸ‡º CSIRO (Commonwealth Scientific and Industrial Research Organisation)
  - ğŸ”¬ Australia's national science agency
  - ğŸ“Š Published major 2024 report on foundation models
  - ğŸ›ï¸ Researching sovereign AI capability
  - ğŸ”§ Infrastructure and strategy focus

## ğŸŒ South American & Latin American AI Labs

- [x] ğŸ‡²ğŸ‡½ Mexico IA / Mexican National LLM (NVIDIA + CCE + Mexican government)
  - ğŸ¤– NVIDIA + Mexican government partnership
  - ğŸ›ï¸ National AI Lab (April 2025)
  - ğŸ‡²ğŸ‡½ "ChatGPT in Spanish made in Mexico"
  - ğŸ¯ Mexican national AI initiative

- [x] Brazil's National LLM Projects
  - ğŸ¤– Amazonia AI (largest Brazilian LLM)
  - ğŸ¤– Sabia/Maritaca AI (Portuguese, Brazilian culture)
  - ğŸ¤– AmazonIA
  - ğŸ’» Runs on Oracle data centers in Brazil
  - âš–ï¸ Complies with LGPD (Brazilian data protection)

## ğŸŒ African AI Labs

- [x] Lelapa AI
  - ğŸ¤– InkubaLM (Africa's first multilingual LLM)
  - ğŸ¤– InkubaLM-0.4B (1.9 billion tokens, 5 African languages)
  - ğŸ—£ï¸ Support for Swahili, Yoruba, IsiXhosa, Hausa, isiZulu
  - ğŸ¤ Hybrid approach with linguists and local communities
  - ğŸ’° Backed by Mozilla Ventures and Atlantica Ventures
- [x] Awarri
  - ğŸ¤– N-ATLAS (Nigeria's first government-backed LLM)
  - ğŸ—£ï¸ Support for Yoruba, Hausa, Igbo, Nigerian-accented English
  - ğŸ“– Open-source commitment
  - ğŸ‘¥ LangEasy platform for crowdsourced training
- [x] ğŸ‡¿ğŸ‡¦ğŸ‡°ğŸ‡ªğŸ‡³ğŸ‡¬ Jacaranda Health / UlizaLlama (Africa)
  - ğŸ¤– UlizaLlama - open-source African health LLM
  - ğŸ—£ï¸ 5 African languages (Swahili, Hausa, Yoruba, Xhosa, Zulu)
  - ğŸ¥ Health-focused foundation models
  - ğŸŒ Expanding across African markets (2025)
  - ğŸ“– Open-source commitment

## ğŸŒ Middle Eastern & North African AI Labs

- [x] ğŸ‡¸ğŸ‡¦ ALLaM (SDAIA - Saudi Data and AI Authority)
  - ğŸ¤– Arabic-first LLM (Meta Llama-2 based)
  - ğŸ“Š 540B+ Arabic tokens training
  - ğŸ›ï¸ Saudi Vision 2030 initiative
  - ğŸ‡¸ğŸ‡¦ Saudi national model

- [x] ğŸ‡¸ğŸ‡¦ Mulhem (SDAIA, Saudi Arabia)
  - ğŸ¤– Open-source Arabic-first LLM
  - ğŸŒ Arabic language preservation
  - ğŸ‡¸ğŸ‡¦ Saudi open-source initiative

- [x] ğŸ‡¸ğŸ‡¦ METABRAIN (stc Group, Saudi Arabia)
  - ğŸ¤– Multimodal AI platform
  - ğŸ“Š Text, images, data analytics
  - ğŸ¢ Saudi telco (stc Group)
  - ğŸ‡¸ğŸ‡¦ Saudi enterprise model

- [x] ğŸ‡¶ğŸ‡¦ Fanar (Hamad Bin Khalifa University + QCRI)
  - ğŸ¤– Fanar Star and Fanar Prime
  - ğŸ“Š 1 trillion tokens (Arabic, English, code)
  - ğŸ¯ Qatar's national LLM

- [x] ğŸ‡©ğŸ‡¿ğŸ‡²ğŸ‡¦ğŸ‡¹ğŸ‡³ Hadretna (Fentech, Algeria)
  - ğŸ¤– Algerian Arabic dialect (Daridja) + Berber (Tamazight)
  - ğŸŒ 96M speakers (Algeria, Morocco, Tunisia)
  - ğŸ‘¥ Crowdsourced community data
  - ğŸ“… Version 2 expected early 2025

- [x] ğŸ‡®ğŸ‡· PersianLLaMA (Iran)
  - ğŸ¤– First large Persian language model
  - ğŸ“Š 7B and 13B parameters
  - ğŸ‡®ğŸ‡· Iranian national model

- [x] ğŸ‡®ğŸ‡· PersianMind (Iran)
  - ğŸ¤– Cross-lingual Persian-English LLM
  - ğŸ“ˆ 9% improvement over LLaMA2-7B
  - ğŸ‡®ğŸ‡· Persian bilingual model

- [x] ğŸ‡®ğŸ‡· Maral-7B (MaralGPT, Iran)
  - ğŸ¤– 7B bilingual Persian-English model
  - ğŸ‡®ğŸ‡· Persian commercial LLM

- [x] ğŸ‡¦ğŸ‡ª G42
  - ğŸ¤– JAIS 70B (Arabic-focused model)
  - ğŸ¤– 20+ additional Arabic models
  - ğŸ“Š 370B tokens training (330B Arabic tokens)
  - ğŸ¯ Multimodal foundation models (with Core42, Inception)
  - ğŸ¤ Partnership with Mistral AI
  - ğŸ¤– K2 Think (reasoning model, 2025)
- [x] ğŸ‡¦ğŸ‡ª AIQ
  - âš ï¸ No foundation models for LLMs
  - âš¡ Energy data and analytics platform (AD.WE with Presight)
  - âŒ NOT a foundation model builder
- [x] ğŸ‡¦ğŸ‡ª Mozn
  - ğŸ—£ï¸ Arabic Natural Language Understanding engine
  - ğŸŒ Focus on 2 billion Arabic speakers
  - ğŸ¯ Vision: World's largest and most effective Arabic AI models
  - ğŸ’° Raised $10M Series A funding
- [x] ğŸ‡¸ğŸ‡¦ Humain (Saudi Arabia)
  - ğŸ¤– Allam (first Arabic foundation model)
  - ğŸ’° $100+ billion investment commitment
  - ğŸ›ï¸ Backed by Public Investment Fund
  - ğŸ¯ Target: Top AI company globally by 2030
- [x] ğŸ‡®ğŸ‡± AI21 Labs
  - ğŸ¤– Jurassic-2 (J2)
  - ğŸ¤– Jamba family
  - âš¡ Jamba 1.5 Mini
  - ğŸ¤– Jamba 1.5 Large
  - ğŸ“ 256K context window models

### Commercial Tier (17)

**Strong foundation models for specialized domains. Competitive but focused on specific use cases or markets.**

- [x] ğŸ‡ºğŸ‡¸ Reka AI
  - ğŸ¤– Reka Core (largest, frontier model)
  - âš¡ Reka Flash (21B parameters, fast)
  - âš¡ Reka Edge (7B parameters, efficient)
  - ğŸ¯ Multimodal: text, image, video, audio
  - ğŸ“ 128K context window
  - ğŸŒ 20+ languages
  - ğŸ‘¥ Founded by DeepMind/Google Brain/FAIR alumni
- [x] ğŸ‡ºğŸ‡¸ Writer
  - ğŸ¤– Palmyra X 004 (flagship model)
  - ğŸ‘ï¸ Palmyra Vision (multimodal)
  - ğŸ¨ Palmyra Creative (content generation)
  - ğŸ¤– Palmyra X5 (1M context window)
  - ğŸ¥ Palmyra-Med-70B (medical domain)
  - ğŸ› ï¸ First on tool-calling benchmarks
  - ğŸ¢ Enterprise AI focus
- [x] ğŸ‡ºğŸ‡¸ Inflection AI
  - ğŸ¤– Inflection-1 (foundation model)
  - ğŸ¤– Inflection-2.5 (nearly matches GPT-4)
  - ğŸ’¬ Powers Pi chatbot
  - ğŸ‘¥ Founded by Mustafa Suleyman (DeepMind co-founder)
  - âš ï¸ Note: Team mostly moved to Microsoft in 2024
- [x] ğŸ‡ºğŸ‡¸ Adept AI
  - ğŸ¤– ACT-1 (Action Transformer)
  - ğŸ¤– Large Action Models (LAMs)
  - ğŸ”§ Pioneer in action-focused models
  - ğŸ–¥ï¸ Software automation and tool use
  - ğŸ¯ Desktop and web application control
- [x] ğŸ‡ºğŸ‡¸ Databricks
  - ğŸ¤– DBRX (132B total, 36B active parameters)
  - ğŸ’¬ DBRX Instruct
  - ğŸ¤– DBRX Base
  - ğŸ“Š 12T tokens training
- [x] ğŸ‡ºğŸ‡¸ Snowflake
  - ğŸ¤– Arctic LLM (480B parameters, 17B active)
  - ğŸ”Œ Snowflake Cortex AISQL
  - ğŸ“– Apache 2.0 licensed
  - âš™ï¸ Mixture-of-experts architecture
- [x] ğŸ‡ºğŸ‡¸ Cerebras
  - ğŸ¤– Cerebras-GPT family (111M-13B)
  - ğŸ¥ Mayo Clinic Genomic Foundation Model
  - âš™ï¸ Chinchilla scaling laws compliance
  - ğŸ”§ Domain-specific fine-tuning support
- [x] ğŸ‡ºğŸ‡¸ Salesforce
  - ğŸ¤– XGen-7B (8K sequence length, 1.5T tokens)
  - ğŸ‘ï¸ XGen-MM (multimodal LMMs)
  - ğŸ¤– XGen-Sales (autonomous sales tasks)
  - ğŸ¤– xLAM (Large Action Models)
  - ğŸ’» XGen-Code (developer-focused, 2025)
  - ğŸ”’ Stringent data privacy controls
  - ğŸ¢ Powers Agentforce enterprise AI
- [x] ğŸ‡ºğŸ‡¸ IBM
  - ğŸ¤– Granite 3.0 (1B-8B parameters, Oct 2024)
  - ğŸ¤– Granite 3.3
  - ğŸ¤– Granite 4.0 (hybrid transformer-Mamba, Oct 2025)
  - ğŸ¤– Granite 4.0 Nano (browser-capable)
  - ğŸ¢ watsonx platform for enterprise AI
  - ğŸ“‹ ISO 42001 certified
  - ğŸ“Š Trained on trusted enterprise data (code, legal, finance)
  - ğŸ”“ Multi-platform distribution (Hugging Face, Docker, Ollama)
- [x] ğŸ‡ºğŸ‡¸ ServiceNow
  - ğŸ¤– Apriel-1.5-15b-Thinker (reasoning models)
  - ğŸ¤– Now LLM (generative LLM for enterprise)
  - ğŸ’» StarCoder2 (code generation, partnership with Hugging Face/NVIDIA)
  - ğŸ¤– Apriel Nemotron 15B (with NVIDIA, agentic tools)
  - ğŸ”§ Fast-LLM framework (open-source training library)
  - ğŸ¢ Enterprise domain-specific LLMs
  - ğŸ“… Q2 2025 availability for customers
- [x] ğŸ‡ºğŸ‡¸ Character.AI
  - ğŸ¤– Proprietary LLM technology
  - ğŸ’¬ Character-optimized conversational AI
  - ğŸ¯ Licensed to Google (non-exclusive)
  - ğŸ¢ Full-stack AI company
  - ğŸ’° $350M+ funding
- [x] ğŸ‡ºğŸ‡¸ Perplexity
  - ğŸ¤– Custom LLM technology
  - ğŸ” AI search and knowledge discovery
  - ğŸ’¬ Multi-model synthesis (own + partner models)
  - ğŸ’° $20B valuation, $100M+ ARR
  - ğŸŒ Multi-language support
- [x] ğŸ‡ºğŸ‡¸ Runway
  - ğŸ¬ Foundation models for creative AI
  - ğŸ¨ Gen-2 (video generation)
  - ğŸ¨ Runway Custom Models
  - ğŸ’° $308M Series D, $3B valuation
  - ğŸ¯ Generative tools for creators
- [x] ğŸ‡ºğŸ‡¸ Nomic AI
  - ğŸ¤– Nomic Embed Text V2 (Feb 2025)
  - ğŸ¤– Nomic Embed Multimodal (April 2025)
  - ğŸ“Š Embedding foundation models
  - ğŸ“Š Atlas platform for AI data visualization
  - ğŸ”“ Open-source approach
- [x] ğŸ‡ºğŸ‡¸ Chai AI
  - ğŸ¤– In-house LLM for social engagement
  - ğŸ’¬ Social AI chatbot platform
  - ğŸ“ˆ +10% engagement from custom models
  - ğŸ¯ Community-driven AI companions
- [x] ğŸ‡ºğŸ‡¸ Replika
  - ğŸ¤– Internally developed LLMs
  - ğŸ’­ Emotional connection + personalization
  - ğŸ¯ AI companion focus
  - ğŸ‘¥ 30M+ users

- [x] ğŸ‡ºğŸ‡¸ Inworld AI
  - ğŸ¤– Emotionally intelligent AI character models
  - ğŸ® Game developer + narrative focus
  - ğŸ™ï¸ Real-time voice, facial expressions
  - ğŸ¬ Character generation LLMs
  - ğŸ¢ Interactive media foundation models

## ğŸ‡¬ğŸ‡§ UK Commercial AI Companies

- [x] ğŸ‡¬ğŸ‡§ Faculty AI (UK)
  - ğŸ¤– Bespoke enterprise LLMs
  - ğŸ’¼ Custom foundation models using PyTorch Lightning
  - ğŸ¢ UK business-focused models
  - ğŸ‡¬ğŸ‡§ UK commercial LLM builder

## ğŸŒ Other Commercial Companies

**Note: Companies with unclear status or limited information. Primarily consulting/platforms.**

- [x] ğŸ‡·ğŸ‡º Yandex
  - ğŸ¤– YandexGPT 5.1 Pro (launched Aug 2025 for business, 32K context window, cost reduced 3x)
  - ğŸ¤– YandexGPT 5 Lite (available via API)
  - ğŸ¨ YandexART 2.0 (text-to-image, 110B image-text training, hybrid CNN-transformer)
  - ğŸ’¬ Alice assistant (uses YandexGPT 3+ family)
  - ğŸŒ Russian frontier foundation models
- [x] Contextual AI
  - ğŸ”§ RAG 2.0 platform (Retrieval-Augmented Generation)
  - ğŸ’¼ Enterprise LLM customization and fine-tuning
  - ğŸ“Š Back-propagation optimization for retriever + LLM
  - ğŸ’° $80M Series A (includes NVIDIA investment)
  - âŒ Does NOT build foundation models (customizes existing ones)
- [x] MosaicML
  - ğŸ¤– MPT-7B (3M+ downloads, most downloaded open LLM)
  - ğŸ¤– MPT-30B (major foundation model)
  - ğŸ¤– MPT series (full lineup open-source models)
  - ğŸ“Š Trained on cutting-edge hardware efficiency
  - ğŸ“… Acquired by Databricks (July 2023)
  - ğŸ”„ Integrated into Mosaic AI platform
- [x] Hugging Face
  - ğŸ¤– SmolVLM family (Vision-Language Models):
    - SmolVLM-256M (world's smallest VLM, Jan 2025, runs on <1GB GPU)
    - SmolVLM-500M
    - SmolVLM-2B (open-source)
  - ğŸ¤– SmolLM series (efficient language models)
  - ğŸ“Š FineVision dataset (24M samples for VLM training, Sept 2025)
  - ğŸ›ï¸ Model hub/platform (500K+ hosted models) + foundation model builder
  - ğŸ”“ 100% open-source focus, democratizing AI

## ğŸ”¬ Research Labs (University & Private)

- [x] Berkeley Artificial Intelligence Research (BAIR) Lab
  - Research focus on foundation models
  - Koala model
  - Privacy-aware foundation models research
  - Visual foundation models work
- [x] Stanford AI Lab
  - ğŸ›ï¸ Center for Research on Foundation Models (CRFM)
  - ğŸ¤– Marin-8B (2024-2025, fully open-source, Apache 2.0)
    - Complete research pipeline (code, datasets, training logs transparent)
    - Built with JAX and Levanter framework
    - Trained on 12 trillion tokens
  - ğŸ“‹ CLMBR (clinical language models, 141M parameters)
  - ğŸ¤– Robotics foundation models (ILIAD)
  - ğŸ¯ Specialized domains: law, music, robotics, biomedicine
  - ğŸ‘¨â€ğŸ’¼ New director (2025): Carlos Guestrin
- [x] MIT (CSAIL & Media Lab)
  - Self-learning language models research
  - Co-LLM algorithm (expert LLM collaboration)
  - PRoC3S framework (robotics with foundation models)
  - Large Language Model for Mixed Reality (LLMR)
- [x] Carnegie Mellon University (CMU)
  - ğŸ›ï¸ CMU Foundation and Language Model Center (FLAME)
  - ğŸ¤– FLAME-MoE (released 2025) - suite of 7 decoder-only models (38M - 1.7B active parameters, fully open)
  - ğŸ§® Llemma (math foundation model)
  - ğŸ™ï¸ OWSM & OWSM-CTC (speech models, 180K hours audio, multilingual)
  - ğŸ“„ XL-Net (long-document modeling)
  - ğŸ’» Polycoder (2.7B code generation, 12 languages)
  - ğŸ”§ MLC LLM (universal deployment)
  - ğŸ”€ FlexFlow Serve, Sotopia (multi-agent research)
- [x] University of Toronto
  - "Large Models" course (Llama 3 deep dive)
  - LLM research for Canadian political transparency
  - Focus on uncertainty estimates and scalability
- [x] Allen Institute for AI (AI2)
  - ğŸ¤– OLMo series (7B, 13B, 32B)
  - ğŸ¤– OLMo 2 (trained to 6T tokens, 32B outperforms GPT-3.5-Turbo)
  - ğŸ¤– OlmoEarth (ğŸŒ multimodal earth observation, released Nov 4, 2025)
    - First open end-to-end satellite/sensor data solution
    - Trained on 10TB+ earth observation data
  - ğŸ¨ Molmo (multimodal models)
  - ğŸ”§ Tulu 3 (leading instruction-following, fully open-source recipes)
- [x] EleutherAI
  - GPT-Neo (125M, open-source GPT-3 alternative)
  - GPT-J (6B parameters)
  - GPT-NeoX (20B parameters)
  - The Pile (800GB text corpus)
  - Focus shifted to interpretability and alignment
- [x] ğŸ¤ BigScience / BLOOM
  - ğŸ¤– BLOOM (176B parameters)
  - ğŸŒ 46 natural languages + 13 programming languages
  - ğŸ‘¥ 1000+ researchers collaboration
  - ğŸ¤ Led by HuggingFace
  - ğŸ–¥ï¸ Trained on Jean Zay supercomputer (France)
  - ğŸ”“ Fully open-source
- [x] ğŸ‡ºğŸ‡¸ Together AI
  - ğŸ“Š RedPajama V1 (1.2T tokens training data)
  - ğŸ“Š RedPajama V2 (30T+ tokens, used by 500+ models)
  - ğŸ”§ GPU infrastructure (10K+ clusters) for model training
  - ğŸ¢ Open-source research organization with 200+ hosted models
  - âš ï¸ Note: Infrastructure/dataset provider, not proprietary foundation model builder
- [x] ğŸ‡ºğŸ‡¸ LMSYS (UC Berkeley)
  - ğŸ¤– Vicuna (7B, 13B, 33B)
  - ğŸ† Chatbot Arena (1.5M+ votes)
  - âš¡ FastChat platform
  - ğŸ“Š LMSYS-Chat-1M dataset
  - ğŸ”¬ LLM evaluation and benchmarking
- [x] Technology Innovation Institute (TII) - Abu Dhabi ğŸ‡¦ğŸ‡ª
  - ğŸ¤– NOOR (largest Arabic LLM)
  - ğŸ¤– Falcon series (40B parameters, 1T tokens training)
  - ğŸ¤– Falcon Mamba 7B (2024)
  - ğŸ¤– Falcon Arabic (released May 2025) - first Arabic in Falcon series, native Arabic + dialects
  - ğŸ¤– Falcon-H1 (2025) - hybrid Transformer-Mamba architecture, faster inference
  - ğŸ›ï¸ Digital Science Research Centre
  - ğŸ”“ Falcon Foundation (non-profit for open-source)
- [x] Occiglot - German/EU ğŸ‡©ğŸ‡ª
  - ğŸ¤– Occiglot-7B-EU5 (Mistral-7B based) - English, Spanish, French, German, Italian
  - ğŸ¤– Llama-3-8B based models (2025) - continually pre-trained on German tokens
  - ğŸŒ Planning all 24 official EU languages + regional languages
  - ğŸ“Š Built on Community OSCAR multilingual datasets
  - ğŸ›ï¸ Led by DFKI (German Research Center for AI) and hessian.AI
  - ğŸ¯ Focus on robust generalization across European languages
- [x] LatamGPT - ğŸŒ Latin America
  - ğŸ¤– Latam-GPT (50B parameters, LLaMA-based, ~GPT-3.5 comparable)
  - ğŸ“Š Trained on 8TB regional data (2.6M documents from 20 LAC countries + Spain)
  - ğŸ’¬ Spanish, Portuguese, English + Indigenous languages (Rapa Nui translator)
  - ğŸ¤ Collaboration across 30+ institutions, 12 countries
  - ğŸ“… Beta: October 2025 | Official Launch: December 2025
  - ğŸ›ï¸ Led by CENIA (Chile's National Center for AI)
- [x] ğŸ‡¦ğŸ‡ª Mohamed bin Zayed University (MBZUAI) - Abu Dhabi
  - ğŸ›ï¸ Institute of Foundation Models (IFM) - launched May 2025 with Silicon Valley, Paris, Abu Dhabi labs
  - ğŸ¤– Jais (world's most advanced Arabic LLM, with G42 & Cerebras)
  - ğŸ¨ Multimodal foundation models for healthcare, finance, environment
  - ğŸ“Š LLM360 initiative (fully transparent development with code/data/checkpoints)
  - ğŸ”“ Leading transparent AI development globally
- [x] University of Oxford
  - LLM research replication benchmarks
  - LLM alignment and diversity research (Microsoft AFMR partnership)
  - Oxford LLMs program (social science applications)
  - Focus on model interpretability and fair representation
- [x] ğŸ‡©ğŸ‡ª Technical University of Munich (TUM)
  - ğŸ§¬ Nicheformer (published Nature Methods 2025)
    - First large-scale foundation model integrating single-cell + spatial transcriptomics
    - Trained on 110M+ cells
    - Consistently outperforms existing approaches
    - Co-developed with Helmholtz Munich
  - ğŸ§¬ Nucleotide Transformer (with NVIDIA & InstaDeep)
    - Collection of 4 DNA LLMs (500M - 2.5B parameters)
    - Largest model trained on 850+ species genetic data
  - ğŸ”¬ Software Engineering & AI chair LLM research
  - ğŸ¨ CompAI Lab (computational imaging + foundation models)
- [x] ğŸ‡¨ğŸ‡­ ETH Zurich + EPFL - Swiss AI Initiative
  - ğŸ¤– Apertus (released Sept 2, 2025) - 8B and 70B multilingual models
    - 70B ranks among most powerful fully open models worldwide
    - 15 trillion tokens across 1000+ languages
    - 40% non-English (includes Swiss German, Romansh, underrepresented languages)
  - ğŸŒ Fully open: weights, data, recipes, architecture ("Apertus" = Latin for "open")
  - ğŸ“Š Swiss AI Initiative: 800+ researchers, 20M+ GPU hours/year
  - ğŸ›ï¸ Partnership: ETH AI Center, EPFL AI Center, CSCS (Swiss National Supercomputing)
  - ğŸ’¼ Available via: Swisscom, Hugging Face, Public AI network
- [x] National University of Singapore (NUS)
  - LLM research lab (collaboration with MIT, Stanford, Google)
  - ScholAIstic (educational LLM with RAG)
  - Focus on practical LLM applications in education/nursing
- [x] Max Planck Institute for Informatics
  - ELLIS-LAION Workshop on Foundation Models
  - LLM research across multiple institutes
  - Focus on technical improvements and societal implications
  - Brain science + foundation models symposium
- [x] ğŸ‡ªğŸ‡º European Laboratory for Learning and Intelligent Systems (ELLIS)
  - ğŸŒ Pan-European AI network (43 sites, 16 research programs, 16 countries)
  - ğŸ“ ELLIS PhD & Postdoc Program
  - ğŸ† ELLIOT Project (July 2025 - June 2029, â‚¬25M Horizon Europe)
    - European Large Open Multi-Modal Foundation Models for Robust Generalization
    - 30 partners across 12 countries
    - Focus: Fairness, explainability, privacy, EU AI regulation alignment
    - Early-career researcher training component
  - ğŸ“š ELLIS Winter School on Foundation Models
  - ğŸ¯ Focus on robust generalization across real-world data
- [x] Next Realm AI Innovation Lab
  - Researching LLMs, Transformers, Foundation Models
  - AGI research via JEPA and energy-based models
  - Focus on practical generative AI solutions
  - Collaboration with academic and commercial institutions

## ğŸ¯ Specialized Foundation Model Builders (Underserved Languages/Modalities)

**Note: These companies build proprietary foundation models for underserved languages and specialized modalities. Unlike general-purpose LLM builders, they focus on filling important gaps in linguistic diversity and new modalities (speech, translation).**

- [x] ğŸ‡¿ğŸ‡¦ Botlhale AI
  - ğŸ¤– Proprietary speech recognition & NLP models for African languages
  - ğŸ—£ï¸ Supports 11 South African languages (English, IsiZulu, IsiXhosa, Afrikaans, Sesotho, Setswana, Sepedi, others)
  - ğŸ’¼ SaaS platform for enterprise call centers
  - ğŸ”§ Conversational AI, voice-to-text, text-to-voice capabilities
  - ğŸŒ Expanding to Ghana, Kenya, Nigeria (2025)
  - ğŸ† First to bring speech models to underserved African languages

- [x] ğŸ‡©ğŸ‡ª Lesan AI
  - ğŸ¤– Proprietary machine translation models (specialized, not general-purpose)
  - ğŸ—£ï¸ Focuses on Ethiopian/Eritrean languages: Tigrinya, Amharic, English
  - ğŸ† Outperforms Google Translate and Microsoft Translator on low-resource languages
  - ğŸ”§ Transformer-based with back-translation approach
  - ğŸŒ 10M+ translations served, community-based data collection
  - ğŸ¤ Partnerships with Dalberg, i4policy, UNICEF

---

## ğŸ“Š Summary

### âœ… Research Complete! (LLM Foundation Models)

**ğŸ“ˆ Total LLM Entries Researched: 190+** (organized by geography + commercial type)

**âœ¨ Companies/Labs WITH LLM Foundation Models: ~175+** (verified to have their own LLMs/foundation models)

**ğŸ“š See non-llm-overview.md for:** Video, image, audio, robotics, and scientific foundation models

**North America:**
- ğŸ‡ºğŸ‡¸ US Frontier AI Companies (9): Google, OpenAI, Microsoft, Meta, Amazon, NVIDIA, Apple, **Anthropic, xAI**
- ğŸ‡ºğŸ‡¸ US Commercial AI Companies (17): Reka AI, Writer, Inflection AI, Adept AI, Databricks, Snowflake, Cerebras, Salesforce, IBM, ServiceNow, Character.AI, Perplexity, Runway, Nomic AI, Chai AI, Replika, Inworld AI
- ğŸ‡¬ğŸ‡§ UK Commercial (1): Faculty AI
- ğŸ‡¨ğŸ‡¦ Canadian AI Labs (4): Mila, Vector Institute, Recursal AI, Cohere

**Europe (Expanded):**
- ğŸ‡ªğŸ‡º European AI Labs - Core (13): ğŸ‡«ğŸ‡· Kyutai, ğŸ‡©ğŸ‡ª Black Forest Labs, ğŸ‡©ğŸ‡ª LAION, EuroLLM, OpenEuroLLM, ğŸ‡¬ğŸ‡§ Stability AI, ğŸ‡«ğŸ‡· Mistral AI, ğŸ‡©ğŸ‡ª Aleph Alpha, ğŸ‡«ğŸ‡· LightOn, ğŸ‡®ğŸ‡¹ iGenius, ğŸ‡«ğŸ‡® Silo AI, ğŸ‡«ğŸ‡· Bioptimus, ğŸ‡©ğŸ‡ª Nyonic
- ğŸ‡ªğŸ‡º Extended European Labs (17): ğŸ‡¬ğŸ‡§ BritLLM, ğŸ‡©ğŸ‡ª OpenGPT-X/Teuken, ğŸ‡©ğŸ‡ª LeoLM, ğŸ‡«ğŸ‡· LUCIE, ğŸ‡³ğŸ‡± GPT-NL, ğŸ‡³ğŸ‡± Geitje, ğŸ‡¸ğŸ‡ª GPT-SW3, ğŸ‡³ğŸ‡´ NorskGPT, ğŸ‡®ğŸ‡¹ Minerva, ğŸ‡®ğŸ‡¹ Domyn, ğŸ‡ªğŸ‡¸ Spanish National LLM, ğŸ‡ªğŸ‡¸ Clibrain, ğŸ‡µğŸ‡± PLLuM, ğŸ‡µğŸ‡± Bielik, ğŸ‡¹ğŸ‡· Kumru AI, ğŸ‡¹ğŸ‡· T3AI, ğŸ‡ºğŸ‡¦ Lapa LLM, ğŸ‡®ğŸ‡ª UCCIX, ğŸ‡®ğŸ‡¸ MiÃ°eind

**Asia-Pacific (Greatly Expanded):**
- ğŸ‡¨ğŸ‡³ Chinese AI Labs (9): Alibaba, Baidu, Baichuan, Zhipu, 01.AI, SenseTime, DeepSeek, Moonshot, Tencent
- ğŸ‡¹ğŸ‡¼ Taiwan (4): TAIDE, Formosa, FoxBrain, Llama-3-Taiwan (TAME)
- ğŸ‡­ğŸ‡° Hong Kong (2): HKGAI V1, InvestLM
- ğŸ‡°ğŸ‡· South Korean (8): Naver Cloud, SK Telecom, Upstage, LG AI, NC AI, KT Corp, Samsung, Kakao
- ğŸ‡¯ğŸ‡µ Japanese (6): ELYZA, Cogent Labs, NTT, SoftBank, SB Intuitions, Rakuten
- ğŸ‡®ğŸ‡³ Indian (4): Sarvam, Krutrim, Uniphore, CoRover
- ğŸ‡¸ğŸ‡¬ SEA/Singapore (4): WIZ.AI, AI Singapore, SEA AI Lab, Alibaba DAMO

**Middle East & North Africa (Expanded):**
- ğŸ‡¸ğŸ‡¦ Saudi Arabia (3): ALLaM, Mulhem, METABRAIN
- ğŸ‡¦ğŸ‡ª UAE (3): G42, TII (Falcon), MBZUAI (Jais)
- ğŸ‡¶ğŸ‡¦ Qatar (1): Fanar
- ğŸ‡·ğŸ‡º Russia (2): Sber/GigaChat, Kandinsky
- ğŸ‡®ğŸ‡· Iran (3): PersianLLaMA, PersianMind, Maral
- ğŸ‡©ğŸ‡¿ğŸ‡²ğŸ‡¦ğŸ‡¹ğŸ‡³ North Africa (1): Hadretna
- ğŸ‡¦ğŸ‡ª Other ME (1): AI21 Labs (Israel)

**Africa:**
- ğŸ‡¿ğŸ‡¦ South Africa (1): Botlhale AI
- ğŸ‡ªğŸ‡¹ Ethiopia (1): Lesan AI
- ğŸŒ Pan-African (3): Lelapa AI, Awarri, Jacaranda Health/UlizaLlama

**South America & Latin America:**
- ğŸ‡²ğŸ‡½ Mexico (1): Mexico IA National LLM
- ğŸ‡§ğŸ‡· Brazil (1): Brazil's National LLM Projects
- ğŸŒ Multi-country (1): LatamGPT

**Oceania:**
- ğŸ‡¦ğŸ‡º Australian (3): Kangaroo LLM, Isaacus/Kanon 2, CSIRO

**Research Labs & Universities (24+):**
Berkeley BAIR, Stanford (Marin-8B), CMU (FLAME-MoE), MIT, Allen AI (OLMo/OlmoEarth), EleutherAI, BigScience/BLOOM, Together AI, LMSYS, TII, MBZUAI, Occiglot, LatamGPT, Oxford, ETH Zurich (Apertus), Freiburg (TabPFN), ELLIS, TUM (Nicheformer), and 6+ others

**ğŸŒ Other Commercial:**
- ğŸ‡·ğŸ‡º Yandex (Russian frontier model)
- Contextual AI (RAG platform), MosaicML (acquired), Hugging Face (now builds models too)

**âŒ NOT Foundation Model Builders (~25):**
- ğŸ’¼ Consulting/Services: Springs, WhyLabs, Dextralabs, InData Labs, LeewayHertz, Markovate, SoluLab, Rain Infotech, Debut, TechAhead, Vstorm, Sunrise, Blue Crystal, AI Advancements, HatchWorks, Dualboot, Rootstrap, VulaVula, Mymanu, GotBot, TrainingData, LMTD, Saal.ai
- ğŸ”§ Platforms/Non-Builders: Uniphore, AIQ, Play.ht (moved to specialized)

**âœ… All items now marked as [x] COMPLETE**
