# GPU and AI Chip Kernel Development: History and Technical Exploration

## Evolution of GPU Kernel Programming: From Graphics to General Compute

Graphics Processing Units (GPUs) began as fixed-function chips in the 1990s dedicated to accelerating 3D graphics tasks (transformations, lighting, rasterization). Early GPUs had a fixed graphics pipeline, handling specific stages with hardwired logic. Around the early 2000s, GPUs became programmable with the introduction of vertex and pixel shaders, allowing developers to write small programs (shaders) for those pipeline stages. This was a turning point – by 2001 GPUs supported programmable shading with floating-point math, enabling more flexible graphics effects and opening the door to non-graphics uses. However, using these GPUs for general computation initially meant hijacking the graphics APIs (DirectX or OpenGL): developers had to cast computations into graphics terms (e.g. treating input data as textures, operations as shader programs), a cumbersome process.

By the mid-2000s, the potential of GPU parallelism for scientific and general-purpose computing became evident. Researchers demonstrated linear algebra and matrix operations running faster on GPUs than CPUs as early as 2003. This era of so-called GPGPU (General-Purpose Computing on GPUs) saw the creation of early frameworks like Brook (Stanford University) and Sh/RapidMind, which abstracted some of the graphics details.

A major milestone came in **2007** when NVIDIA released **CUDA** (Compute Unified Device Architecture), the first widely adopted C/C++-like programming model for general GPU computing. CUDA provided a more familiar programming model without requiring graphics API knowledge, allowing developers to launch kernels (functions executed in parallel on the GPU) with a straightforward syntax. In **2009**, an open industry standard, **OpenCL**, emerged, aiming to support GPU computing across vendors (initially supported by AMD, NVIDIA, and others). Together, CUDA and OpenCL transformed GPUs into fully programmable parallel processors, no longer limited to graphics.

By the 2010s, GPUs began incorporating features for high-performance computing (HPC): support for double-precision arithmetic, error-correcting memory, and larger memory buffers, making them attractive for scientific computing and simulation workloads. A revolutionary driver of GPU development in the 2010s was **deep learning**. After breakthroughs in neural network training (e.g. the 2012 AlexNet result), GPUs (especially NVIDIA's) became the workhorse for accelerating AI training, due to their ability to perform the massively parallel linear algebra operations required.

This led GPU vendors to optimize architectures for AI: e.g. NVIDIA's 2017 Volta architecture introduced **Tensor Cores** (specialized matrix-multiply units) to speed up neural network training by an order of magnitude. AMD likewise introduced **Matrix Core** units in its CDNA GPUs (Instinct MI100/MI200 series) to accelerate mixed-precision matrix operations for AI. The heavy use of GPUs in AI and HPC is reflected in supercomputers – as of the mid-2020s, the majority of top supercomputers leverage GPU accelerators to achieve their performance.

This demand for AI performance also spurred the rise of domain-specific AI accelerators beyond the traditional GPU. Google debuted its **Tensor Processing Unit (TPU)** ASIC in 2015 (announced 2017) specifically for neural network inference, leveraging a 65,536-unit matrix multiply array for massive parallelism. Others followed: e.g. Graphcore's IPU (2018) with a many-core design for AI, Apple's Neural Engine (first in 2017's iPhone A11 chip) for on-device machine learning, and numerous startup chips (Habana, Cerebras, etc.). By the 2020s, the landscape includes general-purpose GPUs increasingly optimized for AI, and specialized AI chips targeting higher performance or efficiency for neural networks.

In summary, GPU kernel programming has evolved from manually repurposing graphics shaders for computation, to robust parallel programming models like CUDA/OpenCL, and now to an era where both GPUs and dedicated AI chips provide unprecedented compute throughput. Next, we dive into the technical foundations that underlie these processors and how kernels are written and optimized for them.

## Architecture and Execution Models for Parallel Kernels

Writing efficient GPU/AI chip kernels requires understanding the underlying architecture: how these processors execute many operations in parallel. Key architectural concepts include the thread execution model, memory hierarchy, and specialized instruction capabilities.

### Thread Hierarchy and SIMT Execution (Warps and Wavefronts)

GPUs achieve massive parallelism by executing hundreds or thousands of threads concurrently. Unlike CPU threads which run independently, GPU threads are grouped and execute in a **SIMT** (Single-Instruction, Multiple-Threads) fashion. On NVIDIA GPUs, threads are organized into **warps** of 32 threads (on AMD GPUs these are called **wavefronts**, typically 64 threads on older GCN architectures, with newer RDNA GPUs supporting 32 as well). All threads in a warp execute the same instruction in lockstep on different data – this is similar to SIMD vector execution, but presented as individual threads in the programming model (hence SIMT).

Warps are scheduled by the hardware: the GPU's streaming multiprocessors (SMs) or compute units execute one warp's instruction at a time. If threads within a warp diverge (take different branches), the warp serializes the execution of each branch path (inactive threads are masked off), which means branch divergence can harm performance. Thus, kernel developers strive to have threads in a warp follow the same control flow when possible.

Threads are further organized hierarchically. In CUDA and OpenCL, threads are grouped into **thread blocks** (also called Cooperative Thread Arrays, CTAs) of a programmer-specified size, e.g. 128 or 256 threads. Each block is scheduled onto one SM and can share fast on-chip memory and synchronize internally. The collection of all blocks for a kernel launch is called a **grid**. This hierarchy (grid → blocks → warps → threads) allows scalable programming: the developer writes a kernel for one thread, and the runtime transparently executes many threads across blocks and warps. Blocks can be scheduled in any order on available SMs, enabling GPUs to seamlessly scale to different core counts.

Importantly, the warp size imposes an alignment: using block sizes that are multiples of the warp (e.g. 32) avoids partial warps that waste resources. On AMD and Intel GPUs, the model is conceptually similar (OpenCL and SYCL also use global→workgroup→work-item hierarchy). AMD's "wavefronts" of 64 threads play the same role as NVIDIA warps (with RDNA allowing 32). Intel GPUs (Xe architecture) use hardware threads running SIMD vector instructions under the hood – for example, an Intel Xe core might execute 16-wide SIMD instructions per thread, and each core supports multiple hardware threads (e.g. 7 threads per execution unit) to hide latency. In SYCL programming, the term **sub-group** corresponds to a set of threads that execute together (analogous to a warp) and can be of size 8, 16, etc., depending on the device.

High parallel throughput is achieved because a GPU SM can keep many warps in flight. For example, an NVIDIA SM might support 32 resident warps (1024 threads) concurrently, switching between them to hide memory latency. This hardware multithreading means when one warp stalls (e.g. waiting for a memory read), the SM quickly schedules another ready warp – keeping the arithmetic units busy. This design trades single-thread latency for throughput.

In contrast, specialized AI chips like the TPU and IPU do not use warps; they exploit parallelism in other ways (discussed below), but the general principle of concurrency remains: keep many operations in flight to utilize hardware fully.

### Memory Hierarchy and Data Locality

Memory access patterns are often the limiting factor in kernel performance. GPUs and AI accelerators employ a hierarchy of memory with different speeds and scopes to mitigate the relatively slow off-chip memory (DRAM) access. A typical GPU's memory hierarchy includes (from fastest to slowest): **registers** (per thread), **shared memory** (a software-managed on-chip scratchpad shared by threads in a block), one or more levels of **cache** (L1 per SM and a global L2 cache), and finally the **global memory** in VRAM (GDDR or HBM).

Latency to global memory is hundreds of cycles, but caches and shared memory can serve data in a few tens of cycles. Effective kernel programming means using these fast memories to maximize data reuse and coalescing access to slow memory.

**Registers**: Each thread has a set of registers for its private variables. GPUs have a huge register file (e.g. 256 KB per SM on recent NVIDIA GPUs, holding many thousands of 32-bit registers). This allows keeping many threads' contexts resident. If a kernel uses too many registers per thread, it limits how many threads can reside on an SM (a factor in achieving good occupancy). Tuning register usage can thus influence occupancy and latency hiding.

**Shared Memory**: Shared memory (also called local data store on AMD) is a programmer-managed memory that resides on chip. Threads in the same block can read/write shared memory as a fast scratchpad (usually a few tens of KB per block). By tiling algorithms to load a block of data into shared memory, kernels can reuse data with low latency and reduce redundant global memory transactions. For example, in matrix multiplication kernels, it's common to stage sub-matrices into shared memory and then multiply, reusing each element many times. Shared memory latency is roughly comparable to an L1 cache (on the order of 20-30 cycles) but with deterministic programmer control. One must also consider bank conflicts (shared memory is banked and concurrent accesses to the same bank by different threads incur serialization).

**Global Memory**: Global memory (device DRAM) is the largest but slowest storage, accessible by all threads. Effective usage of global memory is crucial. GPUs try to service memory accesses from threads in a warp simultaneously via **memory coalescing**: if threads in a warp access addresses that lie in the same aligned 128-byte segment, the hardware can combine them into a single memory transaction. If instead threads access scattered addresses, the hardware will perform multiple transactions, lowering bandwidth utilization. Thus, kernel optimizations include arranging data structures and thread access patterns so that threads in the same warp access contiguous memory regions whenever possible. Strided or random access patterns tend to under-utilize memory bandwidth.

Modern GPUs have L1/L2 caches that can also mitigate uncoalesced accesses (e.g. caching helps gather scattered accesses), but coalescing is still a best practice. On accelerators like TPUs/IPUs, global memory (often called HBM or streaming memory) is accessed in larger bursts and explicit transfers; we discuss that shortly.

In summary, kernel developers must consider **data locality**: keep data in the fastest memory as long as needed (e.g. use registers and shared memory for reuse) and **access alignment**: arrange parallel accesses so they utilize memory channels efficiently. The use of prefetching (loading data to shared memory or registers in advance of computation) and double buffering (overlapping computation on one data chunk while loading the next) are common patterns to hide memory latency.

### Instruction Sets and Specialized Compute Units

GPUs have evolved their instruction sets to support the above execution model and to incorporate new capabilities. NVIDIA GPUs expose a virtual ISA called **PTX** for compilation, which is JIT-compiled by the driver to the hardware's native instructions (SASS). PTX abstracts many features across GPU generations (similar to how LLVM IR works), while each generation's SM has its own native opcodes. GPU ISAs include typical arithmetic (INT, FP) and control flow, but also SIMD-vector or SIMT-specific instructions (e.g. ballot or shuffle instructions for warp-level operations). AMD's GPUs similarly have an evolving ISA (GCN ISA for older, and now new ISA for RDNA; with intrinsics or HLSL exposing wavefront operations).

A major addition in recent years are **matrix math instructions** to accelerate deep learning. NVIDIA's **Tensor Cores** and AMD's **Matrix Cores** implement matrix–multiply–accumulate (MMA) operations as single instructions that perform dozens or hundreds of FMA operations in parallel. For example, the first-gen Tensor Core in NVIDIA Volta processes a 4×4 × 4×4 matrix multiply-add in one instruction, yielding 64 FMA operations (128 FLOPs) per clock. With 8 such cores per SM, a Volta SM could do 512 FP16 FMA per clock. Newer GPUs expanded this: the NVIDIA A100's Tensor Core can do 256 FMA per clock and supports FP16, BF16, INT8, etc.

AMD's CDNA GPUs introduced **MFMA instructions** (Matrix Fused-Multiply-Add) that operate on a per-wavefront basis, distributing matrix elements across the 64 lanes of a wavefront. For instance, an AMD MI250 Matrix Core can perform a 16×16×? MMA (exact geometry depends on data type) – delivering up to 128 FP32 ops per CU per cycle, and much higher for FP16/INT8. These matrix units are a form of built-in acceleration, and using them (either via libraries or via intrinsics/warp-level code) is essential for optimal AI kernel performance.

Another aspect of the ISA is support for **precision variants**. Beyond standard single (FP32) and double (FP64) precision, GPUs now handle half-precision FP16 and the brain-float BF16, as well as low precision integer (INT8, INT4) for inference. The hardware may have distinct execution units for different types (e.g. separate INT units, or Tensor Cores for half/INT). Efficient kernel programming means choosing the right data type that balances precision and performance. For example, using mixed precision (FP16 for compute with accumulation in FP32) can dramatically speed up training on hardware with Tensor Cores.

Finally, **AI chips** have their own specialized units and execution style. Google TPU (v1) for instance is built around a giant **systolic array** of MAC units rather than dozens of thread processors. Its ISA is more limited (around a dozen instructions tailored to neural network operations). The TPU's matrix unit streams data through a 256×256 array of ALUs, achieving extremely high throughput for matrix multiplies, but it relies on a compiler to map high-level operations to those few instructions.

Graphcore IPU packs 1,472 independent tiles (small cores), each with a custom ISA including an "Accumulating Matrix Product" unit that can do 64 parallel MACs per cycle (for FP16). Each tile's instruction set supports arbitrary control flow (the IPU cores are fully programmable), but to use them efficiently, developers leverage the bulk-synchronous model and the on-chip memory (discussed next).

Apple's **Neural Engine** (ANE), while not publicly detailed at the instruction level, is known to include units for matrix multiplies and convolution operations on 8-bit or 16-bit data, optimized for neural network layers. Apple's documentation simply exposes that the Neural Engine can perform up to tens of trillions of operations per second (TOPS) in matrix/ML tasks with very low power, indicating highly parallel fixed-function units under the hood.

In summary, modern GPUs and AI chips provide an abundance of parallel arithmetic units and a rich memory hierarchy. The thread/block model (SIMT) and scratchpad memories make GPUs efficient for a wide range of parallel kernels, while specialized instructions (tensor/matrix ops) turbocharge linear algebra-heavy workloads like AI. Next, we examine how developers write and optimize kernels on various major platforms, leveraging these architectural features.

## Major Platforms and Programming Models

Although many parallel processors share similar principles, each platform (NVIDIA, AMD, Intel, Google, Apple, Graphcore, etc.) offers its own programming model, APIs, and tools for kernel development. Here we compare the evolution of kernel programming techniques across the major platforms, highlighting how one writes and optimizes kernels on each.

### NVIDIA CUDA GPUs

NVIDIA's CUDA platform (introduced 2007) set the template for modern GPU programming. CUDA is essentially an extension of C/C++ with keywords to designate device kernels (`__global__` functions) and built-in variables for thread indexing. Developers launch kernels with a triple-angle-bracket syntax, specifying the grid and block dimensions. Under the hood, the CUDA compiler (nvcc) separates host and device code, compiling device code to NVIDIA's PTX intermediate representation. The GPU driver JIT-compiles PTX to the native ISA for the specific GPU at runtime (or uses cached binaries). This toolchain abstracts hardware details, but to write high-performance CUDA kernels, one must still consider the hardware architecture (warps, memory hierarchy, etc.) discussed earlier.

Optimizing CUDA kernels has become a fine art over years of HPC and ML usage. Key techniques include:

**Memory coalescing and alignment**: Arrange data structures so that threads in the same warp access contiguous memory. For instance, structure of arrays (SoA) layout is often favored over array of structures for CUDA, to ensure adjacent threads access adjacent memory addresses. The CUDA Programming Guide emphasizes this, and tools like cuda-memcheck and Nsight can analyze access patterns.

**Shared memory tiling**: Using `__shared__` memory to hold a tile of data that threads reuse. For example, in matrix multiply, each thread block loads a submatrix of A and B into shared memory and multiplies them, dramatically reducing global memory traffic. One must use `__syncthreads()` synchronization when coordinating shared memory accesses among threads in a block.

**Leveraging warp-level primitives**: Newer CUDA versions expose shuffle instructions and cooperative groups, allowing threads in a warp or block to exchange data without going through shared memory, and to perform collective operations (like warp-wide reductions). These can eliminate some memory use and latency when used appropriately.

**Occupancy tuning**: Adjusting block size and register/shared memory usage to maximize the number of active warps on each SM (e.g., using the CUDA occupancy calculator to choose launch parameters). Sometimes using slightly smaller blocks can allow more blocks to be resident, thus more warps to hide latency.

**Asynchronous concurrency**: Overlapping data transfers with computation using CUDA streams – while one kernel executes, an independent data copy (in a different stream) can happen, etc. This is more about scheduling than kernel code itself, but it's vital for overall throughput in pipelines (like feeding data to the GPU while the previous batch is processed).

NVIDIA provides a rich ecosystem: libraries such as cuBLAS, cuFFT, and cuDNN (for deep learning primitives) are heavily optimized kernel bundles that developers can use rather than writing their own. These libraries internally use techniques like kernel fusion and low-level hand-tuned assembly to maximize performance. In fact, NVIDIA's success in AI is partly due to software maturity – e.g. cuDNN uses tensor cores with carefully crafted kernels that achieve near-peak FLOPs for convolutions and matrix multiplies.

The CUDA platform also offers thrust (C++ STL-like library for parallel algorithms), NCCL (multi-GPU collectives), and debugging/profiling tools (Nsight Compute, Nsight Systems) which are invaluable for kernel developers to analyze performance bottlenecks (warp divergence, memory stalls, achieved occupancy, etc.). The maturity of CUDA's ecosystem – established community knowledge, extensive documentation, and frequent hardware-software co-design updates – makes it often the baseline against which other platforms are compared for both ease-of-use and performance.

### AMD GPUs (ROCm, HIP and OpenCL)

AMD's GPUs can be programmed via open standards like OpenCL, or through AMD's own ROCm stack and HIP API. Historically, AMD supported Brook+ and Close-to-Metal (CTM) in the late 2000s, but those gave way to OpenCL as the primary means to write portable GPU kernels. OpenCL uses a C-based kernel language and an API for runtime (queueing kernels, managing buffers). It is portable across vendors, but in practice NVIDIA's support waned as they focused on CUDA, while AMD and Intel continued with OpenCL. Many HPC codes did use OpenCL, but the developer experience (managing context, command queues, manually compiling kernels at runtime) was more verbose than CUDA.

To offer a more CUDA-like experience and ease porting, AMD introduced **HIP** (Heterogeneous-compute Interface for Portability). HIP is essentially a C++ CUDA-like kernel language and runtime that can target AMD GPUs (and even NVIDIA GPUs through a translation layer). Developers can compile the same code with hipcc for AMD or with nvcc for NVIDIA, making it a "write once, run on both" model. The HIP API functions closely mirror CUDA runtime APIs (e.g. `hipMalloc` vs `cudaMalloc`), and AMD provides a tool to translate CUDA code to HIP (hipify). Underneath, HIP on AMD uses the ROCm driver, which employs an LLVM-based compiler to generate GCN/RDNA machine code. The ROCm stack also supports OpenMP offloading and OpenACC directives on AMD GPUs via LLVM, giving HPC programmers alternative paths.

In terms of kernel optimization, AMD GCN GPUs share many characteristics with NVIDIA: they have warps (wavefronts) of 64 threads, a similar memory hierarchy (global VRAM, L2 cache, on-chip LDS which is equivalent to shared memory, registers). Therefore, techniques like memory coalescing and blocking in LDS apply similarly. One difference: AMD wavefront size being 64 means memory coalescing benefits if 64 threads access a contiguous block (older AMD hardware coalesced half-wavefronts of 16 or 32 depending on generation, but modern ones effectively coalesce 64-wide accesses). The larger warp can be an advantage for vectorizable code but also means divergence penalty for 64 threads instead of 32. RDNA architecture (used in newer Radeon and some Instinct GPUs) added the ability to execute wavefronts of 32, which can reduce latency and power for workloads that don't need 64-wide vectors.

AMD's recent **CDNA architecture** (for compute/AI accelerators like MI100, MI200) introduced **Matrix Cores** (accelerated MMA units). Kernel developers can access these via WMMA interfaces (in HIP) or via libraries (rocBLAS, MIOpen for DL) or even via inline assembly and LLVM intrinsics. The concept is similar to CUDA's tensor cores: to use them, one typically writes at least part of the kernel at a warp level. In fact, AMD's MFMA operates per wavefront, so a full 64-thread wavefront collaboratively performs a matrix operation. AMD's ROCm 5+ releases include rocWMMA (analogous to CUDA's WMMA API) to help write matrix-multiply kernels that use these specialized units. This is an example of how kernel programming is converging: both NVIDIA and AMD now encourage writing tile-level code where each group of threads computes a matrix fragment via special instructions.

A challenge historically with AMD programming was tooling – but it has improved. ROCm provides debuggers and profilers (rocProfiler, rocTracer, and HIP version of Nsight-like analysis). Still, the ecosystem maturity is not as high as CUDA's: fewer high-level libraries (though ROCm has hipBLAS, hipDNN, etc. that mirror NVIDIA's), and many community codes arrived later. That said, AMD GPUs are proving themselves in top supercomputers (like Frontier, which uses AMD Epyc CPUs and MI250X GPUs exclusively), showing that with effort, one can achieve excellent performance on AMD hardware for HPC and AI. The ROCm software stack being open source is a plus for portability and community contributions. Additionally, frameworks like TensorFlow and PyTorch now have improved support for AMD GPUs (using ROCm under the hood), meaning the gap for AI workloads is closing.

In summary, AMD GPU kernel programming today often means using HIP for a CUDA-like experience or OpenCL for portability. The optimization concepts mirror CUDA's (thread/block model, memory hierarchy), with differences in specifics (wavefront size, some cache behavior). As AMD continues to invest in ROCm, kernel developers targeting AMD have growing resources to achieve performance parity with NVIDIA in many cases.

### Intel GPUs (oneAPI and SYCL)

Intel's approach to GPU programming is via **oneAPI**, an initiative to provide a unified programming model across CPU, GPU, and other accelerators (like FPGAs). The core of oneAPI is **Data Parallel C++** (DPC++), which is based on the SYCL standard from Khronos. SYCL is a higher-level, modern C++ approach: instead of writing kernels in a separate C-like language (as in CUDA or OpenCL C), you write them in C++ lambda functions or functors, and the code is compiled with the host code. SYCL code is single-source – meaning host and device code in one file, with the device code marked by parallel dispatch calls.

A simple SYCL example: using `queue.parallel_for(range<N>, [=](id<1> i){ ... })` to launch N work-items on a device. The SYCL compiler (DPC++ in Intel's case) compiles the device lambda to **SPIR-V** (an intermediate similar to PTX but standardized). At runtime, the oneAPI Level Zero or OpenCL driver compiles SPIR-V to the native GPU ISA (JIT compilation). Alternatively, Intel's toolchain can do ahead-of-time (AOT) compilation for known targets (especially useful for deployment where JIT latency is a concern).

The SYCL runtime manages data transfer through buffer abstractions and command graphs: instead of explicit `cudaMemcpy`, you use buffer objects and the runtime ensures memory is moved to the device when needed and back after kernel execution. This is a more modern C++ task-based approach. From a kernel-writing perspective, SYCL code looks different from CUDA in syntax, but the underlying concepts of threads and memory exist. SYCL defines **work-items** and **work-groups** analogous to threads and thread blocks. Inside a `parallel_for` you can use `nd_range` to specify work-group size. Synchronization is available via barriers for work-group scope. SYCL 2020 also added **unified shared memory** (USM) to allocate pointers accessible by both host and device, akin to CUDA's unified memory.

Intel GPU architecture (like the integrated Gen9/Gen11 or the newer Xe-HPG discrete GPUs) differs in some terminology. An Intel **Execution Unit** (EU) might run 7 hardware threads, each executing 16-wide SIMD instructions. SYCL abstracts this so you still think in terms of scalar work-item code, but the compiler will auto-vectorize or use sub-groups to utilize the 16-wide SIMD hardware. For explicit control, SYCL provides `intel::sub_group` or `nv::warp` extensions where you can operate on groups of 16 or 32 lanes directly (e.g., shuffle within a subgroup, ballot, etc.). This is analogous to warp-level programming in CUDA, but with a portable interface that can map to different hardware (for NVIDIA devices, a subgroup of size 32 maps to a warp; for Intel, subgroup of 16 maps to a vector width, etc.).

Optimizing kernels on Intel GPUs involves many of the same concerns: coalesced accesses (Intel GPUs also benefit when adjacent threads access adjacent memory; the architecture has an L3 cache and on-die memory fabric to gather accesses), using local memory (OpenCL/SYCL local memory maps to either hardware shared memory or is emulated if not available – on some Intel iGPUs, "shared local memory" is actually just a reserved portion of L3). The compiler plays a big role: for instance, the Intel oneAPI compiler can unroll and vectorize kernel code to match the SIMD width. There's an abstraction of hardware threads vs vector lanes that the compiler manages – developers often rely on the compiler to generate efficient code, but can guide it via attributes (like `[[intel::reqd_sub_group_size(16)]]` to force a certain subgroup size for consistency).

Intel provides performance tuning tools such as **Intel VTune** and **Graphics Performance Analyzers**, which can profile GPU kernels for EU active occupancy, memory throughput, etc. Because oneAPI aims to cover CPUs and GPUs, developers can even run the same SYCL kernel on a multicore CPU (where the runtime will use a thread pool) or on FPGAs (where the kernel is synthesized). This portability is a strength, but the highest performance usually requires some specialization (for example, using intrinsics or device-specific extensions when running on a GPU versus CPU).

One advantage of oneAPI/SYCL is that it's an open standard and not tied to only Intel GPUs. Codeplay's implementation (now part of Intel) allows SYCL on NVIDIA GPUs (translating to PTX) and on AMD (via a portability layer), which means in theory a single SYCL codebase could run on any vendor's GPU. However, the ecosystem and community for SYCL are still growing; it's newer than CUDA or OpenCL. Documentation and community support are improving, and HPC centers are exploring SYCL as a way to future-proof codes for multiple GPU vendors (e.g., the European supercomputers with Intel GPUs encourage SYCL).

In summary, programming Intel GPUs with oneAPI/SYCL involves a modern C++ approach, where writing kernels feels different than CUDA but the underlying optimization principles remain: maximize parallel work, use fast memory (insofar as the hardware provides), and ensure good memory access patterns. As Intel's discrete GPUs (like Ponte Vecchio in the Aurora supercomputer) come online, we expect to see more real-world feedback on how SYCL kernel performance compares and what low-level tuning might be needed to fully exploit their architecture.

### Google TPUs

Google's Tensor Processing Units (TPUs) are quite different from GPUs in programming model. TPUs are designed specifically for neural network workloads, and Google provides access to them primarily through high-level machine learning frameworks (TensorFlow and JAX). As such, one does not normally write "kernels" for a TPU in the same way as for a GPU – instead, the typical flow is: write your ML model in TensorFlow, and when you target a TPU, the TensorFlow **XLA compiler** will compile the computation graph (a graph of ops like matmul, convolution, etc.) into TPU executable code. This compilation maps high-level ops to TPU instructions, performs optimizations like operation fusion, and handles graph-level scheduling (e.g. data transfers to and from the TPU).

To understand kernel development on TPU, we consider the TPU's architecture: it is built around a **systolic array matrix unit** and is optimized for dense linear algebra. The original TPU v1 has a 256×256 matrix multiply unit capable of performing 65,536 8-bit multiply-accumulates per cycle. Rather than running thousands of independent threads, the TPU crunches through large matrix ops with a largely pipelined dataflow. A program on TPU is orchestrated by a **CISC-style ISA** – TPU instructions are high-level, such as "read data from host memory to on-chip buffer", "perform matrix multiply of A and B", "apply activation function", etc.

In fact, TPU's designers chose a CISC approach so that each instruction does significant work (e.g., one instruction triggers the entire 256×256 matrix multiply). This reduces instruction fetch overhead and allows explicit scheduling of the matrix unit and the vector/activation units. The TPU has on-chip memory (the **Unified Buffer**, e.g. 24 MB in v1) to store inputs/weights, and uses stream/batch processing – data is fed in from host or network, processed through the matrix engine, and results written out.

When writing custom operations for TPU (if one goes beyond what TensorFlow offers), one typically writes them in TensorFlow's XLA HLO language or as C++ functions that get compiled by XLA. Directly writing assembly for TPU is not common outside Google. However, it's useful to note some optimization aspects: **kernel fusion** is essentially mandatory – TPU's compiler will combine many small ops into larger compound ops to utilize the matrix unit efficiently (for example, fuse activation functions or elementwise ops into the tail of a matmul). **Quantization** is another big factor: TPU v1 was an 8-bit integer machine for inference, and TPUs for training (v2 onward) use bfloat16 (16-bit) for speed with special support to accumulate in higher precision. This means developers need to consider numeric range and stability when targeting TPUs, often using quantization or mixed precision techniques so the TPU can run efficiently in low precision.

One explicit "kernel-like" programming scenario on TPU is using XLA's custom call or writing a C++ op for TensorFlow that has a TPU kernel implementation. In those cases, the developer uses library functions provided by the TPU runtime (which are again at a high level, like launching a GEMM on the matrix unit). But direct per-thread algorithm design isn't a concept on TPU – instead, one thinks in terms of linear algebra: how to batch and tile operations to best fill the systolic array, how to arrange memory so that data flows without stalls.

TPUs also operate in large **pod configurations** – many chips working in parallel on different parts of a model (model parallelism or data parallelism via all-reduce operations). The XLA compiler handles generating collective communication instructions for the TPU mesh as needed. The performance optimization thus is often at an algorithmic level (ensure the matrix multiplications are large enough to utilize the full array, ensure data is reused out of on-chip memory as much as possible, etc.) rather than micro-optimizing instruction sequences. Google provides profiler tools (TPU profile plugin in TensorBoard) to see utilization of various units and memory BW, guiding high-level optimizations like increasing batch size or restructuring the computation to use the MXU more.

In summary, kernel development for TPU is high-level and compiler-driven. The programmer's focus is on the computation graph; the "kernels" are essentially the TPU's firmware-level ops which users indirectly invoke via library calls. It's a very different paradigm from CUDA but extremely powerful for the specific domain of neural networks – TPUs can achieve extraordinary efficiency on matrix-heavy workloads, at the cost of flexibility (they are not suited for irregular code or non-ML tasks). The success of TPU also influenced GPUs (as noted, NVIDIA added tensor cores partly in response to TPU's advantage on matrix ops).

### Apple Neural Engine (ANE)

Apple's Neural Engine is a specialized NPU integrated in Apple's mobile and desktop SoC chips since 2017 (A11 Bionic for iPhones, later in M1/M2 for Macs). The ANE is designed to accelerate common machine learning inference tasks (e.g. image recognition, natural language processing) on-device with high energy efficiency. Apple's design is not fully public, but it is known to consist of multiple cores that perform matrix and vector operations at low precision (such as 8-bit). For example, Apple stated the Neural Engine in the M1 chip can do 11 trillion operations per second (TOPS), and by the M4 chip it reached 38 TOPS – indicating Apple has increased core counts or per-core performance steadily.

From a developer's perspective, you do not write low-level kernels for the ANE. Instead, you use Apple's **Core ML** framework or **Metal Performance Shaders** (MPS). Core ML allows you to take a trained ML model (e.g. a .mlmodel file) and the system will decide whether to run each layer on CPU, GPU, or ANE. When the ANE is available and the model is supported, Core ML will compile the neural network to ANE-compatible instructions (this happens ahead-of-time when the model is added to an app, or just-in-time on device). Apple provides tools like Core ML Tools and Xcode's ML compiler that handle this. The programmer simply makes Core ML API calls (or high-level Swift APIs for Vision, NLP, etc.), so the "kernel optimization" is largely out of their hands.

Under the hood, the ANE likely uses a batching and tiling approach. For instance, it might have units to perform convolution or matrix multiplies on chunks of data, with a compiler dividing a large neural network layer into tasks for each ANE core. The ANE has its own on-chip memory for weights/activations and a high-bandwidth path to the SoC's unified memory. Because it's designed for inference, it focuses on fixed-point or low-precision arithmetic (to save power and silicon area) and has fixed-function units for common activation functions (ReLU, sigmoid, etc.).

One notable aspect is latency vs throughput: the ANE can run asynchronously alongside CPU/GPU. For real-time tasks like camera processing or Siri voice recognition, offloading to ANE frees up CPU/GPU and is more power-efficient, but there is a setup cost to send work to the ANE. Apple likely pipelines execution so that multiple Neural Engine cores work in parallel on different layers of a neural network. The developer doesn't micromanage this – the CoreML system does. Therefore, "optimizing" for ANE often means structuring your neural network in a way that the compiler can fully offload it (using supported layers, avoiding operations that might force falling back to CPU).

In summary, Apple's Neural Engine is a closed, automated acceleration unit. Kernel development per se isn't exposed; instead, Apple provides a very high-level interface. This makes it extremely easy to use (no need to write CUDA or anything – just a few lines to run a Core ML model), but it also means you rely on Apple's compiler to get good performance. The upside is when it works, you get incredible performance-per-watt. The downside is less flexibility (if you have a novel operation not supported by Core ML, you can't run it on ANE). In the broader context of this discussion, ANE represents a trend of domain-specific kernels being generated by compilers – a bit like TPU in that sense, but on a smaller scale for consumer devices.

### Graphcore IPUs

Graphcore's Intelligence Processing Unit (IPU) is an AI accelerator with a unique architecture: it features a large number of small independent cores (called **tiles**) with distributed on-chip memory. A single Graphcore MK2 IPU chip contains **1,472 tiles**, each with its own ~624 KB SRAM memory (for a total of ~900 MB on-chip). Each tile is a simple 32-bit processor core that can execute independent instruction streams (MIMD execution rather than SIMT). The tiles communicate via an ultra-fast interconnect (exchange fabric) that allows data transfers between tiles in a synchronized manner.

The IPU programming model is based on **bulk-synchronous parallel (BSP)** execution. Computation on IPU is divided into steps: in each step, all tiles perform local computations in parallel, then all tiles synchronize (barrier), then optionally exchange data with each other (or with off-chip memory) before the next step. This repeating pattern of compute/sync/communicate is well-suited to machine learning workloads where phases of independent computation are occasionally interleaved with reductions or all-to-all communications (for example, compute forward pass for each data sample locally, then synchronize to do a collective weight update, etc.).

The IPU's software stack (**Poplar SDK**) takes high-level ML models (from frameworks like TensorFlow or PyTorch) and partitions the work across tiles and steps. When writing kernels for IPU, one can either rely on the Graphcore libraries (which provide many pre-optimized ops) or write custom **vertex programs**. A vertex in Poplar corresponds to a small computation that runs on one tile, usually on a subset of the data. For example, if you implement a custom layer, you might write a C++ vertex program that computes one slice of the output, and the Poplar compiler will instantiate that vertex across many tiles, each handling a slice of data. The vertex code can use C++ (with some restrictions) and is compiled to the IPU's ISA.

The IPU ISA includes typical arithmetic (including vector instructions that operate on 2, 4, or 8-element vectors) and specialized operations like an **accumulating matrix product (AMP)** unit per tile, which performs 16×16 matrix multiplies (in half precision) very efficiently. Each tile's AMP can do 64 FP16 MACs per cycle (which is analogous to a very mini "tensor core" on each tile). Unlike GPU warps, each tile's control flow is independent – there's no lockstep across tiles, only the global synchronization between phases.

Optimizing for IPU involves balancing workload evenly across tiles and maximizing on-chip memory usage. The IPU's on-chip memory (**In-Processor-Memory**) is limited per tile, so models must be sharded or pipelined such that what each tile needs for its computation step fits in ~0.6 MB. This forces a style of optimization where you cut your data into many pieces. The advantage is that on-chip SRAM is extremely fast (and there's no cache hierarchy – all local memory accesses are deterministic), yielding huge memory bandwidth per tile. But if an algorithm requires more data than fits on chip, the compiler must orchestrate streaming from off-chip DRAM (Graphcore calls this **Streaming Memory**) via explicit copy instructions and synchronization. Thus, kernel developers aim to keep data on-chip as much as possible, perhaps trading off recomputation for memory (recompute some values rather than store them).

Parallel patterns on IPU often use fine-grained parallelism – e.g., break a matrix multiply into many subtasks distributed over tiles, then use the all-to-all exchange to rotate partial results among tiles (the BSP model). Graphcore provides primitives like `poplar::collectives::allReduce`, etc., which hides the low-level exchange in an easy interface. Anecdotally, achieving top performance on IPU might require algorithmic adjustments: because each tile is small, the overhead of synchronization and communication must be amortized by doing enough work per sync. The Poplar compiler and runtime attempt to schedule communications in parallel with computation (overlap is possible via double-buffering of exchange). Kernel fusion is also important: if you have a sequence of elementwise ops, Poplar will try to fuse them into one vertex so that data is read from memory once and multiple operations are applied.

In terms of writing a custom high-performance IPU kernel (vertex), one might use Graphcore's intrinsics to use the AMP unit, similar to using WMMA in CUDA. The development flow and mindset is closer to writing code for a many-core CPU (with explicit message passing) than a GPU. But the end goal is the same: maximize parallel utilization and keep data local to minimize external memory traffic.

Graphcore's toolchain includes profilers and visualization (**PopVision Graph Analyser**) to see how workloads map onto tiles, tile memory usage, and compute/communication timelines. This helps developers identify imbalances (e.g., if some tiles are idle or running out of memory). Because the architecture is so different, developers often have to rethink algorithms that were designed for GPUs. Some workloads that are memory-bound on GPUs might perform well on IPU if they fit in on-chip memory, whereas extremely large matrix multiplies (which GPUs excel at) might not fit well on IPU without partitioning.

In summary, Graphcore IPUs represent a massively parallel MIMD approach to kernel execution. Kernel programming involves writing small tile programs and relying on the compiler to replicate and coordinate them. The optimization focus is on distributed parallelism (across 1000+ cores) and local memory utilization. The IPU is highly effective for certain AI models, but the programming is complex – typically only pursued by those needing maximum performance on specific workloads that benefit from the IPU's design. It's a more specialized scenario compared to the relatively general-purpose GPU programming.

### Other Notable Accelerators and Models

Beyond the platforms above, there are other AI accelerators with their own kernel development approaches. For example, **FPGAs** can be used for AI or HPC kernels, typically programmed via high-level synthesis (in C/C++ or OpenCL) or RTL – trading off speed for flexibility. Writing FPGA kernels is quite different (pipeline design, etc.), and frameworks like Xilinx Vitis or Intel OpenCL try to abstract it.

Another example is **Cerebras Wafer-Scale Engine**, which has an enormous array of cores on a wafer; it uses a graph compiler where kernels are essentially mapped onto a spatial array – again, not something end-users program at thread-level, but via a graph API. Similarly, **Habana Gaudi** (by Intel) uses an AI-specific cluster of tensor cores and is programmed via standard frameworks (with custom libraries for each operation).

Common to all these is a trend: as we get more domain-specific, the "kernel" is often generated by a compiler or provided by vendor libraries, rather than handwritten by the end-user. GPUs remain the platform where a large community still writes custom kernels manually for performance (though even there, tools like CUDA Graphs and auto-tuners, and ML compilers like TVM, are helping automate kernel generation).

Now that we've surveyed platforms, we'll focus on the AI/ML workloads that drive many of these innovations, and discuss specific kernel optimization techniques in that context.

## Kernel Development for AI/ML Workloads: Training and Inference

AI/ML workloads, especially deep neural networks, place heavy demands on hardware. They involve tensor operations (matrix multiplies, convolutions) with large amounts of parallelism, and often are both compute- and memory-intensive. Writing kernels for ML means optimizing these operations for throughput while handling issues like precision, memory usage, and irregular computation patterns.

### Training vs Inference Considerations

Training a neural network (e.g. a deep learning model) is typically more computationally intensive than inference because it involves forward and backward passes (backpropagation) and uses higher precision. Training workloads often use FP32 or mixed FP16/BF16 precision for adequate numerical stability. During training, certain phases (like calculating weight gradients) involve reductions and matrix multiplications; the memory access patterns can be more complex (e.g. gathering gradients, sparse updates in some cases). Kernels for training often need to fuse multiple operations – for example, a fused kernel that computes both the forward and backward for a layer norm, or fused elementwise operations in backprop – to avoid writing large intermediate tensors to memory.

Inference (using a trained model to make predictions) can often be done in lower precision (e.g. INT8 or even lower) for speed, and usually involves only forward passes through the network. Inference kernels prioritize throughput per watt, especially on edge devices, and often use batch processing (multiple inputs at once) to maximize hardware utilization. Inference can also involve more branching (for example, conditionally executing parts of a network, or operator fusion decisions at runtime for dynamic shapes).

For GPUs, inference sometimes uses specialized tensor core instructions for INT8 (NVIDIA has INT8 and now FP8 support on tensor cores; AMD supports INT8 on Matrix Cores as well). When writing or choosing kernels, a key difference is that training requires atomic or deterministic updates in some cases (like accumulating gradients). This means kernel developers might use atomic operations or design reductions in a way that is reproducible. In contrast, inference is mostly stateless computation on the input data, which is easier to parallelize.

### Mixed Precision and Tensor Cores

One of the most impactful techniques for ML kernels is **mixed precision**. This refers to using lower precision for most computations while maintaining higher precision for a subset (often the accumulation or a master copy of weights). For example, NVIDIA's Volta and newer GPUs support using FP16 (half) inputs and accumulating the results in FP32 (the tensor core HMMA instruction does this). By using FP16 for the bulk of math, throughput is increased (because tensor cores can do 8–16X more half ops than single-precision per cycle), yet final accuracy can be preserved by keeping critical parts in FP32. Frameworks provide automated mixed precision training (loss scaling, etc., to adjust for narrower range of FP16). From a kernel perspective, supporting mixed precision often means writing kernels templated for different data types and ensuring any reductions or summations are done in higher precision to avoid overflow.

Tensor cores specifically are units that have to be explicitly used either by calling libraries or by using new instructions. Initially, these were mostly accessed via cuBLAS or cuDNN (for example, using a GEMM API that automatically uses tensor cores if available). But later CUDA introduced **wmma** (Warp Matrix Multiply Accumulate) intrinsics that allow developers to write their own custom kernels that leverage tensor cores. These intrinsics require structuring your code at the warp level: you have to partition your data into small matrix fragments per warp, load them into special fragment objects, call `wmma::mma_sync` to perform matrix multiply on the tensor core, and then store the result. This is a higher skill task, but it lets expert developers create fused kernels (doing some preprocessing, then a matrix multiply on tensor core, then postprocessing in the same kernel, for example).

On AMD, a similar interface exists (using compiler intrinsics for MFMA). Intel's recent GPUs also have matrix instructions (called **DPAS** – dot product accumulate systolic – used for INT8 and BF16 matrix ops in XeHP, accessible through Intel's GPU compiler via vector intrinsics).

For inference, **INT8 quantization** is common. Using INT8 multiply-accumulate can double the throughput vs FP16 on tensor cores. Many inference kernels (like those in NVIDIA's TensorRT library) will fuse quantization steps (converting FP32 activations to INT8 on the fly, performing INT8 matrix conv, then dequantizing if needed). This fusion needs careful calibration of scale factors but yields tremendous speedups. Kernel-wise, an INT8 convolution kernel might load 32 int8 values into a 256-bit register (for example), do packed arithmetic, and accumulate into an INT32 accumulator which is then scaled to FP16/FP32 output. These require using vectorized loads/stores (to move 8-bit data efficiently) and often use instructions like DP4A (dot product of four 8-bit pairs accumulating into 32-bit) on GPUs that support it.

In summary, taking advantage of hardware capabilities like tensor cores and low precision is now a must for ML kernels. The compiler or libraries often will do it for you, but when writing custom kernels, being aware of these (and using them explicitly if needed) is crucial. For instance, a custom CUDA kernel for an RNN might use half-precision arithmetic if available, and accumulate in float, to get a big speed boost.

### Memory Optimization: Coalescing, Tiling, and Reuse

Memory access optimization is as important as compute for ML kernels, because many deep learning operations are memory-bound (e.g. elementwise nonlinearities, or layers with small compute per data element). We already discussed coalescing – ensuring threads read/write contiguous chunks. This is particularly vital in, say, a batch normalization kernel that reads an array, does a small computation, writes back: such kernels should be designed such that each thread handles multiple elements separated by stride that results in coalesced warp accesses, etc.

**Tiling** is ubiquitous in ML kernels: almost every matrix multiplication or convolution kernel is tiled to fit in the GPU shared memory. For example, a GEMM kernel might partition the M×K × K×N multiply into tiles of size 128×128 (handled by a thread block) which further is composed of sub-tiles (say 16×16 loaded by each warp). Each sub-tile goes into shared memory to be reused for multiple multiply-accumulate steps. Tiling ensures that data fetched from global memory is reused across many operations before it's evicted – this improves effective bandwidth usage. The tile sizes are often chosen to match hardware: e.g., using a tile that results in each thread loading a contiguous 128-bit or 256-bit segment (to fully utilize memory transaction width) and fits in the available shared memory per block.

Another important technique is **kernel fusion**, which we touched on earlier. In ML models, you often have sequences of simple operations (like activation functions, scaling, adding bias, etc.). Rather than launching a separate kernel for each, modern approaches fuse them. For example, instead of one kernel for matrix multiply and then one for adding bias and then one for ReLU, you can fuse the bias-add and ReLU into the tail of the GEMM kernel. This way the data (the result of GEMM) doesn't have to be written to global memory and read back; it can be kept in a register and the next operations applied, then the final result written once. Compiler frameworks (like TVM, XLA, TensorRT) automate such fusion at the graph level. Manually, a developer can fuse by just writing a kernel that does multiple things in one loop. The benefit is reduced memory traffic and reduced kernel launch overhead (which on GPU involves some cost on the CPU for dispatch). As noted, kernel fusion improves data locality and often enables more efficient memory access patterns. It's essentially trading some increase in instruction complexity for saving a lot of memory bandwidth – a good trade-off when bandwidth is precious. However, one must be mindful: fusing very large or dissimilar operations can sometimes hurt cache behavior or reduce opportunities for parallelism. There's a balance – which is why automated compilers use models to decide what to fuse.

**Use of shared memory and registers for reuse**: In convolution kernels (especially for CNNs), a classic trick is to use shared memory to store the input tile (or "patch") that will be used by multiple output pixels. For example, in a 3×3 convolution, threads can collaboratively load a patch of input image into shared memory, and then each thread slides a 3×3 filter over a portion of that tile. This saves re-reading the same input pixels for each output pixel. This concept extends to more complex ops too.

**Memory hierarchy on AI chips**: On TPUs/NPUs, you also aim to maximize on-chip reuse. For instance, a TPU's 24 MB buffer should hold as much of the model's weights or activations as possible to avoid going out to DDR. The XLA compiler automatically arranges compute to re-use data in the MXU pipeline – e.g., it will try to keep a weight matrix in the unified buffer while streaming different batches of activations through the systolic array, so the weight is reused many times (since DRAM access is slow). On Graphcore IPU, as discussed, keeping data in the tile memory is crucial. So an optimization might be to recompute certain values on-chip rather than store them if it saves memory (Graphcore explicitly suggests techniques like recomputation trading off compute for memory). This is a bit different from GPU, where recomputation is usually avoided due to compute cost – but IPU has abundant compute relative to memory, so it's a worthwhile strategy.

Another memory-related optimization in ML kernels is avoiding redundant transfers by using **compute-in-place**. For example, in a sequence of ops, if one can do the operation in-place (overwriting the input with the output) safely, it halves memory use and bandwidth. This is not always possible (e.g., activation function can be in-place, but something like pooling might not if overlapping windows), but when it is, it's beneficial. Libraries like cuDNN will often have in-place versions of activation functions for this reason.

### Kernel Fusion and Graph Compilers

We've already highlighted kernel fusion in a general sense, but it's worth noting how it's applied through graph compilers and what it means for kernel writers. Modern ML frameworks (TensorFlow XLA, PyTorch's Glow or NVFuser, TVM, etc.) include graph-level optimization passes that generate fused kernels. This means that instead of the developer writing a fused kernel by hand, they express the computation in Python or whatever, and the compiler identifies opportunities to combine operations. For example, in PyTorch with NVFuser (a JIT compiler for CUDA kernels), if you write something like `y = relu(x * W + b)`, NVFuser will JIT a single CUDA kernel that performs the fused multiply-add-ReLU in one pass. The generated kernel will be similar to what an expert might write by hand, including using vectorized loads if x is aligned, using shared memory if beneficial, etc.

From the perspective of kernel development, this means the role is shifting: instead of writing dozens of custom kernels for each combination of ops, the compiler can do it. But it's still important for developers to understand how fusion works to ensure their high-level code can be optimized (for instance, sometimes minor changes in how code is written can enable or prevent fusion). One should also be aware of limitations of fusion: if a fused kernel becomes too large (doing a huge amount of work per thread) it might reduce occupancy or not fit in instruction cache, etc. Compilers usually mitigate that by limiting certain fusions. Another concept is memory aliasing – if two operations can't be fused because one's output is used in multiple places, the compiler won't fuse them since it would require duplicating computation. In such cases, manual refactoring or redesigning the computation might be needed to get the benefits.

### Parallelism and Concurrency in ML Kernels

Beyond single-kernel optimizations, ML training especially requires exploiting multiple levels of parallelism. Within a device, we've discussed data parallel threads. But across devices and across host and device, concurrency is key. For multi-GPU (or multi-accelerator) training, techniques like **data parallelism** (each GPU gets different data, then gradients are all-reduced) or **model parallelism** (split the neural network layers across devices) or **pipeline parallel** (different micro-batches in different stages) are used. While this is a bit outside the scope of "kernel development" (it's more about distributed algorithms), it does impact kernel scheduling. For example, overlapping communication and computation: a typical optimized training loop will launch gradient all-reduce (communication kernel) in parallel with the next forward pass computation so that network latency is hidden by computation. This requires using multiple streams or the communication APIs (like NCCL) effectively.

On single GPU, concurrency can mean overlapping data copy (CPU-GPU) with compute, as mentioned, using streams. In frameworks, this is often handled automatically by an asynchronous execution engine, but kernel developers sometimes need to insert appropriate synchronization to maximize overlap (for example, making sure to start data prefetch copies early, and using events to synchronize only when needed).

Some ML models have irregular parallelism – e.g. natural language processing with varying sequence lengths. In such cases, one might write kernels that include a loop or condition per thread to skip work on padded elements, etc. These can cause warp divergence, so another approach is to compact the work (use one kernel to pack active data, then run a dense kernel). These are more algorithmic challenges in kernel land – requiring creativity to maintain efficiency when parallelism isn't nicely uniform.

### Summary of Performance Tuning Opportunities

To wrap up this section, here is a table summarizing key architecture features and optimization techniques across the platforms in the context of AI/ML workloads:

| Platform | Parallelism Model | Notable Hardware Features | Optimization Techniques |
|----------|-------------------|---------------------------|------------------------|
| **NVIDIA GPU (Ampere/Ada)** | SIMT warps (32 threads), many SMs | Tensor Cores (FP16/INT8/TF32 support), L1/L2 caches, Shared Memory (~100 KB/SM) | Use CUDA threads/blocks for massive parallelism; exploit tensor cores via WMMA or libraries; optimize occupancy and warp coherence; shared memory tiling and memory coalescing are critical; use streams for overlap; leverage cuDNN/cuBLAS for well-tuned kernels. |
| **AMD GPU (CDNA/RDNA)** | SIMT wavefronts (64 or 32 threads) | Matrix Cores (MFMA for FP16/BF16/INT8), large register file, LDS (up to 64 KB/CU), L2 cache | Use HIP/OpenCL work-groups similarly to CUDA blocks; ensure memory accesses are coalesced for 64-wide warps; leverage AMD's Matrix Cores via rocBLAS or inline intrinsics; tune EU occupancy (consider wave64 vs wave32); use asynchronous compute engines if available for copy/compute overlap. |
| **Intel GPU (Xe)** | SIMD-8/16 vectors with 4-8 HW threads/EU | XMX/DPAS matrix engines (for INT8/BF16), L1 caches and large L3, shared local mem (some arch) | Write SYCL/DPC++ kernels, rely on compiler to vectorize across 8-16 lanes; use sub-groups to explicitly control SIMD; ensure memory accesses align with 64-byte cache lines; use oneAPI MKL/oneDNN for tuned primitives; manage USM or buffers to allow overlap of transfers via tasks. |
| **Google TPU (v2/v3)** | Systolic array matrix multiply, 2×2 cores per chip (each core highly parallel MAC array) | Matrix Unit (e.g. 128×128 or larger systolic array), on-chip SRAM (tens of MB), high-bandwidth interconnect for all-reduce | Use XLA to compile graphs – ensure large matrix ops to utilize array; batch computations to keep MXU busy; use bfloat16 for speed (with FP32 accumulate); fuse elementwise ops into matrix ops via XLA; pipeline host-to-TPU transfers using infeed/outfeed queues for overlap. |
| **Graphcore IPU (MK2)** | 1472 independent cores (6 threads each) in BSP sync model | 624 KB SRAM per core (In-Processor Memory), fast all-to-all exchange fabric, per-core matrix engine (64 FP16 MAC/cycle) | Partition computations into ~1k parallel tasks (vertices); maximize on-chip data usage (fit working set in 900MB total); overlap compute and exchange in BSP cycles; use pipelining for multi-IPU workloads; exploit the AMP unit via Poplibs (vendor libs) or custom code to do matrix ops locally. |
| **Apple ANE (M-series)** | 16+ NPU cores, running specialized tasks (not user-threaded) | Fixed-function units for conv/MATMUL, INT8/16 compute, unified memory access with CPU/GPU, Neural Engine controller dispatching ops | Let Core ML compiler map neural layers to ANE; use models and layer types that are ANE-friendly (avoid unsupported ops that would fall back to CPU/GPU); use quantization to INT8 since ANE is optimized for it; batch inputs if possible to utilize all cores; ensure asynchronous execution by not blocking CPU while ANE runs (framework handles this). |

This table encapsulates the diversity: from NVIDIA/AMD GPUs where developers manage threads and memory in detail, to TPUs/NPUs where compilers handle most of it and the task is to make the high-level operations as efficient as possible.

## Comparative Analysis: Ecosystem Maturity, Complexity, and Use Cases

Having looked at each platform, we can compare them on a few axes:

### Programming Complexity

NVIDIA's CUDA is often considered the easiest for someone aiming to hand-write high-performance kernels, thanks to its straightforward C++ API and extensive documentation/examples. HIP on AMD mimics this, so for a CUDA-experienced developer, adapting to HIP is straightforward (though one must be mindful of differences like warp size and some intrinsic availability). Intel's SYCL requires modern C++ knowledge; it has a steeper learning curve if you're not used to templates and lambdas, but it spares you some low-level details (the compiler does more). TPU and ANE essentially don't allow any low-level kernel coding by end-users – they are easiest to use (just write your model and the compiler does the rest), but impossible to customize beyond what the frameworks allow. Graphcore IPU sits on the opposite end: it arguably has the highest complexity for custom development, because you're managing thousands of parallel tasks and the memory of each tile, which can be quite challenging (Graphcore provides a lot of automation, but to really optimize you might need to micromanage allocation and communication).

### Performance Tuning & Tools

NVIDIA again leads with very mature tools (Nsight profilers, Visual Profiler, CUDA-GDB, etc.) and a huge knowledge base on forums like StackOverflow. AMD's tools have improved (with CodeXL in the past and now ROCm profiling and debugging tools), but they are not yet as seamless and sometimes lack features for certain GPUs. Intel's oneAPI toolkit is robust given Intel's history with developer tools – VTune can profile kernels on CPU and GPU, and Intel's analyzer can show occupancy, EU utilization, etc. However, since Intel GPUs are new, the community experience is still limited; performance tuning guides are emerging. Graphcore provides PopVision, which is specialized but good for their architecture (visualizing 1500 cores is a different kind of profiling!). Google's TPU tooling is mostly internal or through TensorBoard plugin – it gives high-level metrics but doesn't allow you to change how the TPU executes beyond altering your model or XLA flags. Apple's ANE has almost no public performance tools (developers mostly rely on observing latency improvements and using instruments for overall app performance).

### Ecosystem and Libraries

In terms of ecosystem, NVIDIA is by far the most extensive – not only official libraries (cuDNN, TensorRT, etc.) but also 3rd-party libraries and community contributions target CUDA first. AMD's ROCm ecosystem now covers many of the same bases (rocBLAS, hipFFT, rocRAND, MIOpen as a cuDNN equivalent, etc.), but these often trail in performance or features (for instance, MIOpen might lag in supporting the latest network architectures). Intel's oneAPI comes with oneDNN (optimized DNN library), oneMKL (math kernel lib), etc., which are high-quality for CPU and have GPU support that is improving. Yet, many AI researchers haven't used Intel GPUs enough for a community to form around sharing GPU kernel tricks. TPU's ecosystem is basically TensorFlow – if your model fits into TensorFlow (or JAX), you're good, and you have access to Google's cutting-edge implementations of various ops on TPU (XLA does it). But outside of that, you can't do much – PyTorch support for TPU was via an XLA backend, which not everyone finds convenient. Graphcore's ecosystem is somewhat proprietary – you use their Poplar and PopLibs; they have integrations for TensorFlow and PyTorch but you might have to adapt your model to fit memory constraints. It's smaller in user base – mostly some research labs and certain commercial partners.

### Performance Characteristics

For general HPC (non-AI), GPUs (NVIDIA and AMD) are the primary options. They handle double-precision heavy workloads (NVIDIA's A100, e.g., has strong FP64, and AMD MI250 as well). TPUs and IPUs are not designed for 64-bit math or things like irregular memory accesses, so they're unsuitable for many HPC tasks like molecular dynamics or finite element codes (which often need DP precision and complex memory use). In AI training, NVIDIA GPUs have been the workhorse and still are, though TPUs have demonstrated top-notch performance in specific cases (Google reports training large models on TPU pods very efficiently). The performance tuning on GPU is a lot about finding the last 10-20% via custom kernels, whereas on TPU it's more about ensuring your model scales and feeds the hardware well (the hardware either gives you near peak or you're underutilizing it due to pipeline stalls or too small matrix sizes). Graphcore IPUs have shown strong performance on some models (like certain graph neural nets or sparse models that benefit from massive parallelism and memory), but on very large dense models they sometimes underperform GPUs which have higher memory bandwidth and larger caches for those scenarios.

### Use Case Maturity

Real-time graphics is still 100% the domain of GPUs (NVIDIA, AMD, plus mobile GPUs from Qualcomm, etc.). The GPU kernel programming for graphics (via HLSL/GLSL shaders or compute shaders) is a well-established field, separate from CUDA but conceptually similar in writing small programs for parallel execution. AI training/inference is now done on a mix: if you are a researcher, you likely use NVIDIA GPUs by default (due to ease and ubiquity); if you have access to TPUs via Google Cloud, you might use them for very large models or if you prefer TensorFlow; if you're in an environment with new Intel GPUs or AMD Instinct GPUs (like some national labs), you'll use those, but the vast majority of software frameworks have only recently (2023-2025) reached a point to make that relatively smooth. Apple's ANE is a special case – it's not something you'd use in a datacenter, but for mobile apps, it's extremely valuable (enabling neural features without draining battery). But developers targeting ANE have to use Core ML, which might mean converting models and ensuring they are quantized etc., which is an extra step compared to just running a float32 model on a GPU.

To crystallize some of these differences, here's a brief comparative table focusing on software and ecosystem aspects:

| Platform | Primary Languages/APIs | Maturity & Support | Domain Focus |
|----------|------------------------|-------------------|--------------|
| **NVIDIA CUDA GPUs** | CUDA C/C++, PTX (low-level), also supports OpenCL, OpenACC, etc. | Very mature (since 2007); extensive documentation, large community; broad framework support (TensorFlow, PyTorch, etc. optimized for CUDA). | General-purpose parallel compute, HPC (FP64 support), AI/ML (excellent library support), and graphics (via separate graphics APIs). |
| **AMD ROCm GPUs** | HIP C/C++, OpenCL, OpenMP offloading, SYCL (community ports) | Moderately mature (ROCm stable since ~2018); growing support in ML frameworks (PyTorch direct HIP backend); tools improving but smaller community. Often requires Linux (Windows support limited for ROCm). | General HPC and AI; used in top supercomputers and some cloud offerings. Good FP64 performance for HPC, strong BF16/FP16 for AI (MI series). Lacks graphics APIs (separate Radeon driver stack for graphics). |
| **Intel oneAPI GPUs** | SYCL/DPC++ (C++17), OpenCL, OpenMP; (also support for CUDA code translation in oneAPI) | New entrant (GPUs in 2022 Aurora etc.); oneAPI is well-designed but real-world use is nascent. Intel provides high-quality math libs, and frameworks like TensorFlow getting oneDNN plugin for GPU. Community still building up. | AI training and inference (especially where Intel can bundle CPU+GPU solutions); HPC workloads (potentially, given FP64 support on Ponte Vecchio). Not used for graphics (Intel uses different GPU stack for 3D). |
| **Google TPU** | TensorFlow and JAX (XLA graph compilation); limited PyTorch support via XLA; no low-level user ISA exposure | Fairly mature in Google's ecosystem (deployed since 2015); outside Google, available via cloud. Limited user base compared to GPUs, but those who use it achieve strong training performance. Must use Google's frameworks; debugging at op level, not per thread. | Deep learning training at scale, and high-throughput inference in datacenters. Not suitable for general HPC or any non-ML workload. No graphics capability. Focused on large matrix operations and dense neural networks. |
| **Graphcore IPU** | Poplar API (C++/Python), integrates with TensorFlow/PyTorch (requires model porting); custom vertex programs in C++ for low-level control | Emerging (first products ~2018); used in some research and pilot industry projects. Smaller community, with Graphcore providing direct support. Documentation exists but developer pool is limited. Must adapt algorithms to IPU style. | Niche AI workloads that benefit from fine-grained parallelism or lower precision; e.g. graph neural nets, Transformers (with model parallelism). Not used for traditional HPC or graphics. High performance on certain ML tasks, but requires effort to utilize fully. |
| **Apple Neural Engine** | Core ML (models in Swift or via conversion from PyTorch/TensorFlow); no direct kernel API | Highly mature for Apple's use cases (since 2017 every year new ANE, billions of devices). But for developers, it "just works" or not at all – limited insight into inner workings. Apple's ecosystem provides some guidance (Core ML Tools) but it's very high-level. | Real-time on-device ML inference (vision, speech, etc.) in mobile and Mac devices. Not for training or general compute. Excels at low-power execution of neural nets. Completely closed environment (black box accelerator). |

From this comparison, one can see that NVIDIA's platform remains the gold standard for flexibility and widespread use, AMD and Intel are strong contenders especially in HPC contexts (and for those wanting open or vendor-neutral solutions), and domain-specific chips like TPU, IPU, ANE shine in their niches but require one to commit to specific software stacks and often specific models that fit their assumptions.

## Beyond AI: GPUs in Scientific Computing and Graphics

While much of the recent excitement is about AI, it's important to note that GPU kernel programming was originally driven by graphics and HPC. In scientific computing, GPUs are used to accelerate simulations in physics, chemistry, climate modeling, and more. Many of these applications involve writing kernels that implement PDE solvers, particle simulations, etc. The techniques don't fundamentally differ from what we described: e.g., an FFT kernel or a finite-difference stencil kernel on a GPU will also use shared memory for data tiling and avoid memory bank conflicts, just as an ML kernel would. The difference is often in precision (HPC often needs FP64 where GPUs have lower throughput, so algorithms might be adapted to use mixed precision or compensate for that) and in problem size (HPC problems might not have the regular dense structure of neural nets, requiring more clever parallelization).

Frameworks like **OpenACC** and **OpenMP 4+** emerged to help port scientific codes to GPUs by allowing the compiler to generate the kernels (developers add pragmas to their C/C++/Fortran code). This has been successful in many cases, though sometimes hand-tuned kernels still outperform compiler-generated ones.

In real-time graphics, GPUs run millions of tiny kernels in the form of **shaders**. A modern graphics pipeline uses vertex, fragment, geometry, tessellation, and compute shaders written in HLSL/GLSL. These are compiled to run on the GPU's SIMT units much like CUDA kernels (in fact, the separation between graphics and compute has narrowed with APIs like Vulkan and DirectX 12 allowing compute shaders and flexible scheduling). The classic rasterization pipeline is largely fixed-function and shader-based, but with the advent of GPU ray tracing (RT cores on NVIDIA, etc.), new types of kernels (e.g., compute-heavy ray intersection shaders) are now running on GPUs as well. The key difference is that graphics shaders operate under tighter real-time constraints (16ms frame budget) and are often pixel-parallel (very regular), whereas GPGPU kernels can run longer. However, both are concerned with maximizing parallel throughput and efficient memory use (e.g., texture caches in graphics serve a similar role to global memory coalescing – 2D locality is exploited).

One interesting crossover is using GPU compute for graphics post-processing (like filters, physics in games, particle effects) – these are essentially GPGPU kernels (often written as compute shaders) integrated into a graphics engine. So the skill set of kernel optimization (thread usage, memory coalescing, etc.) is directly applicable in advanced graphics programming too.

Finally, GPUs in HPC often pair with CPUs in heterogeneous systems (hence the term GPU accelerator). A common pattern is offloading the heavy number-crunching parts of a code to kernels on the GPU, similar to offloading dense linear algebra to a BLAS library. Developers need to orchestrate data movement and kernel launches in a way that the GPU is kept busy and the CPU does any serial or less-parallel work. This has led to complexity in large codes – which is being addressed by directive approaches and unified memory. The challenge remains that a GPU is extremely fast when fed data and kernels optimally, but any imbalance (e.g., waiting on data, launching too small of a kernel) leads to underutilization. So HPC developers have learned to batch operations, use streams for concurrency, and use pinned memory for faster transfers, among other techniques.

In summary, GPU kernel development spans far beyond AI – it's a critical part of modern scientific computing and of course the backbone of computer graphics. The historical knowledge from those domains informed many of the optimizations we apply in ML kernels today (for instance, "thread coarsening" and "loop unrolling" were studied in HPC codes on GPU well before being applied to DL). The convergence of needs (fast linear algebra is common to simulation and ML) means improvements in one domain often benefit the other.

## Conclusion

From the early days of fixed-function graphics accelerators to today's cutting-edge AI chips, the development of GPU and AI kernel programming techniques has been a story of increasing generality, programmability, and performance. We traced how GPUs opened up from graphics-only to general compute, necessitating new programming models (CUDA, OpenCL) and new ways of thinking about algorithms (SIMT parallelism, vectorization, tiling). Over time, both hardware and software evolved in tandem: hardware gained features like sophisticated memory hierarchies and specialized tensor units, while software introduced better compilers, libraries, and abstractions to harness that hardware.

Technically, writing high-performance kernels requires understanding the architecture (threads/warps, memory, etc.) and exploiting it fully – whether that's coalescing memory accesses on a GPU or keeping an IPU's tiles 100% busy with local data. The common themes are **parallelism** (identifying independent work and distributing it), **locality** (arranging computations to reuse data in fast memory), and increasingly **mixing precision** and **fusing operations** to squeeze more performance out of each memory transfer and each instruction.

Different platforms present different trade-offs. NVIDIA's CUDA platform offers a rich, relatively low-level control which rewards expert tuning; AMD and Intel's solutions strive for portability and openness, while catching up in tooling and ecosystem. Domain-specific chips like TPUs and IPUs push the envelope in performance per watt by constraining the problem domain – and they rely on compilers to do most of the heavy lifting in kernel generation.

For a practitioner, this means the skillset might shift more towards leveraging these compilers and less towards hand-writing every kernel from scratch, especially in AI/ML where operator libraries and graph optimizers are prevalent. That said, the need for technical precision and deep understanding isn't going away. To diagnose a performance issue, you might still need to know that, for example, your SYCL kernel is memory-bound due to strided accesses, or that your CUDA kernel is hitting bank conflicts in shared memory, or that your TPU program is input-bound waiting on host data. Thus, the knowledge of kernel development – from history, we learned why things are structured as they are, and from architecture, we learned what the machine under the hood is doing – is invaluable for writing efficient code.

In conclusion, GPU and AI chip kernel development has grown into a multifaceted field encompassing programming languages, compiler technology, and hardware-software co-design. By understanding the historical evolution and the technical details of each platform, developers can choose the right tool for the task and push the limits of performance, whether they are rendering graphics in real time, simulating the cosmos, or training the next breakthrough AI model.

---

## References

- [General-purpose computing on graphics processing units - Wikipedia](https://en.wikipedia.org/wiki/General-purpose_computing_on_graphics_processing_units)
- [A Brief History of GPU | Medium](https://medium.com/altumea/a-brief-history-of-gpu-47d98d6a0f8a)
- [NVIDIA Tensor Core Evolution: From Volta To Blackwell](https://newsletter.semianalysis.com/p/nvidia-tensor-core-evolution-from-volta-to-blackwell)
- [AMD matrix cores — ROCm Blogs](https://rocm.blogs.amd.com/software-tools-optimization/matrix-cores/README.html)
- [THE EXASCALE ERA IS HERE - AMD](https://www.amd.com/content/dam/amd/en/documents/instinct-tech-docs/instinct-mi200-datasheet.pdf)
- [An in-depth look at Google's first Tensor Processing Unit (TPU) | Google Cloud Blog](https://cloud.google.com/blog/products/ai-machine-learning/an-in-depth-look-at-googles-first-tensor-processing-unit-tpu)
- [Neural Engine - Wikipedia](https://en.wikipedia.org/wiki/Neural_Engine)
- [What is a GPU warp? | Modular](https://docs.modular.com/glossary/gpu/warp/)
- [Support warpSize or equivalent to query warp/wavefront size #23599](https://github.com/chapel-lang/chapel/issues/23599)
- [Intel® Xe GPU Architecture](https://www.intel.com/content/www/us/en/docs/oneapi/optimization-guide-gpu/2025-0/intel-xe-gpu-architecture.html)
- [Basics on NVIDIA GPU Hardware Architecture - HECC Knowledge Base](https://www.nas.nasa.gov/hecc/support/kb/basics-on-nvidia-gpu-hardware-architecture_704.html)
- [How to Access Global Memory Efficiently in CUDA C/C++ Kernels | NVIDIA Technical Blog](https://developer.nvidia.com/blog/how-access-global-memory-efficiently-cuda-c-kernels/)
- [Tensor cores and Matrix cores](https://www.glennklockwood.com/garden/tensor-cores)
- [2. IPU hardware overview — IPU Programmer's Guide](https://docs.graphcore.ai/projects/ipu-programmers-guide/en/latest/about_ipu.html)
- [HIP: C++ Heterogeneous-Compute Interface for Portability - GitHub](https://github.com/ROCm/hip)
- [Nvidia Warp: A Python framework for high performance GPU ...](https://news.ycombinator.com/item?id=40680737)
- [Just-In-Time Compilation - Intel](https://www.intel.com/content/www/us/en/docs/oneapi/optimization-guide-gpu/2024-1/jitting.html)
- [Compilation Flow Overview - Intel® oneAPI Programming Guide](https://www.intel.com/content/www/us/en/docs/oneapi/programming-guide/2025-0/compilation-flow-overview.html)
- [Compilers Optimize CUDA with Quantization and Kernel Fusion - Pynomial](https://pynomial.com/2025/07/compilers-optimize-cuda-with-quantization-and-kernel-fusion/)
