# Important LLM Papers

Foundational papers and breakthrough techniques in large language model research (excluding model-specific technical reports).

| Paper                                                                                             | Authors                                                              | Year | Link                                                                                                                             |
| ------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------- | ---- | -------------------------------------------------------------------------------------------------------------------------------- |
| **üèóÔ∏è Foundational Architecture**                                                                    |                                                                      |      |                                                                                                                                  |
| "Attention Is All You Need"                                                                       | Ashish Vaswani et al.                                                | 2017 | [PDF](https://arxiv.org/abs/1706.03762)                                                                                          |
| "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"                | Jacob Devlin et al.                                                  | 2018 | [PDF](https://arxiv.org/abs/1810.04805)                                                                                          |
| "Improving Language Understanding by Generative Pre-Training" (GPT-1)                             | Alec Radford et al.                                                  | 2018 | [PDF](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf)    |
| "Language Models are Unsupervised Multitask Learners" (GPT-2)                                     | Alec Radford et al.                                                  | 2019 | [PDF](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)        |
| "Language Models are Few-Shot Learners" (GPT-3)                                                   | Tom B. Brown et al.                                                  | 2020 | [PDF](https://arxiv.org/abs/2005.14165)                                                                                          |
| "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer" (T5)          | Colin Raffel et al.                                                  | 2020 | [PDF](https://arxiv.org/abs/1910.10683)                                                                                          |
| "LLaMA: Open and Efficient Foundation Language Models"                                            | Hugo Touvron et al.                                                  | 2023 | [PDF](https://arxiv.org/abs/2302.13971)                                                                                          |
| **üîß Architectural Components**                                                                      |                                                                      |      |                                                                                                                                  |
| "Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer"                | Noam Shazeer et al.                                                  | 2017 | [PDF](https://arxiv.org/abs/1701.06538)                                                                                          |
| "Fast Transformer Decoding: One Write-Head is All You Need" (MQA)                                | Noam Shazeer                                                         | 2019 | [PDF](https://arxiv.org/abs/1911.02150)                                                                                          |
| "Root Mean Square Layer Normalization" (RMSNorm)                                                 | Biao Zhang, Rico Sennrich                                            | 2019 | [PDF](https://arxiv.org/abs/1910.07467)                                                                                          |
| "GLU Variants Improve Transformer" (SwiGLU)                                                       | Noam Shazeer                                                         | 2020 | [PDF](https://arxiv.org/abs/2002.05202)                                                                                          |
| "RoFormer: Enhanced Transformer with Rotary Position Embedding" (RoPE)                           | Jianlin Su et al.                                                    | 2021 | [PDF](https://arxiv.org/abs/2104.09864)                                                                                          |
| "Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation" (ALiBi) | Ofir Press et al.                                                    | 2021 | [PDF](https://arxiv.org/abs/2108.12409)                                                                                          |
| "Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity"   | William Fedus et al.                                                 | 2021 | [PDF](https://arxiv.org/abs/2101.03961)                                                                                          |
| "GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints"           | Joshua Ainslie et al.                                                | 2023 | [PDF](https://arxiv.org/abs/2305.13245)                                                                                          |
| **üìä Scaling Laws & Efficiency**                                                                     |                                                                      |      |                                                                                                                                  |
| "Scaling Laws for Neural Language Models"                                                         | Jared Kaplan et al.                                                  | 2020 | [PDF](https://arxiv.org/abs/2001.08361)                                                                                          |
| "Training Compute-Optimal Large Language Models" (Chinchilla)                                     | Jordan Hoffmann et al.                                               | 2022 | [PDF](https://arxiv.org/abs/2203.15556)                                                                                          |
| "FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness"                     | Tri Dao et al.                                                       | 2022 | [PDF](https://arxiv.org/abs/2205.14135)                                                                                          |
| "FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning"               | Tri Dao                                                              | 2023 | [PDF](https://arxiv.org/abs/2307.08691)                                                                                          |
| "YaRN: Efficient Context Window Extension of Large Language Models"                              | Bowen Peng et al.                                                    | 2023 | [PDF](https://arxiv.org/abs/2309.00071)                                                                                          |
| **üéØ Training & Alignment**                                                                          |                                                                      |      |                                                                                                                                  |
| "Deep reinforcement learning from human preferences"                                              | Paul Christiano et al.                                               | 2017 | [PDF](https://arxiv.org/abs/1706.03741)                                                                                          |
| "Finetuned Language Models Are Zero-Shot Learners" (FLAN)                                        | Jason Wei et al.                                                     | 2021 | [PDF](https://arxiv.org/abs/2109.01652)                                                                                          |
| "Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback"      | Yuntao Bai et al.                                                    | 2022 | [PDF](https://arxiv.org/abs/2204.05862)                                                                                          |
| "InstructGPT: Training Language Models to Follow Instructions with Human Feedback"                | Long Ouyang et al.                                                   | 2022 | [PDF](https://arxiv.org/abs/2203.02155)                                                                                          |
| "Self-Instruct: Aligning Language Models with Self-Generated Instructions"                       | Yizhong Wang et al.                                                  | 2022 | [PDF](https://arxiv.org/abs/2212.10560)                                                                                          |
| "Scaling Instruction-Finetuned Language Models"                                                  | Hyung Won Chung et al.                                               | 2022 | [PDF](https://arxiv.org/abs/2210.11416)                                                                                          |
| "Constitutional AI: Harmlessness from AI Feedback"                                                | Yuntao Bai et al.                                                    | 2022 | [PDF](https://arxiv.org/abs/2212.08073)                                                                                          |
| "Direct Preference Optimization: Your Language Model is Secretly a Reward Model" (DPO)           | Rafael Rafailov et al.                                               | 2023 | [PDF](https://arxiv.org/abs/2305.18290)                                                                                          |
| "RLAIF: Scaling Reinforcement Learning from Human Feedback with AI Feedback"                     | Harrison Lee et al.                                                  | 2023 | [PDF](https://arxiv.org/abs/2309.00267)                                                                                          |
| **üß† Reasoning & Prompting**                                                                         |                                                                      |      |                                                                                                                                  |
| "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models"                           | Jason Wei et al.                                                     | 2022 | [PDF](https://arxiv.org/abs/2201.11903)                                                                                          |
| "Self-Consistency Improves Chain of Thought Reasoning in Language Models"                        | Xuezhi Wang et al.                                                   | 2022 | [PDF](https://arxiv.org/abs/2203.11171)                                                                                          |
| "ReAct: Synergizing Reasoning and Acting in Language Models"                                      | Shunyu Yao et al.                                                    | 2023 | [PDF](https://arxiv.org/abs/2210.03629)                                                                                          |
| "Toolformer: Language Models Can Teach Themselves to Use Tools"                                  | Timo Schick et al.                                                   | 2023 | [PDF](https://arxiv.org/abs/2302.04761)                                                                                          |
| "Tree of Thoughts: Deliberate Problem Solving with Large Language Models"                        | Shunyu Yao et al.                                                    | 2023 | [PDF](https://arxiv.org/abs/2305.10601)                                                                                          |
| "Quiet-STaR: Language Models Can Teach Themselves to Think Before Speaking"                       | Eric Zelikman et al.                                                 | 2024 | [PDF](https://arxiv.org/abs/2403.09786)                                                                                          |
| **‚ö° Quantization & Compression**                                                                    |                                                                      |      |                                                                                                                                  |
| "LoRA: Low-Rank Adaptation of Large Language Models"                                              | Edward J. Hu et al.                                                  | 2021 | [PDF](https://arxiv.org/abs/2106.09685)                                                                                          |
| "GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers"              | Elias Frantar et al.                                                 | 2022 | [PDF](https://arxiv.org/abs/2210.17323)                                                                                          |
| "AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration"                 | Ji Lin et al.                                                        | 2023 | [PDF](https://arxiv.org/abs/2306.00978)                                                                                          |
| "QLoRA: Efficient Finetuning of Quantized LLMs"                                                  | Tim Dettmers et al.                                                  | 2023 | [PDF](https://arxiv.org/abs/2305.14314)                                                                                          |
| **üëÅÔ∏è Multimodal**                                                                                    |                                                                      |      |                                                                                                                                  |
| "Learning Transferable Visual Models From Natural Language Supervision" (CLIP)                   | Alec Radford et al.                                                  | 2021 | [PDF](https://arxiv.org/abs/2103.00020)                                                                                          |
| "Flamingo: a Visual Language Model for Few-Shot Learning"                                        | Jean-Baptiste Alayrac et al.                                         | 2022 | [PDF](https://arxiv.org/abs/2204.14198)                                                                                          |
| "Visual Instruction Tuning" (LLaVA)                                                               | Haotian Liu et al.                                                   | 2023 | [PDF](https://arxiv.org/abs/2304.08485)                                                                                          |
| **üìù Benchmarks & Evaluation**                                                                       |                                                                      |      |                                                                                                                                  |
| "Measuring Massive Multitask Language Understanding" (MMLU)                                       | Dan Hendrycks et al.                                                 | 2020 | [PDF](https://arxiv.org/abs/2009.03300)                                                                                          |
| "Evaluating Large Language Models Trained on Code" (HumanEval)                                   | Mark Chen et al.                                                     | 2021 | [PDF](https://arxiv.org/abs/2107.03374)                                                                                          |
| "Training Verifiers to Solve Math Word Problems" (GSM8K)                                         | Karl Cobbe et al.                                                    | 2021 | [PDF](https://arxiv.org/abs/2110.14168)                                                                                          |
| "Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models" (BIG-Bench) | 450 authors                                                | 2022 | [PDF](https://arxiv.org/abs/2206.04615)                                                                                          |
| **üõ°Ô∏è Safety & Red Teaming**                                                                          |                                                                      |      |                                                                                                                                  |
| "Red Teaming Language Models with Language Models"                                                | Ethan Perez et al.                                                   | 2022 | [PDF](https://arxiv.org/abs/2202.03286)                                                                                          |
| "Red Teaming Language Models to Reduce Harms: Methods, Scaling Behaviors, and Lessons Learned"   | Deep Ganguli et al.                                                  | 2022 | [PDF](https://arxiv.org/abs/2209.07858)                                                                                          |
| **üîÑ Alternative Architectures**                                                                     |                                                                      |      |                                                                                                                                  |
| "Mamba: Linear-Time Sequence Modeling with Selective State Spaces"                                | Albert Gu and Tri Dao                                                | 2023 | [PDF](https://arxiv.org/abs/2312.00752)                                                                                          |
| **üí° Analysis & Insights**                                                                           |                                                                      |      |                                                                                                                                  |
| "Sparks of Artificial General Intelligence: Early experiments with GPT-4"                        | S√©bastien Bubeck et al.                                              | 2023 | [PDF](https://arxiv.org/abs/2303.12712)                                                                                          |
