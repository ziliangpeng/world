| Paper                                                                                             | Authors                                                              | Year | Link                                                                                                                             |
| ------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------- | ---- | -------------------------------------------------------------------------------------------------------------------------------- |
| **Foundational & Architectural**                                                                    |                                                                      |      |                                                                                                                                  |
| "Attention Is All You Need"                                                                       | Ashish Vaswani et al.                                                | 2017 | [PDF](https://arxiv.org/abs/1706.03762)                                                                                          |
| **Major Model Milestones**                                                                        |                                                                      |      |                                                                                                                                  |
| "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"                | Jacob Devlin et al.                                                  | 2018 | [PDF](https://arxiv.org/abs/1810.04805)                                                                                          |
| "Improving Language Understanding by Generative Pre-Training" (GPT-1)                             | Alec Radford et al.                                                  | 2018 | [PDF](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf)    |
| "Language Models are Unsupervised Multitask Learners" (GPT-2)                                     | Alec Radford et al.                                                  | 2019 | [PDF](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)        |
| "Language Models are Few-Shot Learners" (GPT-3)                                                   | Tom B. Brown et al.                                                  | 2020 | [PDF](https://arxiv.org/abs/2005.14165)                                                                                          |
| "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer" (T5)          | Colin Raffel et al.                                                  | 2020 | [PDF](https://arxiv.org/abs/1910.10683)                                                                                          |
| "LLaMA: Open and Efficient Foundation Language Models"                                            | Hugo Touvron et al.                                                  | 2023 | [PDF](https://arxiv.org/abs/2302.13971)                                                                                          |
| **Scaling Laws & Efficiency**                                                                       |                                                                      |      |                                                                                                                                  |
| "Scaling Laws for Neural Language Models"                                                         | Jared Kaplan et al.                                                  | 2020 | [PDF](https://arxiv.org/abs/2001.08361)                                                                                          |
| "Training Compute-Optimal Large Language Models" (Chinchilla)                                     | Jordan Hoffmann et al.                                               | 2022 | [PDF](https://arxiv.org/abs/2203.15556)                                                                                          |
| "FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness"                     | Tri Dao et al.                                                       | 2022 | [PDF](https://arxiv.org/abs/2205.14135)                                                                                          |
| **Reasoning & Prompting**                                                                           |                                                                      |      |                                                                                                                                  |
| "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models"                           | Jason Wei et al.                                                     | 2022 | [PDF](https://arxiv.org/abs/2201.11903)                                                                                          |
| "ReAct: Synergizing Reasoning and Acting in Language Models"                                      | Shunyu Yao et al.                                                    | 2023 | [PDF](https://arxiv.org/abs/2210.03629)                                                                                          |
| "Quiet-STaR: Language Models Can Teach Themselves to Think Before Speaking"                       | Eric Zelikman et al.                                                 | 2024 | [PDF](https://arxiv.org/abs/2403.09786)                                                                                          |
| **Other Influential Papers**                                                                      |                                                                      |      |                                                                                                                                  |
| "InstructGPT: Training Language Models to Follow Instructions with Human Feedback"                | Long Ouyang et al.                                                   | 2022 | [PDF](https://arxiv.org/abs/2203.02155)                                                                                          |
| "LoRA: Low-Rank Adaptation of Large Language Models"                                              | Edward J. Hu et al.                                                  | 2021 | [PDF](https://arxiv.org/abs/2106.09685)                                                                                          |
| "Mamba: Linear-Time Sequence Modeling with Selective State Spaces"                                | Albert Gu and Tri Dao                                                | 2023 | [PDF](https://arxiv.org/abs/2312.00752)                                                                                          |
