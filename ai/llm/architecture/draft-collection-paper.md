# Personal Paper Collection

A curated collection of AI/ML research papers organized by research intent and problem domain.

## üß¨ Foundational Architecture Components

Core building blocks and techniques that underpin modern AI. These are the "primitives" that subsequent work builds upon.

| Paper | Year | Link |
|-------|------|------|
| A Neural Probabilistic Language Model (Very early 2003 NN NLP) | 2003 | [PDF](https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf) |
| Efficient Estimation of Word Representations in Vector Space | 2013 | [1301.3781](https://arxiv.org/abs/1301.3781) |
| Distributed Representations of Words and Phrases and their Compositionality (Phrase2vec) | 2013 | [1310.4546](https://arxiv.org/abs/1310.4546) |
| Generating Sequences With Recurrent Neural Networks | 2013 | [1308.0850](https://arxiv.org/abs/1308.0850) |
| Going Deeper with Convolutions (Inception) | 2014 | [1409.4842](https://arxiv.org/abs/1409.4842) |
| Very Deep Convolutional Networks for Large-Scale Image Recognition (VGGNet) | 2014 | [1409.1556](https://arxiv.org/abs/1409.1556) |
| He Initialization - Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification | 2015 | [1502.01852](https://arxiv.org/abs/1502.01852) |
| Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift | 2015 | [1502.03167](https://arxiv.org/abs/1502.03167) |
| Deep Residual Learning for Image Recognition (ResNet) | 2015 | [1512.03385](https://arxiv.org/abs/1512.03385) |
| WaveNet: A Generative Model for Raw Audio | 2016 | [1609.03499](https://arxiv.org/abs/1609.03499) |
| Proximal Policy Optimization Algorithms | 2017 | [1707.06347](https://arxiv.org/abs/1707.06347) |

## üèõÔ∏è Convolutional Architecture Evolution

The progression from basic CNNs to efficient mobile architectures, showing the refinement of convolutional networks.

| Paper | Year | Link |
|-------|------|------|
| Rethinking the Inception Architecture for Computer Vision (Inception v2) | 2015 | [1512.00567](https://arxiv.org/abs/1512.00567) |
| Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning | 2016 | [1602.07261](https://arxiv.org/abs/1602.07261) |
| MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications | 2017 | [1704.04861](https://arxiv.org/abs/1704.04861) |
| ShuffleNet: An Extremely Efficient Convolutional Neural Network for Mobile Devices | 2017 | [1707.01083](https://arxiv.org/abs/1707.01083) |
| 94% on CIFAR-10 in 3.29 Seconds on a Single GPU | 2024 | [2404.00498](https://arxiv.org/abs/2404.00498) |

## üîÆ Transformer & Attention Mechanisms

The paradigm shift to attention-based architectures and innovations for handling long contexts efficiently.

| Paper | Year | Link |
|-------|------|------|
| An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale (ViT) | 2020 | [2010.11929](https://arxiv.org/abs/2010.11929) |
| Longformer: The Long-Document Transformer (Sliding Window Attention) | 2020 | [2004.05150](https://arxiv.org/abs/2004.05150) |
| Attention is Not All You Need: Pure Attention Loses Rank Doubly Exponentially with Depth | 2021 | [2103.03404](https://arxiv.org/abs/2103.03404) |
| Neighborhood Attention Transformer | 2022 | [2204.07143](https://arxiv.org/abs/2204.07143) |
| HyperAttention: Long-context Attention in Near-Linear Time | 2023 | [2310.05869](https://arxiv.org/abs/2310.05869) |
| MatFormer: Nested Transformer for Elastic Inference | 2023 | [2310.07707](https://arxiv.org/abs/2310.07707) |
| Quest: Query-Aware Sparsity for Efficient Long-Context LLM Inference | 2024 | [2406.10774](https://arxiv.org/abs/2406.10774) |
| RetrievalAttention: Accelerating Long-Context LLM Inference via Vector Retrieval | 2024 | [2409.10516](https://arxiv.org/abs/2409.10516) |

## üß† Large Language Models: Scaling & Capabilities

Landmark models that define eras of progress in language modeling, from GPT-2 to modern efficient alternatives.

| Paper | Year | Link |
|-------|------|------|
| Better language models and their implications (GPT-2) | 2019 | [Blog](https://openai.com/index/better-language-models/) |
| Fine-Tuning Language Models from Human Preferences | 2019 | [1909.08593](https://arxiv.org/abs/1909.08593) |
| Pathways: Asynchronous Distributed Dataflow for ML | 2022 | [2203.12533](https://arxiv.org/abs/2203.12533) |
| PaLM: Scaling Language Modeling with Pathways | 2022 | [2204.02311](https://arxiv.org/abs/2204.02311) |
| The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only (Falcon) | 2023 | [2306.01116](https://arxiv.org/abs/2306.01116) |
| Orca 2: Teaching Small Language Models How to Reason | 2023 | [2311.11045](https://arxiv.org/abs/2311.11045) |
| SOLAR 10.7B: Scaling Large Language Models with Simple yet Effective Depth Up-Scaling | 2023 | [2312.15166](https://arxiv.org/abs/2312.15166) |
| Mixtral of Experts | 2024 | [2401.04088](https://arxiv.org/abs/2401.04088) |
| xLSTM: Extended Long Short-Term Memory | 2024 | [2405.04517](https://arxiv.org/abs/2405.04517) |

## ‚ö° Inference Optimization & Efficiency

Making models faster and cheaper at inference time through algorithmic, memory, and compute optimizations.

| Paper | Year | Link |
|-------|------|------|
| Fast Inference from Transformers via Speculative Decoding | 2022 | [2211.17192](https://arxiv.org/abs/2211.17192) |
| Lost in the Middle: How Language Models Use Long Contexts | 2023 | [2307.03172](https://arxiv.org/abs/2307.03172) |
| Efficient Memory Management for Large Language Model Serving with PagedAttention | 2023 | [2309.06180](https://arxiv.org/abs/2309.06180) |
| ProSG: Using Prompt Synthetic Gradients to Alleviate Prompt Forgetting of RNN-like Language Models | 2023 | [2311.01981](https://arxiv.org/abs/2311.01981) |
| MoE-Mamba: Efficient Selective State Space Models with Mixture of Experts | 2024 | [2401.04081](https://arxiv.org/abs/2401.04081) |
| The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits | 2024 | [2402.17764](https://arxiv.org/abs/2402.17764) |
| Mixture-of-Depths: Dynamically allocating compute in transformer-based language models | 2024 | [2404.02258](https://arxiv.org/abs/2404.02258) |
| KAN: Kolmogorov-Arnold Networks | 2024 | [2404.19756](https://arxiv.org/abs/2404.19756) |
| Better & Faster Large Language Models via Multi-token Prediction | 2024 | [2404.19737](https://arxiv.org/abs/2404.19737) |
| MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention | 2024 | [2407.02490](https://arxiv.org/abs/2407.02490) |

## üèóÔ∏è Training Infrastructure & Distributed Systems

Engineering systems for training massive models across many GPUs, from memory optimization to distributed parallelism.

| Paper | Year | Link |
|-------|------|------|
| ZeRO: Memory Optimizations Toward Training Trillion Parameter Models | 2019 | [1910.02054](https://arxiv.org/abs/1910.02054) |
| GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding | 2020 | [2006.16668](https://arxiv.org/abs/2006.16668) |
| Fully Sharded Data Parallel: faster AI training with fewer GPUs (Engineering at Meta) | 2021 | [Blog](https://engineering.fb.com/2021/07/15/open-source/fsdp/) |
| Introducing PyTorch Fully Sharded Data Parallel (FSDP) API | 2022 | [Blog](https://pytorch.org/blog/introducing-pytorch-fully-sharded-data-parallel-api/) |
| OSDP: Optimal Sharded Data Parallel for Distributed Deep Learning | 2022 | [2209.13258](https://arxiv.org/abs/2209.13258) |
| PyTorch FSDP: Experiences on Scaling Fully Sharded Data Parallel | 2023 | [2304.11277](https://arxiv.org/abs/2304.11277) |
| Large Scale Training of Hugging Face Transformers on TPUs With PyTorch/XLA FSDP | 2023 | [Blog](https://pytorch.org/blog/large-scale-training-hugging-face/) |
| FSDP Cites | 2023 | [2304.11277](https://arxiv.org/abs/2304.11277) |

## üß™ Fine-tuning & Adaptation Techniques

Efficient methods for customizing pretrained models to specific tasks without full retraining.

| Paper | Year | Link |
|-------|------|------|
| Intrinsic Dimensionality Explains the Effectiveness of Language Model Fine-Tuning | 2020 | [2012.13255](https://arxiv.org/abs/2012.13255) |
| LQ-LoRA: Low-rank Plus Quantized Matrix Decomposition for Efficient Language Model Finetuning | 2023 | [2311.12023](https://arxiv.org/abs/2311.12023) |
| RAG vs Fine-tuning: Pipelines, Tradeoffs, and a Case Study on Agriculture | 2024 | [2401.08406](https://arxiv.org/abs/2401.08406) |
| LoRA Learns Less and Forgets Less | 2024 | [2405.09673](https://arxiv.org/abs/2405.09673) |
| Evolution of meta's llama models and parameter-efficient fine-tuning of large language models: a survey | 2025 | [2510.12178](https://arxiv.org/abs/2510.12178) |

## üé® Generative Models & Multimodal AI

From GANs to diffusion models, and the integration of vision and language for creative AI applications.

| Paper | Year | Link |
|-------|------|------|
| Diffusion Models Beat GANs on Image Synthesis | 2021 | [2105.05233](https://arxiv.org/abs/2105.05233) |
| Hierarchical Text-Conditional Image Generation with CLIP Latents (DALLE 2) | 2022 | [2204.06125](https://arxiv.org/abs/2204.06125) |
| Scalable Diffusion Models with Transformers | 2022 | [2212.09748](https://arxiv.org/abs/2212.09748) |
| Adding Conditional Control to Text-to-Image Diffusion Models (ControlNet) | 2023 | [2302.05543](https://arxiv.org/abs/2302.05543) |
| Visual ChatGPT: Talking, Drawing and Editing with Visual Foundation Models | 2023 | [2303.04671](https://arxiv.org/abs/2303.04671) |
| HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face | 2023 | [2303.17580](https://arxiv.org/abs/2303.17580) |
| 4M-21: An Any-to-Any Vision Model for Tens of Tasks and Modalities | 2024 | [2406.09406](https://arxiv.org/abs/2406.09406) |
| Vision language models are blind | 2024 | [2407.06581](https://arxiv.org/abs/2407.06581) |
| Contextual Document Embeddings | 2024 | [2410.02525](https://arxiv.org/abs/2410.02525) |

## ü§ñ Agents & Embodied AI

Models that go beyond text completion to plan, act, and interact with environments from games to the real world.

| Paper | Year | Link |
|-------|------|------|
| Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model | 2019 | [1911.08265](https://arxiv.org/abs/1911.08265) |
| WebGPT: Browser-assisted question-answering with human feedback | 2021 | [2112.09332](https://arxiv.org/abs/2112.09332) |
| Voyager: An Open-Ended Embodied Agent with Large Language Models | 2023 | [2305.16291](https://arxiv.org/abs/2305.16291) |
| JARVIS-1: Open-World Multi-task Agents with Memory-Augmented Multimodal Language Models | 2023 | [2311.05997](https://arxiv.org/abs/2311.05997) |
| DriveMLM: Aligning Multi-Modal Large Language Models with Behavioral Planning States for Autonomous Driving | 2023 | [2312.09245](https://arxiv.org/abs/2312.09245) |
| RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control | 2023 | [2307.15818](https://arxiv.org/abs/2307.15818) |
| OS-Copilot: Towards Generalist Computer Agents with Self-Improvement | 2024 | [2402.07456](https://arxiv.org/abs/2402.07456) |

## üî¨ Theory & Understanding

Papers that explain why and how models work, offering fundamental insights into intelligence and model behavior.

| Paper | Year | Link |
|-------|------|------|
| On the Measure of Intelligence | 2019 | [1911.01547](https://arxiv.org/abs/1911.01547) |
| Physics of Language Models: Part 1, Context-Free Grammar | 2023 | [2305.13673](https://arxiv.org/abs/2305.13673) |
| Pretraining Data Mixtures Enable Narrow Model Selection Capabilities in Transformer Models (can't generalize) | 2023 | [2311.00871](https://arxiv.org/abs/2311.00871) |
| System 2 Attention (is something you might need too) | 2023 | [2311.11829](https://arxiv.org/abs/2311.11829) |
| The Instruction Hierarchy: Training LLMs to Prioritize Privileged Instructions | 2024 | [2404.13208](https://arxiv.org/abs/2404.13208) |
| Toward Self-Improvement of LLMs via Imagination, Searching, and Criticizing | 2024 | [2404.12253](https://arxiv.org/abs/2404.12253) |
| Training Language Models to Self-Correct via Reinforcement Learning | 2024 | [2409.12917](https://arxiv.org/abs/2409.12917) |

## üóúÔ∏è Compression & Information Theory

Exploring the deep connections between compression, prediction, and learning through information theory.

| Paper | Year | Link |
|-------|------|------|
| Large Language Models and Nearest Neighbors | 2023 | [Blog](https://magazine.sebastianraschka.com/p/llms-and-nearest-neighbors) |
| Why gzip Just Beat a Large Language Model (Hendrik Erz) | 2023 | [Blog](https://www.hendrik-erz.de/post/why-gzip-just-beat-a-large-language-model) |
| LLMZip: Lossless Text Compression using Large Language Models | 2023 | [2306.04050](https://arxiv.org/abs/2306.04050) |
| Language Modeling Is Compression | 2023 | [2309.10668](https://arxiv.org/abs/2309.10668) |

## üßÆ Systems & Compilers

Making ML run efficiently on hardware through compilers, kernels, and systems-level optimization.

| Paper | Year | Link |
|-------|------|------|
| Triton: An Intermediate Language and Compiler for Tiled Neural Network Computations | 2019 | [PDF](https://www.eecs.harvard.edu/~htk/publication/2019-mapl-tillet-kung-cox.pdf) |
| The Deep Learning Compiler: A Comprehensive Survey | 2020 | [2002.03794](https://arxiv.org/abs/2002.03794) |
| Large Language Models for Compiler Optimization | 2023 | [2309.07062](https://arxiv.org/abs/2309.07062) |

## üéØ Benchmarks & Evaluation

How we measure progress and compare different approaches in AI research.

| Paper | Year | Link |
|-------|------|------|
| ConvNets Match Vision Transformers at Scale | 2023 | [2310.16764](https://arxiv.org/abs/2310.16764) |
| Battle of the Backbones: A Large-Scale Comparison of Pretrained Models across Computer Vision Tasks | 2023 | [2310.19909](https://arxiv.org/abs/2310.19909) |
| GAIA: a benchmark for General AI Assistants | 2023 | [2311.12983](https://arxiv.org/abs/2311.12983) |
| Weak-to-strong generalization (Paper) | 2023 | [2312.09390](https://arxiv.org/abs/2312.09390) |

## üåê Domain-Specific Applications

AI applied to solve problems in specific domains beyond general-purpose tasks.

| Paper | Year | Link |
|-------|------|------|
| GraphCast: Learning skillful medium-range global weather forecasting | 2022 | [2212.12794](https://arxiv.org/abs/2212.12794) |
| SpatialVLM: Endowing Vision-Language Models with Spatial Reasoning Capabilities | 2024 | [2401.12168](https://arxiv.org/abs/2401.12168) |
| ReALM: Reference Resolution As Language Modeling | 2024 | [2403.20329](https://arxiv.org/abs/2403.20329) |
| RedOne: Revealing Domain-specific LLM Post-Training in Social Networking Services | 2025 | [2507.10605](https://arxiv.org/abs/2507.10605) |

## üîó Resources & Blog Posts

| Title | Type | Link |
|-------|------|------|
| Efficient Fine-Tuning with LoRA: A Guide to Optimal Parameter Selection for Large Language Models (Databricks Blog) | Blog | [Link](https://www.databricks.com/blog/efficient-fine-tuning-lora-guide-llms) |
| Weak-to-strong generalization (OpenAI) | Article | [Blog](https://openai.com/index/weak-to-strong-generalization/) |
