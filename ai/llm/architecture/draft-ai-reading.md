# AI Reading List

A curated collection of AI/ML articles, blog posts, tutorials, and resources.

## üìö Reading List

| Title | Type | Source | URL |
|-------|------|--------|-----|
| **Haohuan's Recommendations** | | | |
| Pallas Design ‚Äî JAX documentation | Documentation | JAX | - |
| Autodidax: JAX core from scratch ‚Äî JAX documentation | Tutorial | JAX | - |
| GSPMD: General and Scalable Parallelization for ML Computation Graphs | Paper | arXiv | [2105.04663](https://arxiv.org/abs/2105.04663) |
| Cloud TPU Multislice Overview [Public Preview] | Documentation | Google Cloud | - |
| **From Old List** | | | |
| A brief history of TensorFlow Extended (TFX) | Blog | TensorFlow | https://blog.tensorflow.org/2020/09/brief-history-of-tensorflow-extended-tfx.html |
| An Introduction To HuggingFace Transformers for NLP | Tutorial | Weights & Biases | https://wandb.ai/int_pb/huggingface/reports/An-Introduction-To-HuggingFace-Transformers-for-NLP--VmlldzoyOTgzMjI5 |
| Function calling and other API updates | Blog | OpenAI | https://openai.com/blog/function-calling-and-other-api-updates |
| Neural Network Architectures Guide | Tutorial | V7 Labs | https://www.v7labs.com/blog/neural-network-architectures-guide |
| Neural Network Architectures | Article | Towards Data Science | https://towardsdatascience.com/neural-network-architectures-156e5bad51ba |
| Information Theory, Inference and Learning Algorithms | Book | David MacKay | https://www.inference.org.uk/mackay/itila/ |
| Adversarial Attacks on LLMs | Blog | Lil'Log | - |
| At the Intersection of LLMs and Kernels - Research Roundup | Article | - | - |
| The Transient Nature of Emergent In-Context Learning in Transformers | Paper | arXiv | [2311.08360](https://arxiv.org/abs/2311.08360) |
| AI at Scale | Research | Microsoft Research | - |
| ZeRO & DeepSpeed: New system optimizations enable training models with over 100 billion parameters | Blog | Microsoft Research | - |
| Accelerate PyTorch workloads with PyTorch/XLA | Video | YouTube | - |
| FunSearch: Making new discoveries in mathematical sciences using Large Language Models | Research | Google DeepMind | - |
| What I Wish Someone Had Told Me | Blog | Sam Altman | - |
| 10 Noteworthy AI Research Papers of 2023 | Article | - | - |
| llm-course: Course to get into Large Language Models (LLMs) with roadmaps and Colab notebooks | GitHub | mlabonne | - |
| Awesome-CUDA: A list of useful libraries and resources for CUDA development | GitHub | Erkaman | - |
| Hands-On GPU Programming with Python and CUDA | GitHub | Packt Publishing | - |
| NVIDIA cuda-samples: Samples for CUDA Developers | GitHub | NVIDIA | - |
| Integrating NVIDIA TensorRT-LLM with the Databricks Inference Stack | Blog | Databricks | - |
| My Fave New Podcasts of 2023 | Blog | - | - |
| Introducing Mixtral 8x7B with Databricks Model Serving | Blog | Databricks | - |
| Getting Started with Mixtral 8X7B | Tutorial | Pinecone | - |
| Welcome to the ü§ó Deep Reinforcement Learning Course | Course | Hugging Face | - |
| What do I need to know about Mamba in the ML world | Article | - | - |
| The promise and challenges of crypto + AI applications | Article | - | - |
| Large Language Models, How to Train Them, and xAI's Grok | Article | - | - |
| Practical Tips for Finetuning LLMs Using LoRA (Low-Rank Adaptation) | Tutorial | - | - |
| Human-like systematic generalization through a meta-learning neural network | Paper | Nature | - |
| OpenAI: Facts from a Weekend | Article | LessWrong | - |
| A checklist for switching to open source ML models | Guide | - | - |
| Distill ‚Äî Latest articles about machine learning | Website | Distill | - |
| Research Papers in January 2024 | Newsletter | Sebastian Raschka | - |
| V-JEPA: The next step toward advanced machine intelligence | Research | Meta | - |
| Improving LoRA: Implementing Weight-Decomposed Low-Rank Adaptation (DoRA) from Scratch | Tutorial | - | - |
| ML-YouTube-Courses: Discover the latest machine learning / AI courses on YouTube | GitHub | dair-ai | - |
| SolidGoldMagikarp (plus, prompt generation) | Article | LessWrong | - |
| Explaining SolidGoldMagikarp by looking at it from random directions | Article | LessWrong | - |
| Mamba No. 5 (A Little Bit Of‚Ä¶) | Article | Sparse Notes | - |
| Mamba: The Easy Way | Tutorial | - | - |
| Mamba: The Hard Way | Tutorial | - | - |
| q star info.pdf | Document | Google Drive | - |
| Ilya Sutskever: The brain behind ChatGPT | Article | - | - |
| Training great LLMs entirely from ground zero in the wilderness as a startup | Blog | Yi Tay | - |
| Ben Thimpson podcast | Podcast | - | - |
| Is Cosine-Similarity of Embeddings Really About Similarity? | Paper | arXiv | [2403.05440](https://arxiv.org/abs/2403.05440) |
| OpenXLA is available now to accelerate and simplify machine learning | Blog | Google Open Source | - |
| Many-shot jailbreaking | Research | Anthropic | - |
| The Data Center is the New Compute Unit: Nvidia's Vision for System-Level Scaling | Article | - | - |
| Techniques for training large neural networks | Guide | OpenAI | - |
| The pitfalls of next-token prediction | Paper | arXiv | [2403.06963](https://arxiv.org/abs/2403.06963) |
| The N Implementation Details of RLHF with PPO | Blog | ICLR Blogposts 2024 | - |
| Dear VC's, please stop throwing money at AI founders with no commercial plan, besides AGI | Opinion | - | - |
| Are all LLMs really 1.58 bits? Inference at 4x the speed or more? | Article | - | - |
| Medusa: Simple framework for accelerating LLM generation with multiple decoding heads | Research | - | - |
| Ilya's recom to Carmack | Reference | - | - |
| Speech and Language Processing | Textbook | - | - |
| We need to talk about Agents... | Article | - | - |
| What can LLMs never do? | Article | Rohit Krishnan | - |
| How to Optimize a CUDA Matmul Kernel for cuBLAS-like Performance: a Worklog | Tutorial | - | - |
| Quantizing deep convolutional networks for efficient inference: A whitepaper | Paper | arXiv | [1806.08342](https://arxiv.org/abs/1806.08342) |
| Claude's Character | Research | Anthropic | - |
| Quantization aware training | Documentation | TensorFlow Model Optimization | - |
| Scalable MatMul-free Language Modeling | Paper | arXiv | [2406.02528](https://arxiv.org/abs/2406.02528) |
| Introducing Apple's On-Device and Server Foundation Models | Research | Apple ML Research | - |
| Qwen 2: a story of Alibaba vs Meta | Article | Rick Lamers | - |
| OpenVLA: An Open-Source Vision-Language-Action Model | Research | - | - |
| Transformer Architecture: The Positional Encoding | Blog | Amirhossein Kazemnejad | - |
| [RFC] PagedAttention Support | GitHub Issue | pytorch/pytorch | - |
| Extrinsic Hallucinations in LLMs | Blog | Lil'Log | - |
| Meta PyTorch Team 2024 H2 Roadmaps | Mailing List | PyTorch Developer | - |
| Prime Intellect: OpenDiLoCo | Research | Prime Intellect | - |
| X OpenDiLoCo | Tweet | X/Twitter | - |
| Apple Foundation Model | Research | Apple | - |
| Why do LLM input tokens cost less than output tokens? | Explainer | - | - |
| Welcome to Model Optimizer (ModelOpt) documentation! | Documentation | NVIDIA | - |
| x.com Meta Agent | Tweet | X/Twitter | - |
| Triton Architecture | Documentation | NVIDIA Triton Inference Server | - |
| TensorRT-LLM Architecture | Documentation | TensorRT-LLM | - |
| What's Really Going On in Machine Learning? Some Minimal Models | Article | Stephen Wolfram | - |
| Fire-Flyer AI-HPC: A Cost-Effective Software-Hardware Co-Design for Deep Learning | Paper | arXiv | [2408.14158](https://arxiv.org/abs/2408.14158) |
| awesome-dspy | GitHub | tom-doerr | - |
| Azure/kaito: Kubernetes AI Toolchain Operator | GitHub | Azure | - |
| TensorRT explained | Article | aijobs.net | - |
| The Evolution of Multimodal Model Architectures | Paper | arXiv | [2405.17927](https://arxiv.org/abs/2405.17927) |
| Dynamic sparse attention | Tweet | X/Twitter | - |
| $2 H100s: How the GPU Bubble Burst | Article | Eugene Cheah | - |
| vLLM v0.6.0: 2.7x Throughput Improvement and 5x Latency Reduction | Blog | vLLM | - |
| Making Deep Learning go Brrrr From First Principles | Tutorial | - | - |
| Glow | Framework | - | - |
| ML Compilers Part 2: An Overview of Graph Optimizations | Blog | FuzzyWare | - |
| GPU-Guide: Graphics Processing Unit (GPU) Architecture Guide | GitHub | mikeroyal | - |
| Blog list | Collection | - | - |
| GSM-Symbolic: Understanding the Limitations of Mathematical Reasoning in Large Language Models | Paper | arXiv | [2410.05229](https://arxiv.org/abs/2410.05229) |
| Distilling Llama3.1 8B into 1B in torchtune | Tutorial | PyTorch | - |
| Archive ‚Ä¢ AI News ‚Ä¢ Buttondown | Newsletter | AI News | - |
| Hugging Face ‚Äì Blog | Blog | Hugging Face | - |
| vllm quantization | Documentation | vLLM | - |
| how-to-optim-algorithm-in-cuda: how to optimize some algorithm in cuda | GitHub | BBuf | - |
| INTELLECT-1 Release The First Globally Trained 10B Parameter Model | Announcement | Prime Intellect | - |
| INTELLECT_1_Technical_Report.pdf | Technical Report | PrimeIntellect-ai | - |
| Making WAF ML models go brrr: saving decades of processing time | Blog | Cloudflare | - |
| Claude 3.5 Sonnet Insane Coding Ability | Article | - | - |
| AI-Engineering.academy: Mastering Applied AI, One Concept at a Time | GitHub | adithya-s-k | - |
| Late Takes on OpenAI o1 | Article | - | - |
| Ways to use torch.compile | Blog | ezyang's blog | - |
| On AI for developer productivity | Article | - | - |
| Scaling test-time compute | Demo | Hugging Face Space | - |
| Scaling LLM Test-Time Compute Optimally can be More Effective than Scaling Model Parameters | Paper | arXiv | [2408.03314](https://arxiv.org/abs/2408.03314) |
| Building effective agents | Guide | Anthropic | - |
| 2024 best papers | Collection | - | - |
| Learning CUDA by optimizing softmax: A worklog | Tutorial | Maharshi's blog | - |
| Reinforcement learning from AI feedback (RLAIF): Complete overview | Tutorial | SuperAnnotate | - |
| Agents | Article | - | - |
| Video Generation Models Explosion 2024 | Article | Yen-Chen Lin | - |
| Tenstorrent Blackhole, Grendel, And Buda ‚Äì A Scale Out Architecture | Analysis | SemiAnalysis | - |
| Dario Amodei ‚Äî On DeepSeek and Export Controls | Article | - | - |
| Foundations of Large Language Models (this is a book) | Book | arXiv | [2501.09223](https://arxiv.org/abs/2501.09223) |
| How multi-node inference works for massive LLMs like DeepSeek-R1 | Blog | Baseten | - |
| Understanding Peak, Max-Achievable & Delivered FLOPs, Part 1 | Tutorial | ROCm Blogs | - |
| LLMËÆ≠ÁªÉÁªàÊûÅÊåáÂçó | The Ultra-Scale Playbook | Guide | Hugging Face Space | - |
| Minions: Cost-efficient Collaboration Between On-device and Cloud Language Models | Paper | arXiv | [2502.15964](https://arxiv.org/abs/2502.15964) |
| Circuit Tracing: Revealing Computational Graphs in Language Models | Research | - | - |
| On the Biology of a Large Language Model | Article | - | - |
| TAO: Using test-time compute to train efficient LLMs without labeled data | Blog | Databricks | - |
| RLHF Book by Nathan Lambert | Book | Nathan Lambert | - |
| CUTLASS Tutorial: Writing GEMM Kernels Using Tensor Memory For NVIDIA¬Æ Blackwell GPUs | Tutorial | Colfax Research | - |
| The Way of Code | Article | Rick Rubin | - |
| Why I No Longer Recommend RAG for Autonomous Coding Agents | Opinion | - | - |
| xAI's Grok 4: The tension of frontier performance with a side of Elon favoritism | Analysis | - | - |
| Awesome-ML-SYS-Tutorial: My learning notes/codes for ML SYS | GitHub | zhaochenyang20 | - |
| The Ultra-Scale Playbook | Guide | Hugging Face Space | - |
| TensorRT-LLM Architecture | Documentation | TensorRT-LLM | - |
| A Manual Implementation of Quantization in PyTorch - Single Layer | Tutorial | Hexo | - |
| Measuring the environmental impact of AI inference | Blog | Google Cloud | - |
| How to Think About GPUs | Guide | How To Scale Your Model | - |
| A Primer on LLM Post-Training | Tutorial | PyTorch | - |
| AI 2027 | Prediction | - | - |
| How To Scale Your Model | Guide | - | - |
| Getting Started With CuTe | Documentation | NVIDIA CUTLASS | - |
| AMD Data Center GPUs Explained: MI250X, MI300X, MI350X and Beyond | Guide | - | - |
| GPU L2 Cache Persistence | Blog | simons blog | - |
| RedOne: Revealing Domain-specific LLM Post-Training in Social Networking Services | Paper | arXiv | [2507.10605](https://arxiv.org/abs/2507.10605) |
| The Big LLM Architecture Comparison | Article | - | - |
| The Training Imperative | Article | - | - |
| checkpoint-engine: Checkpoint-engine is a simple middleware to update model weights | GitHub | MoonshotAI | - |
| Improving Cursor Tab With RL | Blog | Cursor | - |
| Paged Attention from First Principles: A View Inside vLLM | Blog | Hamza's Blog | - |
| Speculative cascades ‚Äî A hybrid approach for smarter, faster LLM inference | Article | - | - |
| Cornell Virtual Workshop: Understanding GPU Architecture | Workshop | Cornell | - |
| Disaggregated Inference at Scale with PyTorch & vLLM | Tutorial | PyTorch | - |
| SGLang v0.4: Zero-Overhead Batch Scheduler, Cache-Aware Load Balancer | Blog | LMSYS Org | - |
| sglang pdf | Document | - | - |
| Tree Attention: Topology-aware Decoding for Long-Context Attention on GPU clusters | Paper | arXiv | [2408.04093](https://arxiv.org/abs/2408.04093) |
| Will Amazon S3 Vectors Kill Vector Databases‚Äîor Save Them? | Article | Zilliz blog | - |
| Post-training 101 | Tutorial | Tokens for Thoughts | - |
| Online RL for Cursor Tab | Blog | Cursor | - |
| Stanford CRFM | Research Group | Stanford | - |
| Surprisingly Fast AI-Generated Kernels We Didn't Mean to Publish (Yet) | Research | Stanford Scaling Intelligence Lab | - |
| Advanced RAG Techniques: an Illustrated Overview | Article | Towards AI | - |
| Modular Manifolds | Research | Thinking Machines Lab | - |
| Inside NVIDIA GPUs: Anatomy of high performance matmul kernels | Tutorial | Aleksa Gordiƒá | - |
| Weight Transfer for RL Post-Training in under 2 seconds | Article | - | - |
| Deploying DeepSeek with PD Disaggregation and Large-Scale Expert Parallelism on 96 H100 GPUs | Blog | LMSYS Org | - |
| LLM evolution | Article | - | - |
| Qwen 3 | Announcement | Qwen | - |
| flash decoding | Research | - | - |
| Dissecting Batching Effects in GPT Inference | Analysis | - | - |
| How to think about GPU | Guide | - | - |
