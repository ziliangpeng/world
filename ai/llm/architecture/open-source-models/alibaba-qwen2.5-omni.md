# Qwen2.5-Omni: Revolutionary Thinker-Talker Architecture for Real-time Multimodal Understanding

## Overview

**Qwen2.5-Omni** represents a revolutionary advancement in multimodal AI, introducing the groundbreaking **Thinker-Talker architecture** that enables simultaneous generation of text and natural speech while maintaining true omni-modal understanding across text, images, audio, and video. Released in March 2025 by the Qwen Team at Alibaba Cloud, this flagship model achieves state-of-the-art performance on multimodal integration benchmarks while supporting real-time streaming conversations with remarkably low latency.

The model's innovative dual-component design separates "thinking" (comprehension and reasoning) from "speaking" (speech generation), allowing for concurrent output streams without interference. Combined with **TMRoPE (Time-aligned Multimodal RoPE)** for audio-video synchronization and block-wise processing for streaming, Qwen2.5-Omni sets a new standard for end-to-end multimodal models.

### Quick Facts

- **Release Date**: March 26-27, 2025
- **Developer**: Qwen Team, Alibaba Cloud
- **Model Sizes**: 3B, 7B parameters (LLM component)
- **Total Parameters**: ~6B (3B variant), ~10B (7B variant)
- **License**: Apache 2.0 (fully open source)
- **Context Length**: 32,768 tokens
- **Training Data**: 18T text + 1.2T multimodal tokens
- **Modalities**: Text, images, audio, video â†’ Text + natural speech
- **arXiv Paper**: [2503.20215](https://arxiv.org/abs/2503.20215)

### Model Variants

| Model | LLM Params | Total Params | Model Size | GPU Memory (15-60s video) | Key Features |
|-------|------------|--------------|------------|---------------------------|--------------|
| **Qwen2.5-Omni-7B** | 7B | ~10B | 22.4 GB | 31-60 GB | Flagship performance |
| **Qwen2.5-Omni-3B** | 3B | ~6B | ~12 GB | 18-28 GB | 50% lower VRAM, 90% of 7B performance |

**Note**: GPU memory estimates assume BF16 precision + Flash Attention 2 for video processing.

---

## Key Innovations

### 1. Thinker-Talker Architecture

**Revolutionary Dual-Component Design**: Separates cognitive processing (thinking) from speech generation (talking).

#### Thinker Component

**Function**: The "brain" of the system

**Responsibilities**:
- Process multimodal inputs (text, audio, images, video)
- Perform reasoning and comprehension
- Generate text output (visible response)
- Produce hidden representations (internal understanding)
- Stream representations to Talker in real-time

**Architecture**:
- Large language model (Qwen2.5-3B or Qwen2.5-7B)
- Multimodal encoders (vision + audio)
- Shared attention mechanism for multimodal fusion
- TMRoPE for time-aligned position encoding

#### Talker Component

**Function**: The "mouth" of the system

**Responsibilities**:
- Receive streaming hidden representations from Thinker
- Receive streaming text output from Thinker
- Generate natural speech audio tokens
- Maintain speaker characteristics and naturalness
- Output speech concurrently with text

**Architecture**:
- Dual-track autoregressive model
- Sliding-window DiT (Diffusion Transformer) for speech decoding
- Multi-speaker fine-tuning support
- Streaming audio generation with low latency

#### Information Flow

```
Multimodal Inputs (text/audio/images/video)
           â†“
    [Thinker Component]
    - Vision Encoder (ViT)
    - Audio Encoder (Whisper)
    - Language Model (Qwen2.5)
    - TMRoPE Fusion
           â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  Thinker Outputs:        â”‚
    â”‚  1. Text tokens          â”‚
    â”‚  2. Hidden representationsâ”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â†“
        â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”
        â”‚              â”‚
        â†“              â†“
   Text Output    [Talker Component]
   (streaming)    - Dual-track AR
                  - Sliding-window DiT
                  - Speech decoding
                       â†“
                  Speech Output
                  (streaming)
```

**Key Benefits**:
- **No Interference**: Text and speech generation don't conflict
- **Streaming**: Both outputs stream simultaneously in real-time
- **Natural Flow**: "Thinking" directly informs "speaking"
- **Low Latency**: Talker starts generating as soon as Thinker produces representations
- **Flexibility**: Can generate text-only, speech-only, or both concurrently

#### Comparison with Traditional Approaches

| Architecture | Qwen2.5-Omni (Thinker-Talker) | Traditional (Single-Track) |
|--------------|-------------------------------|---------------------------|
| Text + Speech | Concurrent, no interference | Sequential or conflicting |
| Latency | Low (streaming from hidden reps) | High (wait for full text) |
| Quality | Natural, coherent speech | May sound robotic or delayed |
| Flexibility | Independent streams | Coupled outputs |
| Real-time | Full streaming support | Limited or no streaming |

### 2. TMRoPE: Time-aligned Multimodal RoPE

**Core Innovation**: Novel position embedding that synchronizes timestamps of video inputs with audio.

**Mechanism**:
- Extends standard RoPE (Rotary Position Embedding) to multimodal context
- Aligns audio and video timelines explicitly
- Organizes audio and video sequentially in interleaved manner
- Enables cohesive understanding of audio-visual synchronization

**Benefits**:
- Natural handling of audio-video correspondence
- Better understanding of temporal relationships across modalities
- Improved performance on audio-visual reasoning tasks
- Foundation for processing hour-long videos with audio

**Implementation**:
```
Standard RoPE: Î¸(position)
TMRoPE: Î¸(modality, time_alignment, position)

Example for video frame at time t with audio at time t:
Video token: Î¸(video, t, spatial_pos)
Audio token: Î¸(audio, t, temporal_pos)
â†’ Aligned in the same temporal space
```

### 3. Block-wise Processing for Streaming

**Challenge**: Process long audio and video sequences efficiently in real-time.

**Solution**: Block-wise processing with temporal block-wise attention.

**Audio Processing**:
- Process audio in **2-second blocks**
- Approximately 40ms per frame after pooling
- Enables streaming without waiting for full input
- Start generating answers while still receiving user's speech

**Vision Processing**:
- Block-wise video frame processing
- Synchronized with audio blocks via TMRoPE
- Efficient handling of long videos (hour-long with Stage 3 training)

**Benefits**:
- **Minimal Latency**: Early inference without full input
- **Efficient Memory**: Process long sequences without excessive memory
- **Scalability**: Handle arbitrarily long audio/video streams
- **Real-time**: True real-time conversation capability

### 4. Sliding-Window DiT for Low-Latency Speech

**Challenge**: Reduce initial package delay for real-time speech synthesis.

**Solution**: Sliding-window Diffusion Transformer with restricted receptive field.

**Design**:
- Restricts receptive field to small window (e.g., two lookback blocks)
- Enables streaming audio token generation
- Talker can start generating speech from partial Thinker output

**Latency Sources Reduced**:
1. Multimodal information processing delay
2. First text-to-voice token latency
3. First speech segment conversion delay
4. Architectural inherent latency

**Performance**: Outperforms most existing streaming and non-streaming speech synthesis alternatives in robustness and naturalness.

---

## Architecture Details

### Vision Encoder

**Base Architecture**: Vision Transformer (ViT)

**Specifications**:
- **Parameters**: ~675M (shared with Qwen2.5-VL)
- **Source**: Encoder from Qwen2.5-VL
- **Training**: Pre-trained on both images and videos
- **Processing**: Block-wise for streaming capability

**Integration**:
- Converts images and video frames to visual tokens
- Feeds into Thinker's language model via shared attention
- Synchronized with audio via TMRoPE

### Audio Encoder

**Base Model**: Whisper-large-v3 (pretrained)

**Audio Preprocessing**:
- All audio resampled to **16 kHz** frequency
- Raw waveform â†’ 128-channel mel-spectrogram
- **Window size**: 25ms
- **Hop length**: 10ms

**Architecture**:
- Encoder initialized from Whisper-large-v3
- Includes pooling layer to reduce dimensionality
- Each consecutive frame â‰ˆ **40ms** of original audio
- Processes audio in **2-second blocks** with temporal block-wise attention

**Processing Flow**:
```
Raw Audio (16kHz)
     â†“
Mel-Spectrogram (128 channels, 25ms window, 10ms hop)
     â†“
Whisper-large-v3 Encoder
     â†“
Pooling Layer (~40ms per frame)
     â†“
Audio Tokens (2-second blocks)
     â†“
TMRoPE Position Encoding
     â†“
Thinker LLM
```

### Language Model

**Base Models**: Qwen2.5 series

| Variant | Base LLM | Parameters |
|---------|----------|------------|
| Qwen2.5-Omni-7B | Qwen2.5-7B | 7B |
| Qwen2.5-Omni-3B | Qwen2.5-3B | 3B |

**Architecture**:
- Grouped Query Attention (GQA)
- SwiGLU activation functions
- RMSNorm normalization
- Rotary Position Embedding (RoPE) for text
- TMRoPE for multimodal fusion

**Context Length**: 32,768 tokens (extended in Stage 3 training)

**Extensions**: Can utilize YaRN technique for length extrapolation beyond 32K tokens

### Multimodal Fusion

**Shared Attention Mechanism**:
- Enhances fusion of text, vision, and audio modalities
- Differentiates inputs from different modalities while reasoning jointly
- Enables cross-modal understanding and reasoning

**TMRoPE Integration**:
- Synchronizes audio and video timestamps
- Aligns multimodal sequences temporally
- Enables cohesive audio-visual understanding

**Block-wise Architecture**:
- Decouples handling of long sequences
- Assigns perceptual responsibilities to multimodal encoders
- Entrusts modeling of extended sequences to LLM
- Facilitates efficient streaming (2-second audio segments)

### Speech Generation (Talker)

**Dual-track Autoregressive Model**:
- Takes hidden representations from Thinker
- Takes text tokens from Thinker
- Generates discrete speech tokens fluidly

**Sliding-Window DiT**:
- Diffusion Transformer with restricted receptive field
- Enables streaming generation with low latency
- Two lookback blocks for context
- Superior robustness and naturalness

**Multi-Speaker Support**:
- **Chelsie** (Female, default): Honeyed, velvety voice with gentle warmth
- **Ethan** (Male): Bright, upbeat voice with infectious energy
- Fine-tuned to maintain naturalness while adopting specific voices

**Speech Quality Optimizations**:
- In-Context Learning (ICL) for speech continuation patterns
- Direct Preference Optimization (DPO) to mitigate hallucinations
- Word Error Rate metrics for quality assessment

### Model Architecture Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   Qwen2.5-Omni Architecture                        â”‚
â”‚                   (Thinker-Talker Design)                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Text Input â”‚  â”‚  Image   â”‚  â”‚   Audio    â”‚  â”‚   Video   â”‚
â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
      â”‚              â”‚               â”‚                â”‚
      â”‚              â†“               â†“                â†“
      â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
      â”‚         â”‚ Vision ViT  â”‚  â”‚ Whisper-v3   â”‚   â”‚
      â”‚         â”‚  (~675M)    â”‚  â”‚ Audio Encoderâ”‚   â”‚
      â”‚         â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
      â”‚                â”‚                â”‚            â”‚
      â”‚                â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤            â”‚
      â”‚                â””â”€â”€â”¤  TMRoPE     â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
      â”‚                   â”‚  Alignment  â”‚
      â”‚                   â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
      â”‚                          â”‚
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â†“
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚   Thinker Component        â”‚
        â”‚                            â”‚
        â”‚   Qwen2.5 LLM (3B/7B)      â”‚
        â”‚   - Shared Attention       â”‚
        â”‚   - Multimodal Fusion      â”‚
        â”‚   - Reasoning & Generation â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                  â”‚
        â†“                  â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Text Output  â”‚   â”‚ Hidden Reps      â”‚
â”‚  (streaming)  â”‚   â”‚ (to Talker)      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                             â†“
                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                  â”‚  Talker Component    â”‚
                  â”‚                      â”‚
                  â”‚  - Dual-track AR     â”‚
                  â”‚  - Sliding-window DiTâ”‚
                  â”‚  - Speech Decoding   â”‚
                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                             â†“
                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                  â”‚  Speech Output       â”‚
                  â”‚  (streaming, natural)â”‚
                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

     â•‘ Concurrent Streaming Generation â•‘
```

---

## Training Details

### Training Data Composition

**Total Scale**: Massive multimodal dataset across text, vision, and audio

**Text Data**:
- **18 trillion tokens** (identical to Qwen2.5)
- Multilingual coverage (English, Chinese, and others)
- Same high-quality text corpus as Qwen2.5

**Vision Data**:
- **800 billion tokens** of images and videos
- Diverse visual content
- Aligned with Qwen2.5-VL training data

**Audio Data**:
- **300 billion tokens** of audio
- Speech, music, sound events
- Multilingual audio coverage

**Audio-Visual Data**:
- **100 billion tokens** of video with accompanying audio
- Temporal alignment for audio-video synchronization
- Foundation for TMRoPE training

**Multimodal Data (Stage 2)**:
- **1.2 trillion tokens** of multimodal data
- Complex cross-modal relationships
- Instruction-following across modalities

**Knowledge Cutoff**: Approximately early 2025

### Training Stages

#### Stage 1: Encoder Pretraining

**Objective**: Establish core cross-modal correlations without disrupting pre-trained language understanding

**Frozen Parameters**:
- LLM parameters (Qwen2.5-3B or Qwen2.5-7B)

**Trainable Parameters**:
- Vision encoder adapters
- Audio encoder adapters
- Cross-modal projection layers

**Training Data**:
- Image-text pairs
- Audio-text pairs

**Task**: Generate next text token given multimodal context

**Duration**: Not disclosed

#### Stage 2: Full Multimodal Pretraining

**Objective**: Learn complex cross-modal relationships across all modalities

**Starting Foundations**:
- **LLM**: Qwen2.5-3B or Qwen2.5-7B (pre-trained)
- **Vision**: Qwen2.5-VL encoder (pre-trained)
- **Audio**: Whisper-large-v3 (pre-trained)

**All Parameters Unfrozen**: End-to-end training of entire system

**Training Data**: 1.2 trillion tokens of multimodal data

**Tasks**:
- Generate next text token
- Generate next audio token (for Talker training)
- Cross-modal understanding and reasoning

**Duration**: Not disclosed

#### Stage 3: Long Sequence Training

**Objective**: Enable handling of extended contexts and hour-long videos

**Sequence Length**: Extended to **32,768 tokens**

**Capabilities Enabled**:
- Hour-long videos with accompanying audio
- Extended conversations with visual context
- Long-form multimodal content understanding

**Training Data**: Long-context multimodal sequences

**Duration**: Not disclosed

### Post-training

#### Supervised Fine-Tuning (SFT)

**Format**: ChatML format for structured multimodal instruction data

**Objectives**:
- Instruction-following across all modalities
- Multi-turn conversation capabilities
- Task-specific fine-tuning (VQA, ASR, translation, etc.)

**Data**: High-quality instruction-response pairs

#### Talker-Specific Training

**1. In-Context Learning (ICL)**:
- Learn speech continuation patterns
- Maintain naturalness in generated speech
- Adapt to different speaking styles

**2. Direct Preference Optimization (DPO)**:
- Mitigate hallucinations in speech generation
- Use Word Error Rate (WER) metrics as preference signal
- Improve speech quality and accuracy

**3. Multi-Speaker Fine-tuning**:
- Adopt specific voice characteristics
- Maintain naturalness while following speaker profiles
- Support multiple speaker voices (Chelsie, Ethan)

### Training Infrastructure

**End-to-End Training**: Unified training for both Thinker and Talker models

**Platform**: Alibaba Cloud PAI-Lingjun service (assumed)

**Parallelism**: Likely 3D parallelism (data + tensor + pipeline)

**Software Stack**: PyTorch, Flash-attention (inferred from deployment requirements)

**Training Cost**: Not publicly disclosed

---

## Performance Benchmarks

### Multimodal Integration (State-of-the-Art)

**OmniBench**: Comprehensive multimodal integration benchmark

| Model | Score | Notes |
|-------|-------|-------|
| **Qwen2.5-Omni-7B** | **56.13%** | **State-of-the-art** |
| Gemini-1.5-Pro | 42.91% | -13.22 points |
| MIO-Instruct | 33.80% | -22.33 points |

**Achievement**: Qwen2.5-Omni sets new state-of-the-art on multimodal integration, significantly outperforming Gemini-1.5-Pro.

### Speech Understanding & Recognition

#### Common Voice 15 - Word Error Rate (WER)

| Language | Qwen2.5-Omni-7B | Lower is better |
|----------|-----------------|-----------------|
| English | 7.6 | Strong performance |
| Chinese | 5.2 | Excellent performance |

#### CoVoST2 (Speech-to-Text Translation) - BLEU scores

| Translation Direction | Qwen2.5-Omni-7B | Higher is better |
|----------------------|-----------------|------------------|
| English â†’ German | 30.2 | Strong translation |
| Chinese â†’ English | 29.4 | Strong translation |

#### MMAU (Multimodal Audio Understanding)

| Metric | Qwen2.5-Omni-7B | Notes |
|--------|-----------------|-------|
| Average Score | **65.60%** | **Ranked #1 among open-source models** |
| Sound Reasoning | Excellent | Sound event understanding |
| Music Understanding | Excellent | Music analysis and reasoning |
| Speech Reasoning | Excellent | Speech content understanding |

**Achievement**: First place among open-source models on MMAU leaderboard.

#### MMSU (Multimodal Spoken Language Understanding)

| Metric | Qwen2.5-Omni-7B | Notes |
|--------|-----------------|-------|
| Overall Performance | **Ranked #1** | **First among open-source models** |

### Vision & Image Understanding

#### MMMU (Multimodal Massive Multitask Understanding) - Validation Set

| Model | Score | Notes |
|-------|-------|-------|
| GPT-4o-mini | 60.0 | Proprietary baseline |
| **Qwen2.5-Omni-7B** | **59.2** | Very competitive (-0.8) |

#### MMStar

| Model | Score | Notes |
|-------|-------|-------|
| **Qwen2.5-Omni-7B** | **64.0** | **Significantly outperforms GPT-4o-mini** |
| GPT-4o-mini | 54.8 | -9.2 points |

#### MVBench (Video Understanding)

| Model | Score | Notes |
|-------|-------|-------|
| **Qwen2.5-Omni-7B** | **70.3** | Strong video comprehension |

**Analysis**: Qwen2.5-Omni demonstrates strong vision-language capabilities comparable to dedicated vision-language models (Qwen2.5-VL-7B) while adding audio understanding and speech generation.

### Text Generation & Reasoning

#### GSM8K (Mathematical Reasoning)

| Model | Score | Notes |
|-------|-------|-------|
| Qwen2.5-7B (base) | 91.6% | Pure language model baseline |
| **Qwen2.5-Omni-7B** | **88.7%** | Strong despite multimodal training |
| **Qwen2.5-Omni-3B** | **82.6%** | Competitive for smaller model |

**Analysis**: Minimal degradation from pure language model performance despite extensive multimodal training.

#### MMLU (Speech Instruction Following)

| Model | Performance | Notes |
|-------|-------------|-------|
| **Qwen2.5-Omni-7B** | Comparable to text input | Effective end-to-end speech instruction following |

**Analysis**: Speech understanding matches text understanding capabilities, demonstrating effective multimodal integration.

### Speech Generation Quality

#### SEED-TTS-eval - Word Error Rate (WER)

| Language/Condition | WER | Lower is better |
|-------------------|-----|-----------------|
| Chinese | **1.42** | Excellent |
| English | **2.32** | Excellent |
| Hard samples | 6.54 | Good |

#### Speaker Similarity (Cosine Similarity)

| Language/Condition | Score | Higher is better |
|-------------------|-------|------------------|
| Chinese | **0.754** | Strong similarity |
| English | 0.641 | Good similarity |
| Hard samples | **0.752** | Strong similarity |

#### Subjective Evaluation

- **Robustness**: Superior to both streaming and non-streaming alternatives
- **Naturalness**: High-quality, natural-sounding speech
- **Latency**: Low-latency streaming generation
- **Consistency**: Maintains quality across diverse conditions

### Model Size Comparison: 7B vs. 3B

#### Resource Efficiency (Qwen2.5-Omni-3B)

| Metric | 3B Model | 7B Model | Reduction |
|--------|----------|----------|-----------|
| VRAM Usage | 18-28 GB | 31-60 GB | **~50%** |
| Performance | ~90% of 7B | 100% | -10% |
| Target Hardware | RTX 4090 (24GB) | A100/H100 | Consumer-grade GPUs |

**Qwen2.5-Omni-3B Advantages**:
- Runs on consumer-grade 24GB GPUs (NVIDIA RTX 4090)
- 50% lower VRAM usage
- Retains over 90% of 7B model's multimodal performance
- Enables broader accessibility and deployment

#### Performance Comparison (GSM8K Example)

| Model | GSM8K Score | Î” from 7B |
|-------|-------------|-----------|
| Qwen2.5-Omni-7B | 88.7% | â€” |
| Qwen2.5-Omni-3B | 82.6% | -6.1 points |

**Scaling Behavior**: Predictable performance scaling with model size, strong retention of capabilities in smaller variant.

### Comparison with Related Models

#### vs. Qwen2-Audio

| Capability | Qwen2-Audio | Qwen2.5-Omni | Improvement |
|------------|-------------|--------------|-------------|
| Audio Understanding | Yes | Yes (superior) | Better benchmarks |
| Vision Understanding | No | Yes | New capability |
| Speech Generation | No | Yes (streaming) | Revolutionary |
| Real-time | Limited | Full streaming | Much faster |
| Architecture | Single-track | Thinker-Talker | Concurrent output |

#### vs. Qwen2.5-VL-7B

| Capability | Qwen2.5-VL-7B | Qwen2.5-Omni-7B | Trade-off |
|------------|---------------|-----------------|-----------|
| Vision-Language | State-of-the-art | Comparable | Minimal loss |
| Audio Understanding | No | Yes (MMAU #1) | New capability |
| Speech Generation | No | Yes (natural) | New capability |
| Multimodal Integration | Limited | SOTA (OmniBench) | Superior |

#### vs. Proprietary Models (GPT-4o-mini, Gemini-1.5-Pro)

| Benchmark | Qwen2.5-Omni-7B | Best Proprietary | Comparison |
|-----------|-----------------|------------------|------------|
| OmniBench | **56.13** | 42.91 (Gemini-1.5-Pro) | **+13.22** âœ“âœ“ |
| MMStar | **64.0** | 54.8 (GPT-4o-mini) | **+9.2** âœ“ |
| MMMU | 59.2 | **60.0** (GPT-4o-mini) | -0.8 |

**Key Insight**: Qwen2.5-Omni achieves competitive or superior performance compared to proprietary models while being fully open source under Apache 2.0 license.

---

## Differences from Qwen2-Audio

### Architectural Evolution

| Feature | Qwen2-Audio | Qwen2.5-Omni | Impact |
|---------|-------------|--------------|--------|
| **Architecture** | Single-track | Thinker-Talker (dual-component) | Concurrent text + speech |
| **Input Modalities** | Text + Audio | Text + Audio + Images + Video | True omni-modal |
| **Output Modalities** | Text only | Text + Natural Speech (simultaneous) | Revolutionary |
| **Position Encoding** | Standard RoPE | TMRoPE (time-aligned) | Audio-video sync |
| **Real-time** | Limited | Full streaming support | Low latency |
| **Video Support** | No | Yes (with audio sync) | New capability |
| **Speech Generation** | No | Yes (streaming, natural) | New capability |
| **Training Scale** | Smaller | 18T text + 1.2T multimodal | Massive scale-up |

### Performance Improvements

**Audio Understanding**:
- Qwen2-Audio: Strong audio capabilities
- Qwen2.5-Omni: **Outperforms** Qwen2-Audio + adds multimodal context
- **MMAU**: #1 among open-source models
- **MMSU**: #1 among open-source models

**Multimodal Integration**:
- Qwen2-Audio: Audio + text only
- Qwen2.5-Omni: **State-of-the-art** on OmniBench (56.13%)
- Significant improvement in cross-modal understanding

**New Capabilities**:
- Vision understanding (comparable to Qwen2.5-VL-7B)
- Natural speech generation (streaming, multi-speaker)
- Real-time conversation with low latency
- Hour-long video with audio comprehension

---

## Capabilities and Features

### Core Capabilities

#### 1. Omni-modal Understanding

**Supported Input Modalities**:
- **Text**: Natural language in multiple languages
- **Images**: Photos, diagrams, charts, documents
- **Audio**: Speech, music, sound events, environmental sounds
- **Video**: Video clips, movies, hour-long content with audio

**Cross-modal Reasoning**:
- Simultaneous understanding of multiple modalities
- Audio-visual synchronization via TMRoPE
- Complex multimodal question answering
- State-of-the-art on OmniBench multimodal integration

#### 2. Speech Understanding

**Capabilities**:
- Speech recognition (ASR)
- Speech translation (CoVoST2: 30.2 BLEU enâ†’de)
- Audio classification
- Sound event detection
- Music understanding
- Emotion recognition from speech
- Speaker characteristics understanding

**Performance**:
- Common Voice 15 WER: 7.6 (English), 5.2 (Chinese)
- MMAU: 65.60% (ranked #1 among open-source)
- MMSU: Ranked #1 among open-source

#### 3. Speech Generation

**Natural Speech Synthesis**:
- Streaming generation with low latency
- High speaker similarity (0.641-0.754 cosine similarity)
- Low word error rate (1.42-2.32 WER)
- Superior robustness and naturalness

**Multi-Speaker Support**:
- **Chelsie** (Female, default): Honeyed, velvety voice with gentle warmth
- **Ethan** (Male): Bright, upbeat voice with infectious energy
- Fine-tuned for naturalness while maintaining speaker characteristics

**Concurrent Generation**:
- Simultaneous text and speech output
- No interference between modalities
- Streaming capability for real-time conversations

#### 4. Vision-Language Understanding

**Image Understanding**:
- General image comprehension (MMStar: 64.0)
- Image reasoning (MMMU: 59.2)
- Visual question answering
- Document understanding
- Chart and diagram interpretation

**Video Understanding**:
- Video comprehension (MVBench: 70.3)
- Hour-long video processing (Stage 3 training)
- Audio-visual synchronization
- Temporal reasoning

#### 5. Real-time Conversation

**Streaming Architecture**:
- Block-wise processing (2-second audio segments)
- Sliding-window DiT for low-latency speech
- Start generating while receiving input
- Fully real-time interactions

**Latency Optimization**:
- Minimal multimodal processing delay
- Fast first text-to-voice token generation
- Quick first speech segment conversion
- Low architectural inherent latency

**User Experience**:
- Natural conversational flow
- Immediate responses
- Chunked input support
- Continuous audio output

#### 6. Multilingual Support

**Supported Languages**:
- **English**: Fluent (WER: 7.6, BLEU: 30.2 enâ†’de)
- **Chinese**: Fluent (WER: 5.2, BLEU: 29.4 zhâ†’en)
- **Multiple others**: Present in training data

**Cross-lingual Capabilities**:
- Speech translation
- Multilingual text understanding
- Cross-lingual audio understanding

#### 7. Mathematical Reasoning

**Capabilities**:
- Mathematical problem solving (GSM8K: 88.7% for 7B, 82.6% for 3B)
- Numerical reasoning
- Logical reasoning

**Analysis**: Minimal degradation from pure language model performance (Qwen2.5-7B: 91.6%) despite extensive multimodal training.

### Known Limitations

Based on architecture and benchmarks:

**Hard Samples**:
- Increased WER on challenging audio samples (6.54 vs. 1.42-2.32)
- Maintained high speaker similarity even on hard samples (0.752)

**Model Size Trade-offs**:
- 3B model retains 90% of 7B performance but with some capability reduction
- Smaller models may struggle with more complex multimodal reasoning

**Context Length**:
- 32K token limit (though extendable with YaRN)
- Very long videos may require careful chunking

---

## Evolution Timeline

### Qwen2-Audio (2024)

**Characteristics**:
- First Qwen audio model
- Audio + text understanding
- Text-only output
- Single-track architecture

**Capabilities**:
- Speech recognition
- Audio understanding
- Limited multimodal integration

### Qwen2.5-Omni (March 2025)

**Innovations**:
- Thinker-Talker architecture (dual-component)
- TMRoPE (time-aligned multimodal RoPE)
- True omni-modal: Text + audio + images + video
- Natural speech generation (streaming, multi-speaker)
- Real-time conversation with low latency

**Key Achievements**:
- State-of-the-art on OmniBench (56.13%)
- #1 among open-source on MMAU (65.60%)
- #1 among open-source on MMSU
- Comparable to GPT-4o-mini and Gemini-1.5-Pro

### Qwen3-Omni (Future/Successor)

**Technical Report**: [arXiv:2509.17765](https://arxiv.org/abs/2509.17765)

**Expected Enhancements**:
- Further architectural refinements
- Improved multimodal integration
- Extended context length (potentially 262K like Qwen3-VL)
- Enhanced efficiency

---

## Technical Resources and Integration

### Official Resources

#### Papers
- **Primary**: [Qwen2.5-Omni Technical Report (arXiv:2503.20215)](https://arxiv.org/abs/2503.20215)
- **PDF**: [arxiv.org/pdf/2503.20215](https://arxiv.org/pdf/2503.20215)
- **Related**: [Qwen2.5 Technical Report (arXiv:2412.15115)](https://arxiv.org/abs/2412.15115)
- **Related**: [Qwen2-Audio Technical Report (arXiv:2407.10759)](https://arxiv.org/abs/2407.10759)
- **Successor**: [Qwen3-Omni Technical Report (arXiv:2509.17765)](https://arxiv.org/abs/2509.17765)

#### Official Blog Posts
- [Official Announcement: Qwen2.5-Omni](https://qwenlm.github.io/blog/qwen2.5-omni/)

#### GitHub Repositories
- **Main Repository**: [QwenLM/Qwen2.5-Omni](https://github.com/QwenLM/Qwen2.5-Omni)
- Contains: Code examples, web demo, cookbooks, documentation

#### Model Cards (Hugging Face)
- [Qwen2.5-Omni-7B](https://huggingface.co/Qwen/Qwen2.5-Omni-7B)
- [Qwen2.5-Omni-3B](https://huggingface.co/Qwen/Qwen2.5-Omni-3B)
- [Hugging Face Documentation](https://huggingface.co/docs/transformers/model_doc/qwen2_5_omni)
- [Hugging Face Papers](https://huggingface.co/papers/2503.20215)

### Quantized Versions

Available for resource-constrained deployment:

| Quantization | Model | Link |
|--------------|-------|------|
| **AWQ** | Qwen2.5-Omni-7B | [huggingface.co/Qwen/Qwen2.5-Omni-7B-AWQ](https://huggingface.co/Qwen/Qwen2.5-Omni-7B-AWQ) |
| **GPTQ-Int4** | Qwen2.5-Omni-7B | [huggingface.co/Qwen/Qwen2.5-Omni-7B-GPTQ-Int4](https://huggingface.co/Qwen/Qwen2.5-Omni-7B-GPTQ-Int4) |
| **GGUF** | Qwen2.5-Omni-7B | [huggingface.co/ggml-org/Qwen2.5-Omni-7B-GGUF](https://huggingface.co/ggml-org/Qwen2.5-Omni-7B-GGUF) |

### Framework Integration

#### Installation

**Requirements**:
```bash
# Install transformers with Qwen2.5-Omni support
pip install git+https://github.com/huggingface/transformers@v4.51.3-Qwen2.5-Omni-preview
# OR
pip install transformers==4.52.3

# Additional dependencies
pip install accelerate
pip install qwen-omni-utils

# Recommended for performance
pip install flash-attn  # Requires torch.float16 or torch.bfloat16

# Audio/video processing
# Install ffmpeg (OS-specific)
```

#### Basic Inference Example

```python
from transformers import Qwen25OmniForConditionalGeneration, AutoProcessor
import torch

# Load model and processor
model = Qwen25OmniForConditionalGeneration.from_pretrained(
    "Qwen/Qwen2.5-Omni-7B",
    torch_dtype=torch.bfloat16,
    device_map="auto",
    attn_implementation="flash_attention_2"  # Recommended
)
processor = AutoProcessor.from_pretrained("Qwen/Qwen2.5-Omni-7B")

# Prepare multimodal input
messages = [
    {
        "role": "user",
        "content": [
            {"type": "image", "image": "path/to/image.jpg"},
            {"type": "audio", "audio": "path/to/audio.wav"},
            {"type": "text", "text": "Describe what you see and hear."},
        ],
    }
]

# Process inputs
text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
inputs = processor(
    text=[text],
    images=[...],  # Processed images
    audios=[...],  # Processed audio
    return_tensors="pt",
    padding=True,
)
inputs = inputs.to("cuda")

# Generate response (text)
generated_ids = model.generate(**inputs, max_new_tokens=256)
generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]

# Generate speech (using Talker)
speech_tokens = model.generate_speech(**inputs, voice="Chelsie")
# Convert speech tokens to audio waveform
speech_audio = model.decode_speech_tokens(speech_tokens)
```

#### Web Demo Launch

```bash
# Clone repository
git clone https://github.com/QwenLM/Qwen2.5-Omni.git
cd Qwen2.5-Omni

# Launch web demo (7B model)
python web_demo.py --flash-attn2

# Launch web demo (3B model)
python web_demo.py --flash-attn2 -c Qwen/Qwen2.5-Omni-3B

# Specify device
python web_demo.py --flash-attn2 --device cuda:0
```

### Interactive Demo

**Official Demo**: [chat.qwen.ai](https://chat.qwen.ai)
- Launched March 26, 2025
- Real-time multimodal interaction
- Speech input and output
- Image and video understanding

### API Access

**Platform**: DashScope (Alibaba Cloud) - assumed based on Qwen series pattern

**License**: Apache 2.0 (fully open source, commercial use allowed)

---

## Summary of Technical Contributions

### 1. Thinker-Talker Architecture

**Innovation**: First major multimodal model with explicit separation of thinking (comprehension/reasoning) and talking (speech generation).

**Impact**:
- Enables concurrent text and speech generation without interference
- Streaming capability with low latency
- Natural conversational flow
- Foundation for future multimodal architectures

### 2. TMRoPE (Time-aligned Multimodal RoPE)

**Innovation**: Novel position embedding that synchronizes audio and video timestamps explicitly.

**Impact**:
- Better audio-visual synchronization understanding
- Enables processing of hour-long videos with audio
- Improved temporal reasoning across modalities
- Natural handling of multimodal temporal alignment

### 3. Block-wise Streaming Architecture

**Innovation**: Block-wise processing for audio (2-second blocks) and video with temporal block-wise attention.

**Impact**:
- True real-time conversation capability
- Start generating while receiving input
- Efficient memory usage for long sequences
- Scalable to arbitrarily long audio/video streams

### 4. Sliding-Window DiT for Speech

**Innovation**: Restricted receptive field (two lookback blocks) for low-latency streaming speech generation.

**Impact**:
- Minimal initial package delay
- Superior robustness and naturalness vs. alternatives
- Real-time speech synthesis
- Natural integration with Thinker-Talker architecture

### 5. State-of-the-Art Multimodal Integration

**Innovation**: Achieves SOTA on OmniBench (56.13%), significantly outperforming Gemini-1.5-Pro (42.91%).

**Impact**:
- Demonstrates effective cross-modal understanding
- Sets new benchmark for multimodal integration
- Proves viability of open-source omni-modal models
- Competitive with proprietary alternatives

### 6. Open Source Leadership

**Innovation**: Apache 2.0 licensed omni-modal model with natural speech generation.

**Impact**:
- Democratizes access to advanced multimodal AI
- Enables research and commercial deployment
- Reduces barriers to multimodal AI adoption
- Accelerates innovation in multimodal systems

### 7. Efficient Scaling

**Innovation**: 3B variant retains 90% of 7B performance with 50% lower VRAM usage.

**Impact**:
- Enables deployment on consumer-grade GPUs (RTX 4090)
- Broader accessibility for researchers and developers
- Cost-effective multimodal AI deployment
- Predictable scaling behavior

---

## Conclusion

Qwen2.5-Omni represents a revolutionary advancement in multimodal AI through its groundbreaking **Thinker-Talker architecture**, which enables simultaneous generation of text and natural speech while maintaining true omni-modal understanding across text, images, audio, and video. Released in March 2025, this model achieves state-of-the-art performance on multimodal integration benchmarks (OmniBench: 56.13%) while supporting real-time streaming conversations with remarkably low latency.

Key achievements include:

- **State-of-the-art multimodal integration** (OmniBench: 56.13%, +13.22 vs. Gemini-1.5-Pro)
- **#1 among open-source models** on MMAU (65.60%) and MMSU audio understanding
- **Natural speech generation** with streaming capability (WER: 1.42-2.32)
- **Real-time conversation** with block-wise processing and sliding-window DiT
- **Competitive vision-language performance** (MMStar: 64.0 vs. GPT-4o-mini: 54.8)
- **Efficient 3B variant** (90% of 7B performance, 50% lower VRAM, runs on RTX 4090)

The model's **Apache 2.0 license** democratizes access to advanced omni-modal AI, enabling researchers and developers worldwide to build innovative multimodal applications without proprietary API dependencies.

Qwen2.5-Omni establishes a new paradigm for multimodal models through its dual-component design and provides a strong foundation for future developments in the Qwen multimodal series (Qwen3-Omni).

---

## References and Citations

### Primary Sources

1. **Qwen2.5-Omni Technical Report**
   Qwen Team. (2025). Qwen2.5-Omni Technical Report. *arXiv preprint arXiv:2503.20215*.
   [https://arxiv.org/abs/2503.20215](https://arxiv.org/abs/2503.20215)

2. **Qwen2.5 Technical Report**
   Yang, A., Yang, B., Hui, B., et al. (2024). Qwen2.5 Technical Report. *arXiv preprint arXiv:2412.15115*.
   [https://arxiv.org/abs/2412.15115](https://arxiv.org/abs/2412.15115)

3. **Qwen2-Audio Technical Report**
   Chu, Y., Xu, J., Zhou, X., et al. (2024). Qwen2-Audio Technical Report. *arXiv preprint arXiv:2407.10759*.
   [https://arxiv.org/abs/2407.10759](https://arxiv.org/abs/2407.10759)

4. **Qwen3-Omni Technical Report** (Successor)
   Qwen Team. (2025). Qwen3-Omni Technical Report. *arXiv preprint arXiv:2509.17765*.
   [https://arxiv.org/abs/2509.17765](https://arxiv.org/abs/2509.17765)

### Official Resources

5. **Qwen2.5-Omni Official Blog**
   [https://qwenlm.github.io/blog/qwen2.5-omni/](https://qwenlm.github.io/blog/qwen2.5-omni/)

6. **Qwen2.5-Omni GitHub Repository**
   [https://github.com/QwenLM/Qwen2.5-Omni](https://github.com/QwenLM/Qwen2.5-Omni)

### Model Cards and Documentation

7. **Hugging Face Model Cards**
   - [Qwen2.5-Omni-7B](https://huggingface.co/Qwen/Qwen2.5-Omni-7B)
   - [Qwen2.5-Omni-3B](https://huggingface.co/Qwen/Qwen2.5-Omni-3B)

8. **Hugging Face Documentation**
   [https://huggingface.co/docs/transformers/model_doc/qwen2_5_omni](https://huggingface.co/docs/transformers/model_doc/qwen2_5_omni)

9. **Hugging Face Papers**
   [https://huggingface.co/papers/2503.20215](https://huggingface.co/papers/2503.20215)

### Additional Analysis

10. **LearnOpenCV: Qwen2.5-Omni Overview**
    [https://learnopencv.com/qwen2-5-omni/](https://learnopencv.com/qwen2-5-omni/)

11. **Apidog: Qwen2.5-Omni-7B Analysis**
    [https://apidog.com/blog/qwen2-5-omni-7b/](https://apidog.com/blog/qwen2-5-omni-7b/)

12. **VentureBeat: Qwen2.5-Omni-3B Launch**
    [https://venturebeat.com/ai/qwen-swings-for-a-double-with-2-5-omni-3b-model-that-runs-on-consumer-pcs-laptops](https://venturebeat.com/ai/qwen-swings-for-a-double-with-2-5-omni-3b-model-that-runs-on-consumer-pcs-laptops)

13. **MarkTechPost: Qwen2.5-Omni-3B Analysis**
    [https://www.marktechpost.com/2025/04/30/multimodal-ai-on-developer-gpus-alibaba-releases-qwen2-5-omni-3b-with-50-lower-vram-usage-and-nearly-7b-model-performance/](https://www.marktechpost.com/2025/04/30/multimodal-ai-on-developer-gpus-alibaba-releases-qwen2-5-omni-3b-with-50-lower-vram-usage-and-nearly-7b-model-performance/)

---

## Appendix: Thinker-Talker Architecture Details

### Detailed Information Flow

**Step 1: Multimodal Input Processing**
```
User Input: "Look at this image and tell me what you see" + image.jpg + audio.wav

Vision Encoder (ViT ~675M):
  image.jpg â†’ Visual Tokens (Vâ‚, Vâ‚‚, ..., Vâ‚™)

Audio Encoder (Whisper-large-v3):
  audio.wav â†’ 16kHz â†’ Mel-spectrogram â†’ Audio Tokens (Aâ‚, Aâ‚‚, ..., Aâ‚˜)

Text Tokenizer:
  "Look at this image..." â†’ Text Tokens (Tâ‚, Tâ‚‚, ..., Tâ‚–)
```

**Step 2: TMRoPE Fusion**
```
Align timestamps:
  Video frame at t=1.5s: V_t=1.5
  Audio segment at t=1.5s: A_t=1.5
  â†’ TMRoPE assigns synchronized positions

Interleaved Sequence:
  [Tâ‚, Tâ‚‚, ..., Tâ‚–, Vâ‚(t=0), Vâ‚‚(t=0.04), ..., Aâ‚(t=0), Aâ‚‚(t=0.04), ...]
```

**Step 3: Thinker Processing**
```
Qwen2.5 LLM (3B or 7B):
  Input: Multimodal token sequence with TMRoPE
  Processing: Shared attention across all modalities

  Outputs:
    1. Text Tokens: (Output_Tâ‚, Output_Tâ‚‚, ..., Output_Tâ‚—)
    2. Hidden Representations: (Hâ‚, Hâ‚‚, ..., Hâ‚—) [for Talker]

  Streaming: Both outputs streamed as generated
```

**Step 4: Talker Processing**
```
Talker Dual-track AR:
  Input 1: Hidden Representations (Hâ‚, Hâ‚‚, ...) [streaming from Thinker]
  Input 2: Text Tokens (Output_Tâ‚, Output_Tâ‚‚, ...) [streaming from Thinker]

  Sliding-Window DiT:
    - Receptive field: Two lookback blocks
    - Generates speech tokens: (Speech_Sâ‚, Speech_Sâ‚‚, ...)
    - Streaming generation: Starts as soon as Hâ‚ arrives

  Speech Decoding:
    Speech tokens â†’ Vocoder â†’ Audio waveform
    Output: Natural speech (Chelsie or Ethan voice)
```

**Step 5: Concurrent Output**
```
Text Output Stream: "I can see a beautiful landscape with..."
  [Displayed to user in real-time]

Speech Output Stream: ğŸ”Š [Audio waveform]
  [Played to user in real-time]

Both streams:
  - Generated concurrently
  - No interference
  - Synchronized content
  - Low latency
```

### Key Architectural Advantages

**1. Decoupling of Text and Speech**:
- Thinker focuses on understanding and text generation
- Talker focuses on natural speech synthesis
- No mutual interference between modalities
- Independent optimization of each component

**2. Streaming Efficiency**:
- Talker doesn't wait for complete Thinker output
- Starts generating speech from first hidden representations
- Reduces overall latency significantly
- Enables real-time conversation

**3. Quality Preservation**:
- Text generation not constrained by speech requirements
- Speech generation informed by rich hidden representations
- Maintains high quality in both modalities
- Superior naturalness compared to single-track approaches

---

**Document Version**: 1.0
**Last Updated**: 2025-01-24
**Model Versions Covered**: Qwen2.5-Omni-3B, Qwen2.5-Omni-7B
**License**: Apache 2.0 (fully open source)
