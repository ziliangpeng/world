# Synthetic Data for LLM Training

Synthetic data—training data generated by AI models rather than humans—has emerged as one of the most impactful techniques in modern LLM development. From Microsoft's Phi models proving that "textbook-quality" synthetic data can outperform massive web crawls, to Self-Instruct enabling instruction tuning for $500, synthetic data has democratized and accelerated LLM development.

---

## Why Synthetic Data?

### The Data Bottleneck

| Data Type | Cost | Quality | Scale |
|-----------|------|---------|-------|
| Human-written | $$$$ | High | Limited |
| Web crawl | $ | Mixed | Massive |
| **Synthetic** | $$ | Controllable | Unlimited |

**Key insight**: High-quality data is more valuable than high-quantity data. Synthetic data can provide quality at scale.

### Use Cases

1. **Instruction tuning**: Generate diverse instruction-following examples
2. **Code generation**: Create programming problems with solutions
3. **Reasoning**: Generate step-by-step problem solutions
4. **Preference data**: Create comparison pairs for RLHF/DPO
5. **Domain specialization**: Generate domain-specific content
6. **Data augmentation**: Expand limited human-labeled datasets

---

## Historical Evolution

### Phase 1: Data Augmentation Era (2019-2021)

**Traditional augmentation**: Paraphrasing, back-translation, synonym replacement.

Limited to surface-level variations, didn't add new knowledge.

### Phase 2: Self-Instruct Breakthrough (2022-2023)

**[Self-Instruct](https://arxiv.org/abs/2212.10560)** (December 2022)

Generated instruction-tuning data using GPT-3 itself:

```
Self-Instruct Pipeline:
1. Start with 175 human-written seed tasks
2. Use LLM to generate new task instructions
3. LLM classifies tasks (generation vs classification)
4. LLM generates input-output instances
5. Filter for quality and diversity
6. Add to task pool, repeat
```

**Results**:
- 52K instructions generated from 175 seeds
- GPT-3 + Self-Instruct outperformed GPT-3 on user-facing tasks
- Cost: ~$600 (GPT-3 API calls)

**Impact**: Enabled Stanford Alpaca ($500 to fine-tune LLaMA to match text-davinci-003).

### Phase 3: Evolved Instructions (2023)

**[Evol-Instruct](https://arxiv.org/abs/2304.12244)** (WizardLM, April 2023)

Evolved simple instructions into complex ones:

```python
evolution_methods = {
    "add_constraints": "Add requirement that the solution must...",
    "deepen": "Make the task require more reasoning steps",
    "concretizing": "Replace abstract concepts with specific examples",
    "increase_reasoning": "Ask for explanation of why each step works",
    "complicate_input": "Add edge cases to the input"
}

def evolve_instruction(instruction, method):
    prompt = f"""
Evolve this instruction to make it more challenging using {method}:
Original: {instruction}
Evolved:"""
    return llm.generate(prompt)
```

**Results**: WizardLM-7B outperformed many larger models on complex tasks.

### Phase 4: Textbook Quality (2023)

**[Phi-1](https://arxiv.org/abs/2306.11644)** (Microsoft, June 2023)

Demonstrated that synthetic "textbook-quality" data beats web data:

**Philosophy**: Generate educational content that explains concepts clearly, like a textbook.

```
Phi Data Sources:
- Filtered code (The Stack): 6B tokens
- Synthetic textbooks: 1B tokens (GPT-3.5 generated)
- Synthetic exercises: 180M tokens

Total: ~7B tokens (tiny by LLM standards)
```

**Results**: Phi-1 (1.3B params) matched larger models on code benchmarks.

**[Phi-2](https://www.microsoft.com/en-us/research/blog/phi-2-the-surprising-power-of-small-language-models/)** (December 2023)

Extended to general language:
- 250B tokens (synthetic + filtered web)
- 2.7B parameters
- Matched 25× larger models on reasoning benchmarks

**[Phi-3](https://arxiv.org/abs/2404.14219)** (April 2024)

Further scaling:
- 4.8T tokens
- Heavily synthetic with curriculum learning
- 3.8B params competitive with GPT-3.5

### Phase 5: Distillation and Orca (2023-2024)

**[Orca](https://arxiv.org/abs/2306.02707)** (June 2023)

"Explanation tuning"—distill reasoning from GPT-4:

```python
def orca_style_generation(prompt, teacher_model):
    """Generate detailed reasoning traces."""
    enhanced_prompt = f"""
{prompt}

Think step by step. Explain your reasoning process in detail.
Show how you arrive at each conclusion.
"""
    return teacher_model.generate(enhanced_prompt)
```

**Results**: Orca-13B approached GPT-4 on reasoning tasks.

### Phase 6: Self-Improvement Loops (2024)

**[Self-Rewarding Language Models](https://arxiv.org/abs/2401.10020)** (January 2024)

Model generates training data, evaluates it, and improves:

```
Iteration 1: M₀ generates responses, M₀ judges quality
            → Train M₁ on best responses
Iteration 2: M₁ generates responses, M₁ judges quality
            → Train M₂ on best responses
...
```

**Key insight**: Model can improve its own training signal over time.

---

## Core Techniques

### 1. Instruction Generation

**Seed-based generation**:
```python
def generate_instructions(seeds, llm, n_new=1000):
    """Generate new instructions from seeds."""
    instructions = seeds.copy()

    while len(instructions) < n_new:
        # Sample existing instructions for in-context examples
        examples = random.sample(instructions, k=5)

        prompt = f"""
Here are some example tasks:
{format_examples(examples)}

Generate a new, different task instruction:
"""
        new_instruction = llm.generate(prompt)

        # Filter for quality and diversity
        if is_valid(new_instruction) and is_diverse(new_instruction, instructions):
            instructions.append(new_instruction)

    return instructions
```

**Topic-based generation**:
```python
def generate_by_topic(topics, llm, n_per_topic=100):
    """Generate instructions for specific topics."""
    instructions = []

    for topic in topics:
        prompt = f"""
Generate {n_per_topic} diverse task instructions about {topic}.
Include:
- Simple factual questions
- Complex reasoning problems
- Creative tasks
- Analysis tasks
"""
        topic_instructions = llm.generate(prompt)
        instructions.extend(topic_instructions)

    return instructions
```

### 2. Response Generation

**Direct generation**:
```python
def generate_response(instruction, teacher_model):
    """Simple response generation."""
    return teacher_model.generate(instruction)
```

**Chain-of-thought generation**:
```python
def generate_cot_response(instruction, teacher_model):
    """Generate with step-by-step reasoning."""
    prompt = f"""
{instruction}

Let's think step by step:
"""
    return teacher_model.generate(prompt)
```

**Multi-sample with selection**:
```python
def generate_best_response(instruction, teacher_model, judge_model, n_samples=5):
    """Generate multiple responses, select best."""
    responses = [teacher_model.generate(instruction, temperature=0.7)
                 for _ in range(n_samples)]

    # Judge selects best
    best_idx = judge_model.select_best(instruction, responses)
    return responses[best_idx]
```

### 3. Quality Filtering

**Rule-based filtering**:
```python
def quality_filter(instruction, response):
    """Filter low-quality pairs."""
    # Length checks
    if len(response) < 50:
        return False
    if len(instruction) < 10:
        return False

    # Content checks
    if "I cannot" in response or "I'm not able" in response:
        return False  # Avoid refusals in training
    if response.count("...") > 3:
        return False  # Avoid incomplete responses

    # Duplication check
    if is_near_duplicate(instruction, seen_instructions):
        return False

    return True
```

**Model-based filtering**:
```python
def model_quality_score(instruction, response, scorer_model):
    """Use a model to score quality."""
    prompt = f"""
Rate the quality of this response on a scale of 1-10.

Instruction: {instruction}
Response: {response}

Consider: accuracy, helpfulness, completeness, clarity.
Score (1-10):"""

    score = scorer_model.generate(prompt)
    return int(score)

def filter_by_quality(pairs, scorer_model, threshold=7):
    """Keep only high-quality pairs."""
    return [p for p in pairs
            if model_quality_score(p["instruction"], p["response"], scorer_model) >= threshold]
```

### 4. Diversity Enforcement

**Embedding-based diversity**:
```python
def diversify_dataset(instructions, embedder, min_similarity=0.8):
    """Remove too-similar instructions."""
    embeddings = embedder.encode(instructions)

    diverse = [instructions[0]]
    diverse_embeddings = [embeddings[0]]

    for i, emb in enumerate(embeddings[1:], 1):
        max_sim = max(cosine_similarity(emb, e) for e in diverse_embeddings)
        if max_sim < min_similarity:
            diverse.append(instructions[i])
            diverse_embeddings.append(emb)

    return diverse
```

**Topic balancing**:
```python
def balance_topics(dataset, topic_classifier, target_distribution):
    """Balance dataset across topics."""
    by_topic = defaultdict(list)

    for item in dataset:
        topic = topic_classifier.predict(item["instruction"])
        by_topic[topic].append(item)

    balanced = []
    for topic, target_frac in target_distribution.items():
        n_samples = int(len(dataset) * target_frac)
        balanced.extend(random.sample(by_topic[topic], min(n_samples, len(by_topic[topic]))))

    return balanced
```

### 5. Preference Data Generation

**AI comparison for DPO**:
```python
def generate_preference_pair(prompt, generator, judge):
    """Generate preference pairs for DPO training."""
    # Generate two responses
    response_a = generator.generate(prompt, temperature=0.8)
    response_b = generator.generate(prompt, temperature=0.8)

    # Judge compares
    comparison_prompt = f"""
Which response is better for this prompt?

Prompt: {prompt}
Response A: {response_a}
Response B: {response_b}

Consider: helpfulness, accuracy, safety, clarity.
Better response (A or B):"""

    judgment = judge.generate(comparison_prompt)

    if "A" in judgment:
        return {"prompt": prompt, "chosen": response_a, "rejected": response_b}
    else:
        return {"prompt": prompt, "chosen": response_b, "rejected": response_a}
```

---

## Quality vs Quantity

### The Phi Lesson

Microsoft's research showed clear quality > quantity:

| Model | Training Tokens | Benchmark Score |
|-------|-----------------|-----------------|
| CodeGen-Mono 16B | 577B (web code) | 29.3% HumanEval |
| Phi-1 1.3B | 7B (synthetic) | 50.6% HumanEval |

**10× better with 80× less data**.

### Quality Signals

| Signal | How to Achieve |
|--------|----------------|
| Correctness | Verify outputs (execute code, check math) |
| Clarity | Use "textbook style" prompts |
| Completeness | Require full explanations |
| Coherence | Filter for logical flow |
| Diversity | Embedding-based deduplication |

### When to Scale Quantity

Quantity matters when:
1. Quality is already high (diminishing returns from filtering)
2. Coverage needed (many topics, styles, formats)
3. Robustness needed (handle edge cases)

---

## Common Pitfalls

### 1. Mode Collapse

**Problem**: Generated data becomes repetitive, model overfits to patterns.

**Solution**:
```python
# Diverse sampling
responses = []
for temp in [0.5, 0.7, 0.9, 1.1]:
    responses.append(model.generate(prompt, temperature=temp))

# Choose most diverse valid response
best = select_most_unique(responses, existing_data)
```

### 2. Hallucination Propagation

**Problem**: Errors in synthetic data get amplified in trained model.

**Solution**:
```python
# Verify factual claims
def verify_response(response, knowledge_base):
    claims = extract_claims(response)
    for claim in claims:
        if not knowledge_base.verify(claim):
            return False
    return True
```

### 3. Style Homogenization

**Problem**: All synthetic data sounds the same.

**Solution**:
```python
styles = ["formal", "casual", "technical", "simple", "verbose", "concise"]

def generate_diverse_style(prompt, model):
    style = random.choice(styles)
    styled_prompt = f"Respond in a {style} style:\n{prompt}"
    return model.generate(styled_prompt)
```

### 4. Distribution Shift

**Problem**: Synthetic data doesn't match real user queries.

**Solution**:
- Mix synthetic with real user data
- Use real prompts as seeds
- Evaluate on real user benchmarks

---

## Industry Usage

| Company | Synthetic Data Usage |
|---------|---------------------|
| **Microsoft** | Phi series, code and textbook generation |
| **Meta** | LLaMA data augmentation, CodeLlama |
| **Anthropic** | Constitutional AI, RLAIF |
| **Google** | Gemini training, self-improvement |
| **OpenAI** | InstructGPT, GPT-4 training data |

### Notable Datasets

| Dataset | Size | Generation Method |
|---------|------|-------------------|
| Alpaca | 52K | Self-Instruct from GPT-3.5 |
| WizardLM | 250K | Evol-Instruct |
| UltraChat | 1.5M | Multi-turn dialogues |
| Magicoder | 75K | OSS-Instruct (code) |
| Phi Textbooks | 1B+ tokens | GPT-3.5 textbook generation |

---

## Best Practices

### Data Generation

1. **Use best available teacher**: GPT-4 > GPT-3.5 > open models
2. **Diverse prompting**: Vary style, length, format instructions
3. **Temperature sweep**: Multiple temperatures for diversity
4. **Verification**: Check correctness where possible (code execution, math)

### Filtering

1. **Multi-stage**: Rule-based → Model-based → Human spot-check
2. **Diversity check**: Embedding deduplication
3. **Quality threshold**: Better to have less high-quality than more low-quality
4. **Balance**: Equal representation across topics/skills

### Training

1. **Mix with real data**: Don't rely 100% on synthetic
2. **Curriculum**: Start with easier synthetic, add harder
3. **Iterative**: Generate, train, evaluate, refine generation
4. **Evaluation**: Use held-out human-written test sets

---

## Future Directions

### Near-term (2025)

1. **Automated pipelines**: End-to-end synthetic data generation
2. **Verification systems**: Automatic correctness checking
3. **Adaptive generation**: Generate based on model weaknesses
4. **Multi-modal**: Synthetic image-text, video-text pairs

### Research Frontiers

1. **Self-improvement limits**: How far can models improve themselves?
2. **Data efficiency**: Minimum synthetic data for capability
3. **Quality measurement**: Better metrics for synthetic data quality
4. **Compositional generation**: Complex multi-step synthetic examples

### Open Questions

1. **Ceiling effect**: Is there a limit to synthetic data benefits?
2. **Real data replacement**: Can synthetic fully replace human data?
3. **Attribution**: Legal status of synthetic training data
4. **Contamination**: Synthetic data in benchmarks

---

## Sources

### Foundational Papers
- [Self-Instruct: Aligning LMs with Self-Generated Instructions](https://arxiv.org/abs/2212.10560) - 2022
- [WizardLM: Empowering Large Language Models with Evol-Instruct](https://arxiv.org/abs/2304.12244) - 2023
- [Textbooks Are All You Need (Phi-1)](https://arxiv.org/abs/2306.11644) - Microsoft, 2023

### Quality and Distillation
- [Orca: Progressive Learning from Complex Explanation Traces](https://arxiv.org/abs/2306.02707) - 2023
- [Phi-3 Technical Report](https://arxiv.org/abs/2404.14219) - Microsoft, 2024
- [Self-Rewarding Language Models](https://arxiv.org/abs/2401.10020) - Meta, 2024

### Analysis
- [The False Promise of Imitating Proprietary LLMs](https://arxiv.org/abs/2305.15717) - 2023
- [How Far Can Camels Go?](https://arxiv.org/abs/2306.04751) - 2023

### Datasets and Tools
- [Stanford Alpaca](https://github.com/tatsu-lab/stanford_alpaca)
- [UltraChat](https://github.com/thunlp/UltraChat)
- [Magicoder](https://github.com/ise-uiuc/magicoder)
