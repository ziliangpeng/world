# Open Source Models

Major open source LLM families with full architecture documentation:

## [Meta Llama Series](open-source-models/meta-llama.md)
- Llama 1 (7B, 13B, 33B, 65B)
- Llama 2 (7B, 13B, 70B)
- Llama 3 (8B, 70B)
- Llama 3.1 (8B, 70B, 405B)
- Llama 3.2 (1B, 3B, 11B Vision, 90B Vision)
- Llama 3.3 (70B)
- Llama 4 (Scout 17B, Maverick, Behemoth 288B - MoE, multimodal, 10M context)
- Code Llama (7B, 13B, 34B, 70B - Base/Python/Instruct variants)
- Llama Guard (7B, 8B, 1B, 12B - Safety models)
- Prompt Guard 2 (86M, 22M - Injection attack prevention)

## [Mistral/Mixtral](open-source-models/mistral-mixtral.md)
- Mistral 7B (Dense)
- Mixtral 8x7B (MoE, 46.7B total)
- Mixtral 8x22B (MoE, 141B total)

## [Qwen Series](open-source-models/qwen.md)
- Qwen 2.5 (0.5B, 1.5B, 3B, 7B, 14B, 32B, 72B)
- Qwen 3 (Dense and MoE variants with 128 experts)

## [DeepSeek](open-source-models/deepseek.md)
- DeepSeek-V2 (236B total, 21B active)
- DeepSeek-V3 (671B total, 37B active)

## [Google Gemma](open-source-models/google-gemma.md)
- Gemma 1 (2B, 7B)
- Gemma 2 (2B, 9B, 27B)

## [Microsoft Phi](open-source-models/microsoft-phi.md)
- Phi-3 Family (3.8B mini, 7B small, 14B medium, 4.2B vision)
- Phi-4 (14B)

## [Other Notable Models](open-source-models/other-models.md)
- Yi 1.5 (34B) - 01.ai
- Falcon (40B, 180B) - TII Abu Dhabi
- BLOOM (176B) - BigScience/HuggingFace
- GPT-NeoX (20B) - EleutherAI
- StableLM (1.6B, 12B) - Stability AI
- MPT (7B, 30B) - MosaicML/Databricks
- Apple OpenELM (270M, 450M, 1.1B, 3B) - Apple
- Baichuan (7B, 13B) - Baichuan Inc
- InternLM (7B, 20B) - Shanghai AI Lab
- ChatGLM (6B, 130B) - Tsinghua/Zhipu AI
- Cohere Command R/R+ (104B) - Cohere
- Databricks DBRX (132B MoE) - Databricks
- OLMo (1B-32B) - Allen Institute for AI
- Pythia Suite (70M-12B) - EleutherAI
- StarCoder/StarCoder2 (3B, 7B, 15B) - BigCode/HuggingFace
- CodeGen (350M-16B) - Salesforce
- TinyLlama (1.1B) - Community/Zhang et al
- MiniCPM (1.2B, 2.4B, 8B) - OpenBMB/Tsinghua
- RWKV (0.1B-14B) - Bo Peng et al - RNN-Transformer hybrid
- Jamba (12B-398B MoE) - AI21 Labs - SSM-Transformer hybrid, 256K context
- LLaVA (7B, 13B, 34B) - Microsoft/Wisconsin - Vision-language model
- CogVLM/CogVLM2 (8B-17B) - Tsinghua - Vision understanding
- Fuyu (8B) - Adept - Multimodal for digital agents
- Aya 101 (13B) - Cohere for AI - 101 languages
- Hunyuan-Large (389B MoE) - Tencent - 256K context, Chinese-English
- Falcon Mamba (7B) - TII - Pure SSM architecture
- Palmyra (128M-20B) - Writer - Enterprise, 1M context
- BioGPT (349M) - Microsoft - Biomedical domain
