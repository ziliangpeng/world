# Open Source Models

Major open source LLM families with full architecture documentation.

## Industry Timeline: All Models

| Date | Model | Notes |
|------|-------|-------|
| Feb 2023 | ðŸ“œ [Llama 1](https://arxiv.org/abs/2302.13971) (Meta) | 7B, 13B, 33B, 65B - First major open LLM |
| Jul 2023 | ðŸ“œ [Llama 2](https://arxiv.org/abs/2307.09288) (Meta) | 7B, 13B, 70B - Fully open-source with commercial license |
| Aug 2023 | ðŸ“œ [Code Llama](https://arxiv.org/abs/2308.12950) (Meta) | 7B, 13B, 34B - Code specialized |
| Aug 2023 | ðŸ“œ [Qwen 1.0](https://arxiv.org/abs/2309.16609) (Alibaba) | 1.8B, 7B, 14B, 72B - First Qwen series |
| Sep 2023 | ðŸ“œ [Mistral 7B](https://arxiv.org/abs/2310.06825) (Mistral AI) | Dense 7B, Sliding Window Attention |
| Oct 2023 | ðŸ“œ [Qwen-VL](https://arxiv.org/abs/2308.12966) (Alibaba) | Vision-language with grounding capabilities |
| Nov 2023 | ðŸ“œ [Qwen-Audio](https://arxiv.org/abs/2311.07919) (Alibaba) | Universal audio understanding, 30+ tasks |
| Nov 2023 | ðŸ“œ [DeepSeek-Coder V1](https://arxiv.org/abs/2401.14196) (DeepSeek) | 1.3B-33B, 16K context - First DeepSeek model |
| Nov 2023 | ðŸ“œ [DeepSeek-LLM V1](https://arxiv.org/abs/2401.02954) (DeepSeek) | 7B, 67B - General-purpose LLM |
| Dec 2023 | ðŸ“œ [Mixtral 8x7B](https://arxiv.org/abs/2401.04088) (Mistral AI) | MoE, 46.7B total, 12.9B active - First open MoE |
| Dec 2023 | ðŸ“œ [Llama Guard](https://arxiv.org/abs/2312.06674) (Meta) | 7B safety model |
| Jan 2024 | ðŸ“œ [Code Llama 70B](https://arxiv.org/abs/2308.12950) (Meta) | 70B variant added |
| Feb 2024 | Qwen 1.5 (Alibaba) | 0.5B-110B, includes MoE-A2.7B variant |
| Feb 2024 | Mistral Small (Mistral AI) | Dense 22B, low latency |
| Feb 2024 | ðŸ“œ [DeepSeek-Math](https://arxiv.org/abs/2402.03300) (DeepSeek) | 7B - Math specialized |
| Mar 2024 | ðŸ“œ [DeepSeek-VL](https://arxiv.org/abs/2403.05525) (DeepSeek) | 1.3B, 7B - Vision-language |
| Apr 2024 | ðŸ“œ [Llama 3](https://arxiv.org/abs/2407.21783) (Meta) | 8B, 70B |
| Apr 2024 | ðŸ“œ [Llama Guard 2](https://arxiv.org/abs/2407.21783) (Meta) | 8B safety model |
| Apr 2024 | Mixtral 8x22B (Mistral AI) | MoE, 141B total, 39B active, 64K context |
| May 2024 | Codestral (Mistral AI) | Dense 22B, code specialized, 80+ languages |
| May 2024 | ðŸ“œ [DeepSeek-V2](https://arxiv.org/abs/2405.04434) (DeepSeek) | 236B total, 21B active - MoE with MLA, 128K context |
| Jun 2024 | ðŸ“œ [Qwen2](https://arxiv.org/abs/2407.10671) (Alibaba) | 0.5B-72B, includes 57B-A14B MoE |
| Jul 2024 | ðŸ“œ [Llama 3.1](https://arxiv.org/abs/2407.21783) (Meta) | 8B, 70B, 405B - First 400B+ open model |
| Jul 2024 | ðŸ“œ [Llama Guard 3](https://arxiv.org/abs/2407.21783) (Meta) | 1B, 12B safety models |
| Jul 2024 | ðŸ“œ [Qwen2-Audio](https://arxiv.org/abs/2407.10759) (Alibaba) | Voice chat + audio analysis modes |
| Jul 2024 | Mistral NeMo (Mistral AI) | Dense 12B, NVIDIA collab, Tekken tokenizer, 128K context |
| Jul 2024 | Mistral Large 2 (Mistral AI) | Dense 123B, flagship model, 128K context |
| Jul 2024 | Codestral Mamba (Mistral AI) | SSM 7B, State Space Model, 256K context |
| Jul 2024 | Mathstral 7B (Mistral AI) | Dense 7B, math/STEM specialized, 32K context |
| Sep 2024 | ðŸ“œ [Llama 3.2](https://arxiv.org/abs/2407.21783) (Meta) | 1B, 3B, 11B Vision, 90B Vision - First Llama multimodal |
| Sep 2024 | ðŸ“œ [Qwen2-VL](https://arxiv.org/abs/2409.12191) (Alibaba) | 2B, 7B - Any resolution vision |
| Sep 2024 | ðŸ“œ [Qwen2.5](https://arxiv.org/abs/2412.15115) (Alibaba) | 0.5B-72B, 18T tokens, 128K context |
| Sep 2024 | ðŸ“œ [Qwen2.5-Coder](https://arxiv.org/abs/2409.12186) (Alibaba) | 0.5B-32B, 5.5T code tokens, 92 languages |
| Sep 2024 | ðŸ“œ [Qwen2.5-Math](https://arxiv.org/abs/2409.12122) (Alibaba) | 1.5B-72B - Math specialist via self-improvement |
| Sep 2024 | ðŸ“œ [Pixtral 12B](https://arxiv.org/abs/2410.07073) (Mistral AI) | Multimodal, 12B + 400M vision encoder |
| Oct 2024 | Ministral 3B (Mistral AI) | Dense 3B, edge/on-device, 128K context |
| Oct 2024 | Ministral 8B (Mistral AI) | Dense 8B, edge/on-device, 128K context |
| Nov 2024 | [Mistral Large 24.11](https://mistral.ai/news/pixtral-large) (Mistral AI) | Dense 123B, improved long context, 131K context |
| Nov 2024 | Pixtral Large (Mistral AI) | Multimodal, 123B + 1B vision encoder, frontier multimodal |
| Nov 2024 | QwQ-32B-Preview (Alibaba) | 32B reasoning model, o1-style thinking |
| Nov 2024 | ðŸ“œ [INTELLECT-1](https://arxiv.org/abs/2412.01152) (Prime Intellect) | 10B - First decentralized training across 3 continents |
| Dec 2024 | ðŸ“œ [DeepSeek-V3](https://arxiv.org/abs/2412.19437) (DeepSeek) | 671B total, 37B active - 14.8T tokens, 128K context |
| Dec 2024 | ðŸ“œ [Llama 3.3](https://arxiv.org/abs/2407.21783) (Meta) | 70B |
| Jan 2025 | ðŸ“œ [DeepSeek-R1](https://arxiv.org/abs/2501.12948) (DeepSeek) | 671B MoE - Reasoning via RL, comparable to OpenAI o1 |
| Jan 2025 | Qwen2.5-VL (Alibaba) | 3B-72B - Multi-resolution vision |
| Mar 2025 | Qwen2.5-Omni (Alibaba) | 7B omni-modal (text/image/video/audio) |
| Apr 2025 | ðŸ“œ [Qwen3](https://arxiv.org/abs/2505.09388) (Alibaba) | 0.6B-235B, dense + MoE, 36T tokens, 119 languages |
| Apr 2025 | ðŸ“œ [Llama 4](https://arxiv.org/abs/2510.12178) (Meta) | Scout 17B, Maverick, Behemoth 288B - MoE, multimodal, 10M context |
| Apr 2025 | Prompt Guard 2 (Meta) | 86M, 22M - Injection attack prevention |
| May 2025 | ðŸ“œ [INTELLECT-2](https://arxiv.org/abs/2505.07291) (Prime Intellect) | 32B - First decentralized RL training |
| Jul 2025 | Qwen3-Coder (Alibaba) | 480B MoE (35B active) - Agentic coding |
| Sep 2025 | Qwen3-Omni (Alibaba) | Real-time multimodal with speech generation |
| Sep 2025 | Qwen3-Max (Alibaba) | Flagship proprietary model |
| Sep 2025 | Qwen3-Next (Alibaba) | 80B total, 3B active MoE |

---

## [Meta Llama Series](open-source-models/meta-llama.md)

| Date | Model | Notes |
|------|-------|-------|
| Feb 2023 | ðŸ“œ [Llama 1](https://arxiv.org/abs/2302.13971) | 7B, 13B, 33B, 65B |
| Jul 2023 | ðŸ“œ [Llama 2](https://arxiv.org/abs/2307.09288) | 7B, 13B, 70B |
| Aug 2023 | ðŸ“œ [Code Llama](https://arxiv.org/abs/2308.12950) | 7B, 13B, 34B - Base/Python/Instruct |
| Dec 2023 | ðŸ“œ [Llama Guard](https://arxiv.org/abs/2312.06674) | 7B safety model |
| Jan 2024 | ðŸ“œ [Code Llama 70B](https://arxiv.org/abs/2308.12950) | 70B variant added |
| Apr 2024 | ðŸ“œ [Llama 3](https://arxiv.org/abs/2407.21783) | 8B, 70B |
| Apr 2024 | ðŸ“œ [Llama Guard 2](https://arxiv.org/abs/2407.21783) | 8B safety model |
| Jul 2024 | ðŸ“œ [Llama 3.1](https://arxiv.org/abs/2407.21783) | 8B, 70B, 405B |
| Jul 2024 | ðŸ“œ [Llama Guard 3](https://arxiv.org/abs/2407.21783) | 1B, 12B safety models |
| Sep 2024 | ðŸ“œ [Llama 3.2](https://arxiv.org/abs/2407.21783) | 1B, 3B, 11B Vision, 90B Vision |
| Dec 2024 | ðŸ“œ [Llama 3.3](https://arxiv.org/abs/2407.21783) | 70B |
| Apr 2025 | ðŸ“œ [Llama 4](https://arxiv.org/abs/2510.12178) | Scout 17B, Maverick, Behemoth 288B - MoE, multimodal, 10M context |
| Apr 2025 | Prompt Guard 2 | 86M, 22M - Injection attack prevention |

## [Mistral/Mixtral](open-source-models/mistral-mixtral.md)

| Date | Model | Notes |
|------|-------|-------|
| Sep 2023 | ðŸ“œ [Mistral 7B](https://arxiv.org/abs/2310.06825) | Dense 7B, Sliding Window Attention |
| Dec 2023 | ðŸ“œ [Mixtral 8x7B](https://arxiv.org/abs/2401.04088) | MoE, 46.7B total, 12.9B active |
| Feb 2024 | [Mistral Small](https://mistral.ai/news/mistral-large) | Dense 22B, low latency |
| Apr 2024 | [Mixtral 8x22B](https://mistral.ai/news/mixtral-8x22b) | MoE, 141B total, 39B active, 64K context |
| May 2024 | [Codestral](https://mistral.ai/news/codestral) | Dense 22B, code specialized, 80+ languages |
| Jul 2024 | [Mistral NeMo](https://mistral.ai/news/mistral-nemo) | Dense 12B, NVIDIA collab, Tekken tokenizer, 128K context |
| Jul 2024 | [Mistral Large 2](https://mistral.ai/news/mistral-large-2407) | Dense 123B, flagship model, 128K context |
| Jul 2024 | [Codestral Mamba](https://mistral.ai/news/codestral-mamba) | SSM 7B, State Space Model, 256K context |
| Jul 2024 | [Mathstral 7B](https://mistral.ai/news/mathstral) | Dense 7B, math/STEM specialized, 32K context |
| Sep 2024 | ðŸ“œ [Pixtral 12B](https://arxiv.org/abs/2410.07073) | Multimodal, 12B + 400M vision encoder |
| Oct 2024 | [Ministral 3B](https://mistral.ai/news/ministraux) | Dense 3B, edge/on-device, 128K context |
| Oct 2024 | [Ministral 8B](https://mistral.ai/news/ministraux) | Dense 8B, edge/on-device, 128K context |
| Nov 2024 | [Mistral Large 24.11](https://mistral.ai/news/pixtral-large) | Dense 123B, improved long context, 131K context |
| Nov 2024 | [Pixtral Large](https://mistral.ai/news/pixtral-large) | Multimodal, 123B + 1B vision encoder, frontier multimodal |

## [Qwen Series](open-source-models/qwen.md)

| Date | Model | Notes |
|------|-------|-------|
| Aug 2023 | ðŸ“œ [Qwen 1.0](https://arxiv.org/abs/2309.16609) | 1.8B, 7B, 14B, 72B |
| Oct 2023 | ðŸ“œ [Qwen-VL](https://arxiv.org/abs/2308.12966) | Vision-language with grounding |
| Nov 2023 | ðŸ“œ [Qwen-Audio](https://arxiv.org/abs/2311.07919) | Universal audio understanding |
| Feb 2024 | Qwen 1.5 | 0.5B-110B, includes MoE-A2.7B |
| Jun 2024 | ðŸ“œ [Qwen2](https://arxiv.org/abs/2407.10671) | 0.5B-72B, 57B-A14B MoE |
| Jul 2024 | ðŸ“œ [Qwen2-Audio](https://arxiv.org/abs/2407.10759) | Voice chat + audio analysis |
| Sep 2024 | ðŸ“œ [Qwen2-VL](https://arxiv.org/abs/2409.12191) | 2B, 7B - Any resolution vision |
| Sep 2024 | ðŸ“œ [Qwen2.5](https://arxiv.org/abs/2412.15115) | 0.5B-72B, 18T tokens, 128K context |
| Sep 2024 | ðŸ“œ [Qwen2.5-Coder](https://arxiv.org/abs/2409.12186) | 0.5B-32B - Code specialized |
| Sep 2024 | ðŸ“œ [Qwen2.5-Math](https://arxiv.org/abs/2409.12122) | 1.5B-72B - Math specialized |
| Nov 2024 | QwQ-32B-Preview | 32B reasoning model |
| Jan 2025 | Qwen2.5-VL | 3B-72B vision models |
| Mar 2025 | Qwen2.5-Omni | 7B omni-modal |
| Apr 2025 | ðŸ“œ [Qwen3](https://arxiv.org/abs/2505.09388) | 0.6B-235B, dense + MoE, 36T tokens |
| Jul 2025 | Qwen3-Coder | 480B MoE - Agentic coding |
| Sep 2025 | Qwen3-Omni | Real-time multimodal |
| Sep 2025 | Qwen3-Max | Flagship proprietary |
| Sep 2025 | Qwen3-Next | 80B total, 3B active MoE |

## [DeepSeek](open-source-models/deepseek.md)

| Date | Model | Notes |
|------|-------|-------|
| Nov 2023 | ðŸ“œ [DeepSeek-Coder V1](https://arxiv.org/abs/2401.14196) | 1.3B, 5.7B, 6.7B, 33B - First DeepSeek model, 16K context |
| Nov 2023 | ðŸ“œ [DeepSeek-LLM V1](https://arxiv.org/abs/2401.02954) | 7B, 67B - First general-purpose LLM |
| Feb 2024 | ðŸ“œ [DeepSeek-Math](https://arxiv.org/abs/2402.03300) | 7B - Math specialized |
| Mar 2024 | ðŸ“œ [DeepSeek-VL](https://arxiv.org/abs/2403.05525) | 1.3B, 7B - First vision-language model |
| May 2024 | ðŸ“œ [DeepSeek-V2](https://arxiv.org/abs/2405.04434) | 236B total, 21B active - MoE with MLA, 128K context |
| May 2024 | ðŸ“œ [DeepSeek-Prover V1](https://arxiv.org/abs/2405.14333) | Theorem proving for Lean 4 |
| Jun 2024 | DeepSeek-Coder-V2 | 16B/236B MoE - 128K context |
| Aug 2024 | DeepSeek-Prover-V1.5 | Enhanced theorem proving with RLPAF |
| Sep 2024 | DeepSeek-V2.5 | 236B total, 21B active - Unified general + coding |
| Nov 2024 | ðŸ“œ [JanusFlow](https://arxiv.org/abs/2411.07975) | 1.3B - Unified image understanding + generation |
| Nov 2024 | DeepSeek-R1-Lite-Preview | Reasoning model preview (API only) |
| Dec 2024 | ðŸ“œ [DeepSeek-VL2](https://arxiv.org/abs/2412.10302) | 3.37B-27.5B MoE - Advanced multimodal |
| Dec 2024 | ðŸ“œ [DeepSeek-V3](https://arxiv.org/abs/2412.19437) | 671B total, 37B active - 14.8T tokens, 128K context |
| Jan 2025 | ðŸ“œ [DeepSeek-R1](https://arxiv.org/abs/2501.12948) | 671B MoE - Reasoning via RL, comparable to OpenAI o1 |
| Jan 2025 | DeepSeek-R1 Distilled | 1.5B-70B dense - Distilled from R1 |
| Jan 2025 | Janus-Pro | 1.5B, 7B - Multimodal understanding + generation |
| Mar 2025 | DeepSeek-V3-0324 | 671B MoE - MIT licensed update |
| Apr 2025 | DeepSeek-Prover-V2 | 7B, 671B - State-of-the-art theorem proving |
| May 2025 | DeepSeek-R1-0528 | 685B MoE - System prompts, function calling |
| Aug 2025 | DeepSeek-V3.1 | 840B MoE - Hybrid reasoning modes |
| Sep 2025 | DeepSeek-V3.1-Terminus | 671B MoE - V3 finale, agent capabilities |
| Sep 2025 | DeepSeek-V3.2-Exp | 671B MoE - Sparse attention optimization |
| Oct 2025 | ðŸ“œ [DeepSeek-OCR](https://arxiv.org/abs/2510.18234) | 3B MoE (570M active) - Optical context compression |

## [Google Gemma](open-source-models/google-gemma.md)

| Date | Model | Notes |
|------|-------|-------|
| Feb 2024 | ðŸ“œ [Gemma 1](https://arxiv.org/abs/2403.08295) | 2B, 7B |
| Apr 2024 | ðŸ“œ [CodeGemma](https://arxiv.org/abs/2406.11409) | 2B, 7B - Code specialized |
| May 2024 | ðŸ“œ [PaliGemma](https://arxiv.org/abs/2407.07726) | Vision-language model |
| Jun 2024 | ðŸ“œ [Gemma 2](https://arxiv.org/abs/2408.00118) | 2B, 7B, 9B, 27B |
| Jul 2024 | ðŸ“œ [ShieldGemma](https://arxiv.org/abs/2407.21772) | Safety assessment model |
| Mar 2025 | ðŸ“œ [Gemma 3](https://arxiv.org/abs/2503.19786) | 270M-27B - Multimodal |
| Mar 2025 | ðŸ“œ [ShieldGemma 2](https://arxiv.org/abs/2504.01081) | 4B - Safety assessment model |

## [Microsoft Phi](open-source-models/microsoft-phi.md)

| Date | Model | Notes |
|------|-------|-------|
| Jun 2023 | ðŸ“œ [Phi-1](https://arxiv.org/abs/2306.11644) | 1.3B - Python coding, textbook-quality data |
| Sep 2023 | ðŸ“œ [Phi-1.5](https://arxiv.org/abs/2309.05463) | 1.3B - Common sense reasoning, 2K context |
| Dec 2023 | [Phi-2](https://www.microsoft.com/en-us/research/blog/phi-2-the-surprising-power-of-small-language-models/) | 2.7B - 1.4T tokens, 2K context |
| Apr 2024 | ðŸ“œ [Phi-3-mini](https://arxiv.org/abs/2404.14219) | 3.8B - 4K/128K context variants |
| May 2024 | ðŸ“œ [Phi-3-small](https://arxiv.org/abs/2404.14219) | 7B - 8K/128K context variants |
| May 2024 | ðŸ“œ [Phi-3-medium](https://arxiv.org/abs/2404.14219) | 14B - 4K/128K context variants |
| May 2024 | ðŸ“œ [Phi-3-vision](https://arxiv.org/abs/2404.14219) | 4.2B - Multimodal (text + vision), 128K context |
| May 2024 | [Phi-Silica](https://blogs.windows.com/windowsexperience/2024/12/06/phi-silica-small-but-mighty-on-device-slm/) | 3.3B - Optimized for Copilot+ PC NPUs |
| Aug 2024 | [Phi-3.5-mini-instruct](https://techcommunity.microsoft.com/t5/ai-azure-ai-services-blog/discover-the-new-multi-lingual-high-quality-phi-3-5-slms/ba-p/4225280) | 3.8B - 128K context, multi-lingual |
| Aug 2024 | [Phi-3.5-MoE-instruct](https://techcommunity.microsoft.com/t5/ai-azure-ai-services-blog/discover-the-new-multi-lingual-high-quality-phi-3-5-slms/ba-p/4225280) | 42B total, 6.6B active - 16 experts, 128K context |
| Aug 2024 | [Phi-3.5-vision-instruct](https://techcommunity.microsoft.com/t5/ai-azure-ai-services-blog/discover-the-new-multi-lingual-high-quality-phi-3-5-slms/ba-p/4225280) | 4.2B - Multimodal, 128K context |
| Dec 2024 | [Phi-4](https://techcommunity.microsoft.com/blog/azure-ai-foundry-blog/introducing-phi-4-microsoft%E2%80%99s-newest-small-language-model-specializing-in-comple/4357090) | 14B - 16K context, complex reasoning, synthetic data |
| Feb 2025 | ðŸ“œ [Phi-4-mini](https://arxiv.org/abs/2503.01743) | 3.8B - 128K context, speed optimized |
| Feb 2025 | ðŸ“œ [Phi-4-multimodal-instruct](https://arxiv.org/abs/2503.01743) | 5.6B - Speech + vision + text, 20+ languages |

## [Other Notable Models](open-source-models/other-models.md)

| Date | Model | Size | Organization | Notes |
|------|-------|------|--------------|-------|
| Mar 2021 | [GPT-Neo](https://zenodo.org/records/5297715) | 2.7B | EleutherAI | First free GPT-3 alternative |
| Jun 2021 | [GPT-J](https://www.eleuther.ai/artifacts/gpt-j) | 6B | EleutherAI | Largest public GPT-3 style model at release |
| Jan 2022 | ðŸ“œ [BLIP](https://arxiv.org/abs/2201.12086) | - | Salesforce | Vision-language pre-training with bootstrapping |
| Feb 2022 | ðŸ“œ [GPT-NeoX](https://arxiv.org/abs/2204.06745) | 20B | EleutherAI | Scaled up to 20B |
| Mar 2022 | ðŸ“œ [CodeGen](https://arxiv.org/abs/2203.13474) | 350M-16B | Salesforce | Code generation specialist, competitive with Codex |
| May 2022 | ðŸ“œ [OPT](https://arxiv.org/abs/2205.01068) | 125M-175B | Meta AI | Democratizing LLM access |
| Jul 2022 | ðŸ“œ [BLOOM](https://arxiv.org/abs/2211.05100) | 176B | BigScience/HuggingFace | Multilingual collaborative effort |
| Oct 2022 | ðŸ“œ [BioGPT](https://arxiv.org/abs/2210.10341) | 349M | Microsoft | Biomedical domain |
| Jan 2023 | ðŸ“œ [BLIP-2](https://arxiv.org/abs/2301.12597) | - | Salesforce | Efficient vision-language with frozen encoders |
| Feb 2023 | ðŸ“œ [Pythia Suite](https://arxiv.org/abs/2304.01373) | 70M-12B | EleutherAI | 143 checkpoints for interpretability research |
| Feb 2023 | [Palmyra](https://writer.com/blog/palmyra/) | 128M-20B | Writer | Enterprise, 1M context |
| Mar 2023 | ðŸ“œ [Falcon 40B](https://arxiv.org/abs/2311.16867) | 40B | TII Abu Dhabi | RefinedWeb dataset, Apache 2.0 license |
| Mar 2023 | ðŸ“œ [ChatGLM](https://arxiv.org/abs/2406.12793) | 6B, 130B | Tsinghua/Zhipu AI | Bilingual Chinese-English conversational model |
| Mar 2023 | [Alpaca](https://crfm.stanford.edu/2023/03/13/alpaca.html) | 7B | Stanford | First low-cost instruction model, <$600 training |
| Mar 2023 | [Vicuna](https://lmsys.org/blog/2023-03-30-vicuna/) | 7B, 13B | UC Berkeley LMSYS | 90% ChatGPT quality, trained on ShareGPT |
| Apr 2023 | ðŸ“œ [StableLM](https://arxiv.org/abs/2402.17834) | 1.6B, 12B | Stability AI | 1.5T tokens, permissive commercial license |
| Apr 2023 | ðŸ“œ [LLaVA](https://arxiv.org/abs/2304.08485) | 7B, 13B, 34B | Microsoft/Wisconsin | Vision-language model |
| Apr 2023 | ðŸ“œ [MiniGPT-4](https://arxiv.org/abs/2304.10592) | - | KAUST | GPT-4-like vision with minimal training |
| Apr 2023 | ðŸ“œ [LLaMA-Adapter V2](https://arxiv.org/abs/2304.15010) | - | Shanghai AI Lab | Parameter-efficient multimodal fine-tuning |
| Apr 2023 | ðŸ“œ [WizardLM](https://arxiv.org/abs/2304.12244) | 7B, 13B, 70B | Microsoft/Independent | Evol-Instruct methodology |
| Apr 2023 | [Dolly 2.0](https://www.databricks.com/blog/2023/04/12/dolly-first-open-commercially-viable-instruction-tuned-llm) | 12B | Databricks | First commercial-friendly instruction model |
| Apr 2023 | ðŸ“œ [OpenAssistant](https://arxiv.org/abs/2304.07327) | 12B, 30B | LAION-AI | Community-driven, 161K conversations, 35 languages |
| May 2023 | [MPT-7B](https://www.databricks.com/blog/mpt-7b) | 7B | MosaicML/Databricks | 65K context variant, Apache 2.0, efficient training |
| May 2023 | ðŸ“œ [InstructBLIP](https://arxiv.org/abs/2305.06500) | - | Salesforce | Instruction-tuning for vision-language models |
| May 2023 | ðŸ“œ [StarCoder](https://arxiv.org/abs/2305.06161) | 3B, 7B, 15B | BigCode/HuggingFace | Trained on The Stack, 8K context, fill-in-the-middle |
| May 2023 | ðŸ“œ [RWKV](https://arxiv.org/abs/2305.13048) | 0.1B-14B | Bo Peng et al | RNN-Transformer hybrid |
| Jun 2023 | ðŸ“œ [Orca](https://arxiv.org/abs/2306.02707) | 13B | Microsoft | Learning from GPT-4 reasoning traces |
| Jun 2023 | ðŸ“œ [WizardCoder](https://arxiv.org/abs/2306.08568) | 15B, 34B | Microsoft/Independent | Code Evol-Instruct, surpassed Claude/Bard |
| Jun 2023 | [MPT-30B](https://www.databricks.com/blog/mpt-30b) | 30B | MosaicML/Databricks | 8K context with ALiBi attention |
| Jun 2023 | ðŸ“œ [Baichuan-7B](https://arxiv.org/abs/2309.10305) | 7B | Baichuan Inc | Chinese-optimized bilingual, commercial license |
| Jul 2023 | ðŸ“œ [Baichuan-13B](https://arxiv.org/abs/2309.10305) | 13B | Baichuan Inc | 1.4T tokens, Chinese language benchmarks |
| Jul 2023 | [InternLM](https://github.com/InternLM/InternLM-techreport) | 7B, 20B | Shanghai AI Lab | Bilingual research model from Shanghai AI Lab |
| Aug 2023 | ðŸ“œ [IDEFICS](https://arxiv.org/abs/2306.16527) | 9B, 80B | Hugging Face | Open-source Flamingo-like vision-language model |
| Sep 2023 | ðŸ“œ [Falcon 180B](https://arxiv.org/abs/2311.16867) | 180B | TII Abu Dhabi | Largest open model at release, 3.5T tokens |
| Oct 2023 | [Fuyu](https://www.adept.ai/blog/fuyu-8b) | 8B | Adept | Multimodal for digital agents |
| Nov 2023 | ðŸ“œ [CogVLM](https://arxiv.org/abs/2311.03079) | 8B-17B | Tsinghua | Vision understanding |
| Jan 2024 | ðŸ“œ [TinyLlama](https://arxiv.org/abs/2401.02385) | 1.1B | Community/Zhang et al | 3T tokens in 90 days, 637MB quantized for edge |
| Feb 2024 | ðŸ“œ [OLMo](https://arxiv.org/abs/2402.00838) | 1B-32B | Allen Institute for AI | Fully transparent: data, code, evaluation suite |
| Feb 2024 | ðŸ“œ [MiniCPM](https://arxiv.org/abs/2404.06395) | 1.2B, 2.4B, 8B | OpenBMB/Tsinghua | Efficient edge/smartphone deployment |
| Feb 2024 | ðŸ“œ [StarCoder2](https://arxiv.org/abs/2402.19173) | 3B, 7B, 15B | BigCode/HuggingFace | Improved v2 on larger, more diverse code data |
| Feb 2024 | ðŸ“œ [Aya 101](https://arxiv.org/abs/2402.07827) | 13B | Cohere for AI | 101 languages |
| Mar 2024 | [Databricks DBRX](https://www.databricks.com/blog/introducing-dbrx-new-state-art-open-llm) | 132B MoE | Databricks | Fine-grained MoE (16 experts), 2x faster than Llama2-70B |
| Mar 2024 | ðŸ“œ [Jamba](https://arxiv.org/abs/2403.19887) | 12B-398B MoE | AI21 Labs | SSM-Transformer hybrid, 256K context |
| Apr 2024 | ðŸ“œ [Apple OpenELM](https://arxiv.org/abs/2404.14619) | 270M, 450M, 1.1B, 3B | Apple | Layer-wise scaling, Apple's first open LLM |
| Apr 2024 | [StableLM 2](https://stability.ai/news/introducing-stable-lm-2-12b) | 12B | Stability AI | Improved multilingual and efficiency v2 |
| Apr 2024 | [Cohere Command R/R+](https://cohere.com/blog/command-r) | 104B | Cohere | RAG-optimized with citations, 128K context, 10 languages |
| May 2024 | ðŸ“œ [Yi 1.5](https://arxiv.org/abs/2403.04652) | 34B | 01.ai | Enhanced long context (32K+) |
| May 2024 | ðŸ“œ [CogVLM2](https://arxiv.org/abs/2408.16500) | 8B-17B | Tsinghua | Vision understanding |
| Aug 2024 | ðŸ“œ [Falcon Mamba](https://arxiv.org/abs/2410.05355) | 7B | TII | Pure SSM architecture |
| Nov 2024 | ðŸ“œ [Hunyuan-Large](https://arxiv.org/abs/2411.02265) | 389B MoE | Tencent | 256K context, Chinese-English |
| Nov 2024 | ðŸ“œ [INTELLECT-1](https://arxiv.org/abs/2412.01152) | 10B | Prime Intellect | First decentralized training across continents |
| May 2025 | ðŸ“œ [INTELLECT-2](https://arxiv.org/abs/2505.07291) | 32B | Prime Intellect | First decentralized RL training |
