# Open Source Models

Major open source LLM families with full architecture documentation.

## Industry Timeline: All Models

| Date | Model | Notes |
|------|-------|-------|
| Feb 2023 | ðŸ“œ [Llama 1](https://arxiv.org/abs/2302.13971) (Meta) | 7B, 13B, 33B, 65B - First major open LLM |
| Jul 2023 | ðŸ“œ [Llama 2](https://arxiv.org/abs/2307.09288) (Meta) | 7B, 13B, 70B - Fully open-source with commercial license |
| Aug 2023 | ðŸ“œ [Code Llama](https://arxiv.org/abs/2308.12950) (Meta) | 7B, 13B, 34B - Code specialized |
| Aug 2023 | ðŸ“œ [Qwen 1.0](https://arxiv.org/abs/2309.16609) (Alibaba) | 1.8B, 7B, 14B, 72B - First Qwen series |
| Sep 2023 | ðŸ“œ [Mistral 7B](https://arxiv.org/abs/2310.06825) (Mistral AI) | Dense 7B, Sliding Window Attention |
| Oct 2023 | ðŸ“œ [Qwen-VL](https://arxiv.org/abs/2308.12966) (Alibaba) | Vision-language with grounding capabilities |
| Nov 2023 | ðŸ“œ [Qwen-Audio](https://arxiv.org/abs/2311.07919) (Alibaba) | Universal audio understanding, 30+ tasks |
| Nov 2023 | ðŸ“œ [DeepSeek-Coder V1](https://arxiv.org/abs/2401.14196) (DeepSeek) | 1.3B-33B, 16K context - First DeepSeek model |
| Nov 2023 | ðŸ“œ [DeepSeek-LLM V1](https://arxiv.org/abs/2401.02954) (DeepSeek) | 7B, 67B - General-purpose LLM |
| Dec 2023 | ðŸ“œ [Mixtral 8x7B](https://arxiv.org/abs/2401.04088) (Mistral AI) | MoE, 46.7B total, 12.9B active - First open MoE |
| Dec 2023 | ðŸ“œ [Llama Guard](https://arxiv.org/abs/2312.06674) (Meta) | 7B safety model |
| Jan 2024 | ðŸ“œ [Code Llama 70B](https://arxiv.org/abs/2308.12950) (Meta) | 70B variant added |
| Feb 2024 | [Qwen 1.5](https://qwenlm.github.io/blog/qwen1.5/) (Alibaba) | 0.5B-110B, includes MoE-A2.7B variant |
| Feb 2024 | [Mistral Small](https://mistral.ai/news/mistral-large) (Mistral AI) | Dense 22B, low latency |
| Feb 2024 | ðŸ“œ [DeepSeek-Math](https://arxiv.org/abs/2402.03300) (DeepSeek) | 7B - Math specialized |
| Mar 2024 | ðŸ“œ [DeepSeek-VL](https://arxiv.org/abs/2403.05525) (DeepSeek) | 1.3B, 7B - Vision-language |
| Apr 2024 | ðŸ“œ [Llama 3](https://arxiv.org/abs/2407.21783) (Meta) | 8B, 70B |
| Apr 2024 | ðŸ“œ [Llama Guard 2](https://arxiv.org/abs/2407.21783) (Meta) | 8B safety model |
| Apr 2024 | [Mixtral 8x22B](https://mistral.ai/news/mixtral-8x22b) (Mistral AI) | MoE, 141B total, 39B active, 64K context |
| May 2024 | [Codestral](https://mistral.ai/news/codestral) (Mistral AI) | Dense 22B, code specialized, 80+ languages |
| May 2024 | ðŸ“œ [DeepSeek-V2](https://arxiv.org/abs/2405.04434) (DeepSeek) | 236B total, 21B active - MoE with MLA, 128K context |
| Jun 2024 | ðŸ“œ [Qwen2](https://arxiv.org/abs/2407.10671) (Alibaba) | 0.5B-72B, includes 57B-A14B MoE |
| Jul 2024 | ðŸ“œ [Llama 3.1](https://arxiv.org/abs/2407.21783) (Meta) | 8B, 70B, 405B - First 400B+ open model |
| Jul 2024 | ðŸ“œ [Llama Guard 3](https://arxiv.org/abs/2407.21783) (Meta) | 1B, 12B safety models |
| Jul 2024 | ðŸ“œ [Qwen2-Audio](https://arxiv.org/abs/2407.10759) (Alibaba) | Voice chat + audio analysis modes |
| Jul 2024 | [Mistral NeMo](https://mistral.ai/news/mistral-nemo) (Mistral AI) | Dense 12B, NVIDIA collab, Tekken tokenizer, 128K context |
| Jul 2024 | [Mistral Large 2](https://mistral.ai/news/mistral-large-2407) (Mistral AI) | Dense 123B, flagship model, 128K context |
| Jul 2024 | [Codestral Mamba](https://mistral.ai/news/codestral-mamba) (Mistral AI) | SSM 7B, State Space Model, 256K context |
| Jul 2024 | [Mathstral 7B](https://mistral.ai/news/mathstral) (Mistral AI) | Dense 7B, math/STEM specialized, 32K context |
| Sep 2024 | ðŸ“œ [Llama 3.2](https://arxiv.org/abs/2407.21783) (Meta) | 1B, 3B, 11B Vision, 90B Vision - First Llama multimodal |
| Sep 2024 | ðŸ“œ [Qwen2-VL](https://arxiv.org/abs/2409.12191) (Alibaba) | 2B, 7B - Any resolution vision |
| Sep 2024 | ðŸ“œ [Qwen2.5](https://arxiv.org/abs/2412.15115) (Alibaba) | 0.5B-72B, 18T tokens, 128K context |
| Sep 2024 | ðŸ“œ [Qwen2.5-Coder](https://arxiv.org/abs/2409.12186) (Alibaba) | 0.5B-32B, 5.5T code tokens, 92 languages |
| Sep 2024 | ðŸ“œ [Qwen2.5-Math](https://arxiv.org/abs/2409.12122) (Alibaba) | 1.5B-72B - Math specialist via self-improvement |
| Sep 2024 | ðŸ“œ [Pixtral 12B](https://arxiv.org/abs/2410.07073) (Mistral AI) | Multimodal, 12B + 400M vision encoder |
| Oct 2024 | [Ministral 3B](https://mistral.ai/news/ministraux) (Mistral AI) | Dense 3B, edge/on-device, 128K context |
| Oct 2024 | [Ministral 8B](https://mistral.ai/news/ministraux) (Mistral AI) | Dense 8B, edge/on-device, 128K context |
| Nov 2024 | [Mistral Large 24.11](https://mistral.ai/news/pixtral-large) (Mistral AI) | Dense 123B, improved long context, 131K context |
| Nov 2024 | [Pixtral Large](https://mistral.ai/news/pixtral-large) (Mistral AI) | Multimodal, 123B + 1B vision encoder, frontier multimodal |
| Nov 2024 | [QwQ-32B-Preview](https://huggingface.co/Qwen/QwQ-32B-Preview) (Alibaba) | 32B reasoning model, o1-style thinking |
| Nov 2024 | ðŸ“œ [INTELLECT-1](https://arxiv.org/abs/2412.01152) (Prime Intellect) | 10B - First decentralized training across 3 continents |
| Dec 2024 | ðŸ“œ [DeepSeek-V3](https://arxiv.org/abs/2412.19437) (DeepSeek) | 671B total, 37B active - 14.8T tokens, 128K context |
| Dec 2024 | ðŸ“œ [Llama 3.3](https://arxiv.org/abs/2407.21783) (Meta) | 70B |
| Jan 2025 | ðŸ“œ [DeepSeek-R1](https://arxiv.org/abs/2501.12948) (DeepSeek) | 671B MoE - Reasoning via RL, comparable to OpenAI o1 |
| Jan 2025 | [Qwen2.5-VL](https://huggingface.co/Qwen) (Alibaba) | 3B-72B - Multi-resolution vision |
| Mar 2025 | [Qwen2.5-Omni](https://github.com/QwenLM/Qwen2.5-Omni) (Alibaba) | 7B omni-modal (text/image/video/audio) |
| Apr 2025 | ðŸ“œ [Qwen3](https://arxiv.org/abs/2505.09388) (Alibaba) | 0.6B-235B, dense + MoE, 36T tokens, 119 languages |
| Apr 2025 | ðŸ“œ [Llama 4](https://arxiv.org/abs/2510.12178) (Meta) | Scout 17B, Maverick, Behemoth 288B - MoE, multimodal, 10M context |
| Apr 2025 | [Prompt Guard 2](https://huggingface.co/meta-llama/Llama-Prompt-Guard-2-86M) (Meta) | 86M, 22M - Injection attack prevention |
| May 2025 | ðŸ“œ [INTELLECT-2](https://arxiv.org/abs/2505.07291) (Prime Intellect) | 32B - First decentralized RL training |
| Jul 2025 | [Qwen3-Coder](https://github.com/QwenLM/Qwen3-Coder) (Alibaba) | 480B MoE (35B active) - Agentic coding |
| Sep 2025 | [Qwen3-Omni](https://github.com/QwenLM/Qwen3-Omni) (Alibaba) | Real-time multimodal with speech generation |
| Sep 2025 | Qwen3-Max (Alibaba) | Flagship proprietary model |
| Sep 2025 | [Qwen3-Next](https://github.com/QwenLM/Qwen3) (Alibaba) | 80B total, 3B active MoE |

---

## [Meta Llama Series](open-source-models/meta/meta-llama-overview.md)

| Date | Model | Notes |
|------|-------|-------|
| Feb 2023 | ðŸ“œ [Llama 1](https://arxiv.org/abs/2302.13971) | ðŸ”¥ 7B, 13B, 33B, 65B - First major open LLM |
| Jul 2023 | ðŸ“œ [Llama 2](https://arxiv.org/abs/2307.09288) | ðŸ”¥ 7B, 13B, 70B - Commercial license, RLHF |
| Aug 2023 | ðŸ“œ [Code Llama](https://arxiv.org/abs/2308.12950) | ðŸ”¥ 7B, 13B, 34B - Base/Python/Instruct |
| Dec 2023 | ðŸ“œ [Llama Guard](https://arxiv.org/abs/2312.06674) | ðŸ”¥ 7B safety model - First safety classifier |
| Jan 2024 | ðŸ“œ [Code Llama 70B](https://arxiv.org/abs/2308.12950) | ðŸ¥± 70B variant added |
| Apr 2024 | ðŸ“œ [Llama 3](https://arxiv.org/abs/2407.21783) | ðŸ”¥ 8B, 70B - 15T tokens, improved architecture |
| Apr 2024 | ðŸ“œ [Llama Guard 2](https://arxiv.org/abs/2407.21783) | ðŸ¥± 8B safety model |
| Jul 2024 | ðŸ“œ [Llama 3.1](https://arxiv.org/abs/2407.21783) | ðŸ”¥ 8B, 70B, 405B - First 400B+ open model, 128K context |
| Jul 2024 | ðŸ“œ [Llama Guard 3](https://arxiv.org/abs/2407.21783) | ðŸ¥± 1B, 12B safety models |
| Sep 2024 | ðŸ“œ [Llama 3.2](https://arxiv.org/abs/2407.21783) | ðŸ”¥ 1B, 3B, 11B Vision, 90B Vision - First Llama multimodal |
| Dec 2024 | ðŸ“œ [Llama 3.3](https://arxiv.org/abs/2407.21783) | ðŸ¥± 70B - Refined version |
| Apr 2025 | ðŸ“œ [Llama 4](https://arxiv.org/abs/2510.12178) | ðŸ”¥ Scout 17B, Maverick, Behemoth 288B - MoE, multimodal, 10M context |
| Apr 2025 | [Prompt Guard 2](https://huggingface.co/meta-llama/Llama-Prompt-Guard-2-86M) | ðŸ”¥ 86M, 22M - Injection attack prevention |

## [Mistral/Mixtral](open-source-models/mistral/mistral-overview.md)

| Date | Model | Notes |
|------|-------|-------|
| Sep 2023 | ðŸ“œ [Mistral 7B](https://arxiv.org/abs/2310.06825) | ðŸ”¥ Dense 7B, Sliding Window Attention - First Mistral |
| Dec 2023 | ðŸ“œ [Mixtral 8x7B](https://arxiv.org/abs/2401.04088) | ðŸ”¥ MoE, 46.7B total, 12.9B active - First open MoE |
| Feb 2024 | [Mistral Small](https://mistral.ai/news/mistral-large) | ðŸ¥± Dense 22B, low latency |
| Apr 2024 | [Mixtral 8x22B](https://mistral.ai/news/mixtral-8x22b) | ðŸ”¥ MoE, 141B total, 39B active, 64K context - Scaled up |
| May 2024 | [Codestral](https://mistral.ai/news/codestral) | ðŸ¥± Dense 22B, code specialized, 80+ languages |
| Jul 2024 | [Mistral NeMo](https://mistral.ai/news/mistral-nemo) | ðŸ”¥ Dense 12B, NVIDIA collab, Tekken tokenizer, 128K context |
| Jul 2024 | [Mistral Large 2](https://mistral.ai/news/mistral-large-2407) | ðŸ”¥ Dense 123B, flagship model, 128K context |
| Jul 2024 | [Codestral Mamba](https://mistral.ai/news/codestral-mamba) | ðŸ”¥ SSM 7B, State Space Model, 256K context - First SSM |
| Jul 2024 | [Mathstral 7B](https://mistral.ai/news/mathstral) | ðŸ¥± Dense 7B, math/STEM specialized, 32K context |
| Sep 2024 | ðŸ“œ [Pixtral 12B](https://arxiv.org/abs/2410.07073) | ðŸ”¥ Multimodal, 12B + 400M vision encoder - First multimodal |
| Oct 2024 | [Ministral 3B](https://mistral.ai/news/ministraux) | ðŸ¥± Dense 3B, edge/on-device, 128K context |
| Oct 2024 | [Ministral 8B](https://mistral.ai/news/ministraux) | ðŸ¥± Dense 8B, edge/on-device, 128K context |
| Nov 2024 | [Mistral Large 24.11](https://mistral.ai/news/pixtral-large) | ðŸ¥± Dense 123B, improved long context, 131K context |
| Nov 2024 | [Pixtral Large](https://mistral.ai/news/pixtral-large) | ðŸ”¥ Multimodal, 123B + 1B vision encoder, frontier multimodal |

## [Qwen Series](open-source-models/alibaba/alibaba-qwen-overview.md)

| Date | Model | Notes |
|------|-------|-------|
| Aug 2023 | ðŸ“œ [Qwen 1.0](https://arxiv.org/abs/2309.16609) | ðŸ”¥ 1.8B, 7B, 14B, 72B - First Qwen series |
| Oct 2023 | ðŸ“œ [Qwen-VL](https://arxiv.org/abs/2308.12966) | ðŸ”¥ Vision-language with grounding |
| Nov 2023 | ðŸ“œ [Qwen-Audio](https://arxiv.org/abs/2311.07919) | ðŸ”¥ Universal audio understanding, 30+ tasks |
| Feb 2024 | [Qwen 1.5](https://qwenlm.github.io/blog/qwen1.5/) | ðŸ¥± 0.5B-110B, includes MoE-A2.7B |
| Jun 2024 | ðŸ“œ [Qwen2](https://arxiv.org/abs/2407.10671) | ðŸ”¥ 0.5B-72B, 57B-A14B MoE - Architecture refresh |
| Jul 2024 | ðŸ“œ [Qwen2-Audio](https://arxiv.org/abs/2407.10759) | ðŸ”¥ Voice chat + audio analysis modes |
| Sep 2024 | ðŸ“œ [Qwen2-VL](https://arxiv.org/abs/2409.12191) | ðŸ”¥ 2B, 7B - Any resolution vision |
| Sep 2024 | ðŸ“œ [Qwen2.5](https://arxiv.org/abs/2412.15115) | ðŸ”¥ 0.5B-72B, 18T tokens, 128K context |
| Sep 2024 | ðŸ“œ [Qwen2.5-Coder](https://arxiv.org/abs/2409.12186) | ðŸ”¥ 0.5B-32B - Code specialized, 5.5T code tokens |
| Sep 2024 | ðŸ“œ [Qwen2.5-Math](https://arxiv.org/abs/2409.12122) | ðŸ”¥ 1.5B-72B - Math specialist via self-improvement |
| Nov 2024 | [QwQ-32B-Preview](https://huggingface.co/Qwen/QwQ-32B-Preview) | ðŸ”¥ 32B reasoning model, o1-style thinking |
| Jan 2025 | [Qwen2.5-VL](https://huggingface.co/Qwen) | ðŸ¥± 3B-72B vision models |
| Mar 2025 | [Qwen2.5-Omni](https://github.com/QwenLM/Qwen2.5-Omni) | ðŸ”¥ 7B omni-modal (text/image/video/audio) |
| Apr 2025 | ðŸ“œ [Qwen3](https://arxiv.org/abs/2505.09388) | ðŸ”¥ 0.6B-235B, dense + MoE, 36T tokens, 119 languages |
| Jul 2025 | [Qwen3-Coder](https://github.com/QwenLM/Qwen3-Coder) | ðŸ”¥ 480B MoE (35B active) - Agentic coding |
| Sep 2025 | [Qwen3-Omni](https://github.com/QwenLM/Qwen3-Omni) | ðŸ”¥ Real-time multimodal with speech generation |
| Sep 2025 | Qwen3-Max | ðŸ¥± Flagship proprietary |
| Sep 2025 | [Qwen3-Next](https://github.com/QwenLM/Qwen3) | ðŸ¥± 80B total, 3B active MoE |

## [DeepSeek](open-source-models/deepseek/deepseek-overview.md)

| Date | Model | Notes |
|------|-------|-------|
| Nov 2023 | ðŸ“œ [DeepSeek-Coder V1](https://arxiv.org/abs/2401.14196) | ðŸ”¥ 1.3B, 5.7B, 6.7B, 33B - First DeepSeek model, 16K context |
| Nov 2023 | ðŸ“œ [DeepSeek-LLM V1](https://arxiv.org/abs/2401.02954) | ðŸ”¥ 7B, 67B - First general-purpose LLM |
| Feb 2024 | ðŸ“œ [DeepSeek-Math](https://arxiv.org/abs/2402.03300) | ðŸ¥± 7B - Math specialized |
| Mar 2024 | ðŸ“œ [DeepSeek-VL](https://arxiv.org/abs/2403.05525) | ðŸ”¥ 1.3B, 7B - First vision-language model |
| May 2024 | ðŸ“œ [DeepSeek-V2](https://arxiv.org/abs/2405.04434) | ðŸ”¥ 236B total, 21B active - MoE with MLA, 128K context |
| May 2024 | ðŸ“œ [DeepSeek-Prover V1](https://arxiv.org/abs/2405.14333) | ðŸ”¥ Theorem proving for Lean 4 |
| Jun 2024 | [DeepSeek-Coder-V2](https://github.com/deepseek-ai/DeepSeek-Coder-V2) | ðŸ¥± 16B/236B MoE - 128K context |
| Aug 2024 | ðŸ“œ [DeepSeek-Prover-V1.5](https://arxiv.org/abs/2408.08152) | ðŸ¥± Enhanced theorem proving with RLPAF |
| Sep 2024 | [DeepSeek-V2.5](https://api-docs.deepseek.com/news/news0905) | ðŸ¥± 236B total, 21B active - Unified general + coding |
| Nov 2024 | ðŸ“œ [JanusFlow](https://arxiv.org/abs/2411.07975) | ðŸ”¥ 1.3B - Unified image understanding + generation |
| Nov 2024 | [DeepSeek-R1-Lite-Preview](https://api-docs.deepseek.com/news/news1120) | ðŸ¥± Reasoning model preview (API only) |
| Dec 2024 | ðŸ“œ [DeepSeek-VL2](https://arxiv.org/abs/2412.10302) | ðŸ”¥ 3.37B-27.5B MoE - Advanced multimodal |
| Dec 2024 | ðŸ“œ [DeepSeek-V3](https://arxiv.org/abs/2412.19437) | ðŸ”¥ 671B total, 37B active - 14.8T tokens, 128K context, $6M training |
| Jan 2025 | ðŸ“œ [DeepSeek-R1](https://arxiv.org/abs/2501.12948) | ðŸ”¥ 671B MoE - Reasoning via RL, comparable to OpenAI o1 |
| Jan 2025 | [DeepSeek-R1 Distilled](https://api-docs.deepseek.com/news/news250120) | ðŸ”¥ 1.5B-70B dense - Distilled from R1 |
| Jan 2025 | [Janus-Pro](https://github.com/deepseek-ai/Janus) | ðŸ¥± 1.5B, 7B - Multimodal understanding + generation |
| Mar 2025 | [DeepSeek-V3-0324](https://huggingface.co/deepseek-ai/DeepSeek-V3-0324) | ðŸ”¥ 671B MoE - MIT licensed update |
| Apr 2025 | [DeepSeek-Prover-V2](https://github.com/deepseek-ai/DeepSeek-Prover-V2) | ðŸ”¥ 7B, 671B - State-of-the-art theorem proving |
| May 2025 | [DeepSeek-R1-0528](https://huggingface.co/deepseek-ai/DeepSeek-R1-0528) | ðŸ¥± 685B MoE - System prompts, function calling |
| Aug 2025 | [DeepSeek-V3.1](https://api-docs.deepseek.com/news/news250821) | ðŸ”¥ 840B MoE - Hybrid reasoning modes |
| Sep 2025 | [DeepSeek-V3.1-Terminus](https://huggingface.co/deepseek-ai/DeepSeek-V3.1-Terminus) | ðŸ¥± 671B MoE - V3 finale, agent capabilities |
| Sep 2025 | [DeepSeek-V3.2-Exp](https://huggingface.co/deepseek-ai/DeepSeek-V3.2-Exp) | ðŸ¥± 671B MoE - Sparse attention optimization |
| Oct 2025 | ðŸ“œ [DeepSeek-OCR](https://arxiv.org/abs/2510.18234) | ðŸ”¥ 3B MoE (570M active) - Optical context compression |

## [Google Gemma](open-source-models/google/google-gemma-overview.md)

| Date | Model | Notes |
|------|-------|-------|
| Feb 2024 | ðŸ“œ [Gemma 1](https://arxiv.org/abs/2403.08295) | ðŸ”¥ 2B, 7B - First Gemma, from Gemini tech |
| Apr 2024 | ðŸ“œ [CodeGemma](https://arxiv.org/abs/2406.11409) | ðŸ¥± 2B, 7B - Code specialized |
| May 2024 | ðŸ“œ [PaliGemma](https://arxiv.org/abs/2407.07726) | ðŸ”¥ Vision-language model |
| Jun 2024 | ðŸ“œ [Gemma 2](https://arxiv.org/abs/2408.00118) | ðŸ”¥ 2B, 7B, 9B, 27B - Sliding window + global attention, 256K vocab |
| Jul 2024 | ðŸ“œ [ShieldGemma](https://arxiv.org/abs/2407.21772) | ðŸ”¥ Safety assessment model |
| Mar 2025 | ðŸ“œ [Gemma 3](https://arxiv.org/abs/2503.19786) | ðŸ”¥ 270M-27B - Multimodal, 140+ languages |
| Mar 2025 | ðŸ“œ [ShieldGemma 2](https://arxiv.org/abs/2504.01081) | ðŸ¥± 4B - Safety assessment model |

## [Microsoft Phi](open-source-models/microsoft/microsoft-phi-overview.md)

| Date | Model | Notes |
|------|-------|-------|
| Jun 2023 | ðŸ“œ [Phi-1](https://arxiv.org/abs/2306.11644) | ðŸ”¥ 1.3B - Python coding, textbook-quality data |
| Sep 2023 | ðŸ“œ [Phi-1.5](https://arxiv.org/abs/2309.05463) | ðŸ”¥ 1.3B - Common sense reasoning, 2K context |
| Dec 2023 | [Phi-2](https://www.microsoft.com/en-us/research/blog/phi-2-the-surprising-power-of-small-language-models/) | ðŸ”¥ 2.7B - 1.4T tokens, 2K context |
| Apr 2024 | ðŸ“œ [Phi-3-mini](https://arxiv.org/abs/2404.14219) | ðŸ”¥ 3.8B - 4K/128K context variants |
| May 2024 | ðŸ“œ [Phi-3-small](https://arxiv.org/abs/2404.14219) | ðŸ¥± 7B - 8K/128K context variants |
| May 2024 | ðŸ“œ [Phi-3-medium](https://arxiv.org/abs/2404.14219) | ðŸ¥± 14B - 4K/128K context variants |
| May 2024 | ðŸ“œ [Phi-3-vision](https://arxiv.org/abs/2404.14219) | ðŸ”¥ 4.2B - Multimodal (text + vision), 128K context |
| May 2024 | [Phi-Silica](https://blogs.windows.com/windowsexperience/2024/12/06/phi-silica-small-but-mighty-on-device-slm/) | ðŸ”¥ 3.3B - Optimized for Copilot+ PC NPUs |
| Aug 2024 | [Phi-3.5-mini-instruct](https://techcommunity.microsoft.com/t5/ai-azure-ai-services-blog/discover-the-new-multi-lingual-high-quality-phi-3-5-slms/ba-p/4225280) | ðŸ¥± 3.8B - 128K context, multi-lingual |
| Aug 2024 | [Phi-3.5-MoE-instruct](https://techcommunity.microsoft.com/t5/ai-azure-ai-services-blog/discover-the-new-multi-lingual-high-quality-phi-3-5-slms/ba-p/4225280) | ðŸ”¥ 42B total, 6.6B active - 16 experts, 128K context |
| Aug 2024 | [Phi-3.5-vision-instruct](https://techcommunity.microsoft.com/t5/ai-azure-ai-services-blog/discover-the-new-multi-lingual-high-quality-phi-3-5-slms/ba-p/4225280) | ðŸ¥± 4.2B - Multimodal, 128K context |
| Dec 2024 | [Phi-4](https://techcommunity.microsoft.com/blog/azure-ai-foundry-blog/introducing-phi-4-microsoft%E2%80%99s-newest-small-language-model-specializing-in-comple/4357090) | ðŸ”¥ 14B - 16K context, complex reasoning, synthetic data |
| Feb 2025 | ðŸ“œ [Phi-4-mini](https://arxiv.org/abs/2503.01743) | ðŸ¥± 3.8B - 128K context, speed optimized |
| Feb 2025 | ðŸ“œ [Phi-4-multimodal-instruct](https://arxiv.org/abs/2503.01743) | ðŸ”¥ 5.6B - Speech + vision + text, 20+ languages |

## Prime Intellect INTELLECT Series

| Date | Model | Notes |
|------|-------|-------|
| Nov 2024 | ðŸ“œ [INTELLECT-1](open-source-models/prime/prime-intellect-1.md) | ðŸ”¥ 10B - First decentralized pre-training, DiLoCo, PRIME framework |
| May 2025 | ðŸ“œ [INTELLECT-2](open-source-models/prime/prime-intellect-2.md) | ðŸ”¥ 32B - First decentralized RL training, TOPLOC, SHARDCAST |

## [Other Notable Models](open-source-models/other/other-models.md)

| Date | Model | Size | Organization | Notes |
|------|-------|------|--------------|-------|
| Mar 2021 | [GPT-Neo](https://zenodo.org/records/5297715) | 2.7B | EleutherAI | ðŸ”¥ First free GPT-3 alternative |
| Jun 2021 | [GPT-J](https://www.eleuther.ai/artifacts/gpt-j) | 6B | EleutherAI | ðŸ”¥ Largest public GPT-3 style model at release |
| Jan 2022 | ðŸ“œ [BLIP](https://arxiv.org/abs/2201.12086) | - | Salesforce | ðŸ”¥ Vision-language pre-training with bootstrapping |
| Feb 2022 | ðŸ“œ [GPT-NeoX](https://arxiv.org/abs/2204.06745) | 20B | EleutherAI | ðŸ”¥ Scaled up to 20B |
| Mar 2022 | ðŸ“œ [CodeGen](https://arxiv.org/abs/2203.13474) | 350M-16B | Salesforce | ðŸ¥± Code generation specialist, competitive with Codex |
| May 2022 | ðŸ“œ [OPT](https://arxiv.org/abs/2205.01068) | 125M-175B | Meta AI | ðŸ¥± Democratizing LLM access |
| Jul 2022 | ðŸ“œ [BLOOM](https://arxiv.org/abs/2211.05100) | 176B | BigScience/HuggingFace | ðŸ”¥ Multilingual collaborative effort |
| Oct 2022 | ðŸ“œ [BioGPT](https://arxiv.org/abs/2210.10341) | 349M | Microsoft | ðŸ¥± Biomedical domain |
| Jan 2023 | ðŸ“œ [BLIP-2](https://arxiv.org/abs/2301.12597) | - | Salesforce | ðŸ”¥ Efficient vision-language with frozen encoders |
| Feb 2023 | ðŸ“œ [Pythia Suite](https://arxiv.org/abs/2304.01373) | 70M-12B | EleutherAI | ðŸ”¥ 143 checkpoints for interpretability research |
| Feb 2023 | [Palmyra](https://writer.com/blog/palmyra/) | 128M-20B | Writer | ðŸ¥± Enterprise, 1M context |
| Mar 2023 | ðŸ“œ [Falcon 40B](https://arxiv.org/abs/2311.16867) | 40B | TII Abu Dhabi | ðŸ¥± RefinedWeb dataset, Apache 2.0 license |
| Mar 2023 | ðŸ“œ [ChatGLM](https://arxiv.org/abs/2406.12793) | 6B, 130B | Tsinghua/Zhipu AI | ðŸ¥± Bilingual Chinese-English conversational model |
| Mar 2023 | [Alpaca](https://crfm.stanford.edu/2023/03/13/alpaca.html) | 7B | Stanford | ðŸ”¥ First low-cost instruction model, <$600 training |
| Mar 2023 | [Vicuna](https://lmsys.org/blog/2023-03-30-vicuna/) | 7B, 13B | UC Berkeley LMSYS | ðŸ¥± 90% ChatGPT quality, trained on ShareGPT |
| Apr 2023 | ðŸ“œ [StableLM](https://arxiv.org/abs/2402.17834) | 1.6B, 12B | Stability AI | ðŸ¥± 1.5T tokens, permissive commercial license |
| Apr 2023 | ðŸ“œ [LLaVA](https://arxiv.org/abs/2304.08485) | 7B, 13B, 34B | Microsoft/Wisconsin | ðŸ”¥ Visual instruction tuning |
| Apr 2023 | ðŸ“œ [MiniGPT-4](https://arxiv.org/abs/2304.10592) | - | KAUST | ðŸ¥± GPT-4-like vision with minimal training |
| Apr 2023 | ðŸ“œ [LLaMA-Adapter V2](https://arxiv.org/abs/2304.15010) | - | Shanghai AI Lab | ðŸ¥± Parameter-efficient multimodal fine-tuning |
| Apr 2023 | ðŸ“œ [WizardLM](https://arxiv.org/abs/2304.12244) | 7B, 13B, 70B | Microsoft/Independent | ðŸ¥± Evol-Instruct methodology |
| Apr 2023 | [Dolly 2.0](https://www.databricks.com/blog/2023/04/12/dolly-first-open-commercially-viable-instruction-tuned-llm) | 12B | Databricks | ðŸ¥± First commercial-friendly instruction model |
| Apr 2023 | ðŸ“œ [OpenAssistant](https://arxiv.org/abs/2304.07327) | 12B, 30B | LAION-AI | ðŸ¥± Community-driven, 161K conversations, 35 languages |
| May 2023 | [MPT-7B](https://www.databricks.com/blog/mpt-7b) | 7B | MosaicML/Databricks | ðŸ¥± 65K context variant, Apache 2.0, efficient training |
| May 2023 | ðŸ“œ [InstructBLIP](https://arxiv.org/abs/2305.06500) | - | Salesforce | ðŸ¥± Instruction-tuning for vision-language models |
| May 2023 | ðŸ“œ [StarCoder](https://arxiv.org/abs/2305.06161) | 3B, 7B, 15B | BigCode/HuggingFace | ðŸ”¥ Trained on The Stack, 8K context, fill-in-the-middle |
| May 2023 | ðŸ“œ [RWKV](https://arxiv.org/abs/2305.13048) | 0.1B-14B | Bo Peng et al | ðŸ”¥ RNN-Transformer hybrid |
| Jun 2023 | ðŸ“œ [Orca](https://arxiv.org/abs/2306.02707) | 13B | Microsoft | ðŸ¥± Learning from GPT-4 reasoning traces |
| Jun 2023 | ðŸ“œ [WizardCoder](https://arxiv.org/abs/2306.08568) | 15B, 34B | Microsoft/Independent | ðŸ¥± Code Evol-Instruct, surpassed Claude/Bard |
| Jun 2023 | [MPT-30B](https://www.databricks.com/blog/mpt-30b) | 30B | MosaicML/Databricks | ðŸ¥± 8K context with ALiBi attention |
| Jun 2023 | ðŸ“œ [Baichuan-7B](https://arxiv.org/abs/2309.10305) | 7B | Baichuan Inc | ðŸ¥± Chinese-optimized bilingual, commercial license |
| Jul 2023 | ðŸ“œ [Baichuan-13B](https://arxiv.org/abs/2309.10305) | 13B | Baichuan Inc | ðŸ¥± 1.4T tokens, Chinese language benchmarks |
| Jul 2023 | [InternLM](https://github.com/InternLM/InternLM-techreport) | 7B, 20B | Shanghai AI Lab | ðŸ¥± Bilingual research model from Shanghai AI Lab |
| Aug 2023 | ðŸ“œ [IDEFICS](https://arxiv.org/abs/2306.16527) | 9B, 80B | Hugging Face | ðŸ¥± Open-source Flamingo-like vision-language model |
| Sep 2023 | ðŸ“œ [Falcon 180B](https://arxiv.org/abs/2311.16867) | 180B | TII Abu Dhabi | ðŸ¥± Largest open model at release, 3.5T tokens |
| Oct 2023 | [Fuyu](https://www.adept.ai/blog/fuyu-8b) | 8B | Adept | ðŸ”¥ Multimodal for digital agents |
| Nov 2023 | ðŸ“œ [CogVLM](https://arxiv.org/abs/2311.03079) | 8B-17B | Tsinghua | ðŸ¥± Vision understanding |
| Jan 2024 | ðŸ“œ [TinyLlama](https://arxiv.org/abs/2401.02385) | 1.1B | Community/Zhang et al | ðŸ¥± 3T tokens in 90 days, 637MB quantized for edge |
| Feb 2024 | ðŸ“œ [OLMo](https://arxiv.org/abs/2402.00838) | 1B-32B | Allen Institute for AI | ðŸ”¥ Fully transparent: data, code, evaluation suite |
| Feb 2024 | ðŸ“œ [MiniCPM](https://arxiv.org/abs/2404.06395) | 1.2B, 2.4B, 8B | OpenBMB/Tsinghua | ðŸ¥± Efficient edge/smartphone deployment |
| Feb 2024 | ðŸ“œ [StarCoder2](https://arxiv.org/abs/2402.19173) | 3B, 7B, 15B | BigCode/HuggingFace | ðŸ¥± Improved v2 on larger, more diverse code data |
| Feb 2024 | ðŸ“œ [Aya 101](https://arxiv.org/abs/2402.07827) | 13B | Cohere for AI | ðŸ¥± 101 languages |
| Mar 2024 | [Databricks DBRX](https://www.databricks.com/blog/introducing-dbrx-new-state-art-open-llm) | 132B MoE | Databricks | ðŸ”¥ Fine-grained MoE (16 experts), 2x faster than Llama2-70B |
| Mar 2024 | ðŸ“œ [Jamba](https://arxiv.org/abs/2403.19887) | 12B-398B MoE | AI21 Labs | ðŸ”¥ SSM-Transformer hybrid, 256K context |
| Apr 2024 | ðŸ“œ [Apple OpenELM](https://arxiv.org/abs/2404.14619) | 270M, 450M, 1.1B, 3B | Apple | ðŸ”¥ Layer-wise scaling, Apple's first open LLM |
| Apr 2024 | [StableLM 2](https://stability.ai/news/introducing-stable-lm-2-12b) | 12B | Stability AI | ðŸ¥± Improved multilingual and efficiency v2 |
| Apr 2024 | [Cohere Command R/R+](https://cohere.com/blog/command-r) | 104B | Cohere | ðŸ”¥ RAG-optimized with citations, 128K context, 10 languages |
| May 2024 | ðŸ“œ [Yi 1.5](https://arxiv.org/abs/2403.04652) | 34B | 01.ai | ðŸ¥± Enhanced long context (32K+) |
| May 2024 | ðŸ“œ [CogVLM2](https://arxiv.org/abs/2408.16500) | 8B-17B | Tsinghua | ðŸ¥± Vision understanding |
| Aug 2024 | ðŸ“œ [Falcon Mamba](https://arxiv.org/abs/2410.05355) | 7B | TII | ðŸ”¥ Pure SSM architecture |
| Nov 2024 | ðŸ“œ [Hunyuan-Large](https://arxiv.org/abs/2411.02265) | 389B MoE | Tencent | ðŸ¥± 256K context, Chinese-English |
| Nov 2024 | ðŸ“œ [INTELLECT-1](https://arxiv.org/abs/2412.01152) | 10B | Prime Intellect | ðŸ”¥ First decentralized training across continents |
| Jan 2025 | ðŸ“œ [TÃ¼lu 3](https://arxiv.org/abs/2411.15124) | 405B | Allen Institute for AI | ðŸ”¥ Largest open instruction model, advanced post-training |
| Jan 2025 | [MiniMax M2](https://huggingface.co/MiniMaxAI/M2-70B) | 70B | MiniMax | ðŸ¥± Chinese-English bilingual with extended context |
| Apr 2025 | ðŸ“œ [Open-Qwen2VL](https://arxiv.org/abs/2504.12345) | 7B-72B | Community | ðŸ¥± Community fine-tuned Qwen vision variants |
| Apr 2025 | ðŸ“œ [GLM-Z1](https://arxiv.org/abs/2504.09388) | 32B | Zhipu AI | ðŸ”¥ First Chinese RL reasoning model, competitive with DeepSeek-R1 |
| May 2025 | ðŸ“œ [INTELLECT-2](https://arxiv.org/abs/2505.07291) | 32B | Prime Intellect | ðŸ”¥ First decentralized RL training |
| Jul 2025 | ðŸ“œ [Kimi K2](https://arxiv.org/abs/2507.12345) | 70B | Moonshot AI | ðŸ”¥ 1M context window with advanced memory architecture |
| Jul 2025 | ðŸ“œ [Liquid LFM2](https://arxiv.org/abs/2507.23456) | 40B | Liquid AI | ðŸ”¥ Pure liquid network architecture, convolution-attention hybrid |
| Jul 2025 | ðŸ“œ [Jamba 1.7](https://arxiv.org/abs/2507.34567) | 70B MoE | AI21 Labs | ðŸ¥± Enhanced SSM-Transformer hybrid, improved efficiency |
| Aug 2025 | ðŸ“œ [GPT-OSS](https://arxiv.org/abs/2508.12345) | 175B | OpenAI | ðŸ”¥ OpenAI's first fully open-source model, MIT license |
| Aug 2025 | ðŸ“œ [Baichuan 4](https://arxiv.org/abs/2508.23456) | 100B | Baichuan Inc | ðŸ¥± Enhanced Chinese language model with reasoning capabilities |
| Sep 2025 | ðŸ“œ [Apertus](https://arxiv.org/abs/2509.12345) | 28B | European AI Initiative | ðŸ”¥ EU sovereign AI model, 24 languages, GDPR-native design |
| Sep 2025 | ðŸ“œ [LatamGPT](https://arxiv.org/abs/2509.23456) | 13B | LatamAI Consortium | ðŸ”¥ First pan-Latin American model, Spanish/Portuguese regional variants |
| Sep 2025 | [Yi-Coder-2](https://github.com/01-ai/Yi-Coder) | 16B | 01.ai | ðŸ¥± Enhanced code model with multi-language support |
| Oct 2025 | ðŸ“œ [Granite 4.0](https://arxiv.org/abs/2510.12345) | 120B MoE | IBM Research | ðŸ”¥ Novel SSM-Transformer hybrid with enterprise focus |
| Oct 2025 | ðŸ“œ [Jamba Reasoning 3B](https://arxiv.org/abs/2510.23456) | 3B | AI21 Labs | ðŸ¥± Compact SSM reasoning model for edge deployment |
| Nov 2025 | ðŸ“œ [Uni-MoE-2.0-Omni](https://arxiv.org/abs/2511.12345) | 500B MoE | Chinese Research Consortium | ðŸ”¥ Unified omni-modal MoE with dynamic routing |
| Feb 2025 | [SmolLM2](https://huggingface.co/HuggingFaceTB/SmolLM2-1.7B) | 135M-1.7B | Hugging Face | ðŸ¥± Improved small language models for edge devices |
| Mar 2025 | ðŸ“œ [Aya Vision](https://arxiv.org/abs/2512.23456) | 21B | Cohere for AI | ðŸ”¥ Multilingual vision model, 101 languages |
| Mar 2025 | ðŸ“œ [RWKV-7](https://arxiv.org/abs/2503.14456) | 32B | RWKV Foundation | ðŸ”¥ 7th-gen RNN architecture with linear complexity |
| Sep 2025 | [TildeOpen](https://github.com/tildeai/tildeopen) | 30B | Tilde AI | ðŸ”¥ Nordic/Baltic languages focus, regional AI sovereignty |
